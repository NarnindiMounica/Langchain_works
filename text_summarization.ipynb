{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5db26f",
   "metadata": {},
   "source": [
    "Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70525c30",
   "metadata": {},
   "source": [
    "This notebook walks through how to use LangChain for summarization over a list of documents. It covers three different chain types: stuff, map_reduce, and refine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b706b05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd46458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Langchain\\chainvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"llama-3.1-8b-instant\", api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.schema import AIMessage, HumanMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cf3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech=\"\"\"Salesforce CEO Mark Benioff has declared that he will not use ChatGPT, the AI chatbot developed by Microsoft-backed AI company OpenAI. His comments came after he used Google‚Äôs latest AI model, Gemini 3.0, stating that a two-hour experience with Google‚Äôs new model has convinced him to switch permanently. Google CEO Sundar Pichai termed the Gemini 3 as the company‚Äôs ‚Äúmost intelligent model, that combines all of Gemini‚Äôs capabilities together so you can bring any idea to life.\n",
    "\n",
    "In a post on X, Benioff claimed that Gemini 3.0 has brought a monumental shift in the AI landscape. The CEO specifically praised Gemini 3‚Äôs performance across multiple modalities, noting key improvements in reasoning, speed, image quality, and video processing as the compelling factors behind his decision.\n",
    "\n",
    "Holy shit. I‚Äôve used ChatGPT every day for 3 years. Just spent 2 hours on Gemini 3. I‚Äôm not going back. The leap is insane ‚Äî reasoning, speed, images, video‚Ä¶ everything is sharper and faster. It feels like the world just changed, again. ‚ù§Ô∏è ü§ñ\n",
    "\n",
    "Tech leaders acknowledge Gemini 3.0 success\n",
    "Benioff‚Äôs endorsement highlights the competitive pressure now facing OpenAI as models from rival tech giants like Google make significant advances. Tesla CEO Elon Musk and even OpenAI chief executive Sam Altman praised Pichai following the launch of Gemini 3.\n",
    "The xAI founder said ‚ÄúCongrats‚Äù on Pichai‚Äôs \"Geminiii\" announcement on X, and ‚ÄúNice Work‚Äù to Demis Hassabis, the chief of DeepMind. Altman posted Congrats to Google on Gemini 3! Looks like a great model.\n",
    "\n",
    "Altman even noted potential ‚Äútemporary economic headwinds‚Äù in an internal memo but expressed confidence in OpenAl‚Äôs rapid progress and leadership in the Al race.\n",
    "\n",
    "We have built enough strength as a company to weather great models shipping elsewhere competition... (so), having most of our research team focused on really getting to superintelligence is critically important, Altman wrote.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "823c89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message=[\n",
    "    SystemMessage(content=\"you are an expert with expertise in summarizing speeches\"),\n",
    "    HumanMessage(content=\"Please provide a short and concise summary of following speech:{text}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50935ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_num_tokens(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e350a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems that there\\'s a lot of competition and advancements in the field of AI, particularly from Google\\'s Gemini 3.0 model. Here\\'s a summary of the key points:\\n\\n1. Salesforce CEO Mark Benioff has declared that he will not use ChatGPT, the AI chatbot developed by OpenAI, after using Google\\'s Gemini 3.0 model. He praised its performance across multiple modalities, including reasoning, speed, image quality, and video processing.\\n\\n2. Benioff\\'s endorsement highlights the competitive pressure on OpenAI, as rival tech giants like Google are making significant advances in AI. This is evident from the praise received by Google CEO Sundar Pichai from other tech leaders, including Tesla CEO Elon Musk and OpenAI CEO Sam Altman.\\n\\n3. OpenAI CEO Sam Altman expressed confidence in the company\\'s rapid progress and leadership in the AI race. Despite potential \"temporary economic headwinds,\" Altman believes that OpenAI has built enough strength to weather the competition.\\n\\n4. Altman emphasized the importance of OpenAI\\'s research team focusing on achieving superintelligence, rather than competing with other models in the short term.\\n\\nOverall, it seems that the AI landscape is rapidly evolving, with significant advancements from Google\\'s Gemini 3.0 model. This competition will likely drive further innovation and improvements in AI technology.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(speech).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb113ae1",
   "metadata": {},
   "source": [
    "Prompt Template Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a587b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "generic_template=\"\"\"\n",
    "Write the summary of given context\n",
    "Speech:{speech}\n",
    "Aslo translate the summary speech to {language}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=generic_template,\n",
    "    input_variables=['speech', 'language']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bd81a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prompt=prompt.format(speech=speech, language='hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fdf3536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_num_tokens(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddbdd162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"**Summary:**\\n\\n Salesforce CEO Mark Benioff has expressed his preference for Google's latest AI model, Gemini 3.0, over Microsoft-backed OpenAI's ChatGPT. After using Gemini 3.0 for two hours, Benioff claimed that it has brought a significant shift in the AI landscape, with improvements in reasoning, speed, image quality, and video processing. Tech leaders like Elon Musk and Sam Altman have acknowledged the success of Gemini 3.0, but Altman has expressed confidence in OpenAI's progress and leadership in the AI race.\\n\\n**‡∞∏‡∞Ç‡∞ï‡±ç‡∞∑‡∞ø‡∞™‡±ç‡∞§‡∞Ç:**\\n\\n‡∞∏‡∞æ‡∞≤‡±ç‡∞∏‡±ç\\u200c‡∞´‡±ã‡∞∞‡±ç‡∞∏‡±ç ‡∞∏‡±Ä‡∞à‡∞ì ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±ç ‡∞¨‡±Ü‡∞®‡∞ø‡∞Ø‡±ã‡∞´‡±ç ‡∞Æ‡±à‡∞ï‡±ç‡∞∞‡±ã‡∞∏‡∞æ‡∞´‡±ç‡∞ü‡±ç ‡∞∏‡∞π‡∞ï‡∞æ‡∞∞‡∞Ç‡∞§‡±ã ‡∞â‡∞®‡±ç‡∞® ‡∞ì‡∞™‡±Ü‡∞®‡±ç\\u200c‡∞é‡∞Ø‡∞ø ‡∞§‡∞Ø‡∞æ‡∞∞‡±Å‡∞ö‡±á‡∞∏‡∞ø‡∞® ‡∞ö‡∞æ‡∞ü‡±ç\\u200c‡∞ú‡±Ä‡∞™‡±Ä‡∞ü‡∞ø ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞ó‡±Ç‡∞ó‡±Å‡∞≤‡±ç ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞§‡±ç‡∞Ø‡∞æ‡∞ß‡±Å‡∞®‡∞ø‡∞ï ‡∞é‡∞Ø‡∞ø ‡∞Æ‡∞æ‡∞°‡∞≤‡±ç ‡∞ú‡±Ä‡∞Æ‡∞ø‡∞®‡±Ä 3.0‡∞®‡∞ø ‡∞™‡±ç‡∞∞‡∞ø‡∞∑‡±ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. ‡∞∞‡±Ü‡∞Ç‡∞°‡±Å ‡∞ó‡∞Ç‡∞ü‡∞≤‡±Å ‡∞ó‡±Ç‡∞ó‡±Å‡∞≤‡±ç ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞ï‡±ä‡∞§‡±ç‡∞§ ‡∞Æ‡∞æ‡∞°‡∞≤‡±ç\\u200c‡∞®‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø‡∞® ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§, ‡∞¨‡±Ü‡∞®‡∞ø‡∞Ø‡±ã‡∞´‡±ç ‡∞ú‡±Ä‡∞Æ‡∞ø‡∞®‡±Ä 3.0 ‡∞Ö‡∞®‡±á‡∞¶‡∞ø ‡∞Ö‡∞Ø‡∞ø‡∞Ç‡∞¶‡∞ø ‡∞Ö‡∞®‡∞ø ‡∞™‡±ç‡∞∞‡∞ï‡∞ü‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. ‡∞ú‡±Ä‡∞Æ‡∞ø‡∞®‡±Ä 3.0 ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞µ‡∞ø‡∞ú‡∞Ø‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ü‡±Ü‡∞ï‡±ç ‡∞≤‡±Ä‡∞°‡∞∞‡±ç‡∞≤‡±Å ‡∞ó‡±Å‡∞∞‡±ç‡∞§‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. \\n\\n‡∞™‡∞æ‡∞∞‡±ç‡∞∏‡∞ø‡∞´‡±ç‡∞≤‡±ã ‡∞∏‡±Ä‡∞à‡∞ì ‡∞á‡∞≤‡∞æ‡∞®‡±ç ‡∞Æ‡∞æ‡∞∏‡±ç‡∞ï‡±ç, ‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞ö‡±Ä‡∞´‡±ç ‡∞è‡∞ï‡±ç‡∞∏‡±Ü‡∞ï‡±ç‡∞Ø‡±Ç‡∞ü‡∞ø‡∞µ‡±ç ‡∞∏‡∞æ‡∞Ç ‡∞Ö‡∞≤‡±ç‡∞ü‡±ç\\u200c‡∞Æ‡∞æ‡∞®‡±ç ‡∞ï‡±Ç‡∞°‡∞æ ‡∞™‡∞ø‡∞ö‡±à‡∞®‡∞ø ‡∞Ö‡∞≠‡∞ø‡∞®‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. ‡∞Ö‡∞≤‡±ç‡∞ü‡±ç\\u200c‡∞Æ‡∞æ‡∞®‡±ç ‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞µ‡±á‡∞ó‡∞µ‡∞Ç‡∞§‡∞Æ‡±à‡∞® ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞Ü‡∞Ø‡∞® ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞ö‡±Ä‡∞´‡±ç ‡∞è‡∞ï‡±ç‡∞∏‡±Ü‡∞ï‡±ç‡∞Ø‡±Ç‡∞ü‡∞ø‡∞µ‡±ç ‡∞∏‡∞æ‡∞Ç ‡∞Ö‡∞≤‡±ç‡∞ü‡±ç\\u200c‡∞Æ‡∞æ‡∞®‡±ç ‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞µ‡±á‡∞ó‡∞µ‡∞Ç‡∞§‡∞Æ‡±à‡∞® ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞µ‡±á‡∞ó‡∞µ‡∞Ç‡∞§‡∞Æ‡±à‡∞® ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å. \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï \\n\\n‡∞ì‡∞™‡±Ü‡∞®‡±ç ‡∞é‡∞Ø‡∞ø ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞≠‡∞ø‡∞µ‡±É‡∞¶‡±ç‡∞ß‡∞ø‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡∞æ‡∞°‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞∞‡±Å.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 2468, 'prompt_tokens': 477, 'total_tokens': 2945, 'completion_time': 2.664043795, 'prompt_time': 0.032640299, 'queue_time': 0.04882388, 'total_time': 2.696684094}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--bf831b92-7fcd-442c-aa8f-9bc25ca56d98-0', usage_metadata={'input_tokens': 477, 'output_tokens': 2468, 'total_tokens': 2945})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model\n",
    "\n",
    "summary = chain.invoke({\"speech\":speech, \"language\":\"telugu\"})\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e366b",
   "metadata": {},
   "source": [
    "Text Summarization Stuff Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1b3f733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 0, 'page_label': 'Cover'}, page_content='Mayo Oshin &  \\nNuno Campos\\n Learning \\nLangChain\\nBuilding AI and LLM Applications  \\nwith LangChain and LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 1, 'page_label': 'BackCover'}, page_content='9 781098 167288\\n57999\\nISBN:   978-1-098-16728-8\\nUS $79.99    CAN $99.99\\nDATA\\nIf you‚Äôre looking to build production-ready AI applications that can reason and retrieve external data for \\ncontext-awareness, you‚Äôll need to master LangChain‚Äîa popular development framework and platform \\nfor building, running, and managing agentic applications. LangChain is used by several leading companies, \\nincluding Zapier, Replit, Databricks, and many more. This guide is an indispensable resource for developers \\nwho understand Python or JavaScript but are beginners eager to harness the power of AI.\\nAuthors Mayo Oshin and Nuno Campos demystify the use of LangChain through practical insights and in-depth \\ntutorials. Starting with basic concepts, this book shows you step-by-step how to build a production-ready \\nAI agent that uses your data.\\n‚Ä¢ Harness the power of retrieval-augmented  \\ngeneration (RAG) to enhance the accuracy of  \\nLLMs using external up-to-date data\\n‚Ä¢ Develop and deploy AI applications that interact  \\nintelligently and contextually with users\\n‚Ä¢ Make use of the powerful agent architecture  \\nwith LangGraph\\n‚Ä¢ Integrate and manage third-party APIs and tools  \\nto extend the functionality of your AI applications\\n‚Ä¢ Monitor, test, and evaluate your AI applications  \\nto improve performance\\n‚Ä¢ Understand the foundations of LLM app development  \\nand how they can be used with LangChain\\n Learning LangChain\\n‚Äú With clear explanations and actionable techniques, this is the go-to resource \\nfor anyone looking to harness LangChain‚Äôs power for production-ready \\ngenerative AI and agents. A must-read for developers aiming to push the \\nboundaries of this platform.‚Äù\\nTom Taulli, IT consultant and author of AI-Assisted Programming\\n‚ÄúThis comprehensive guide covers everything from document retrieval and \\nindexing to deploying and monitoring AI agents in production. With engaging \\nexamples, intuitive illustrations, and hands-on code, this book made learning \\nLangChain interesting and fun!‚Äù\\nRajat K. Goel , senior software engineer, IBM\\nMayo Oshin is a tech entrepreneur,  \\nAI advisor, and angel investor. He \\nwas an early developer contributor \\nand advocate for the LangChain \\nopen source library and a pioneer \\nin the popular AI ‚Äúchat with data‚Äù \\nmovement.\\nNuno Campos is a founding software \\nengineer at LangChain. He has a \\ndecade of experience as a Python \\nand JavaScript software engineer, \\narchitect, and open source maintainer.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 2, 'page_label': 'i'}, page_content='Praise for Learning LangChain\\nWith clear explanations and actionable techniques, this is the go-to resource for anyone\\nlooking to harness LangChain‚Äôs power for production-ready generative AI and agents.\\nA must-read for developers aiming to push the boundaries of this platform.\\n‚ÄîTom Taulli, IT consultant and\\nauthor of AI-Assisted Programming\\nThis comprehensive guide on LangChain covers everything from document retrieval\\nand indexing to deploying and monitoring AI agents in production.\\nWith engaging examples, intuitive illustrations, and hands-on code,\\nthis book made learning LangChain interesting and fun!\\n‚ÄîRajat K. Goel, senior software engineer, IBM\\nThis book is a comprehensive LLM guide covering fundamentals to production,\\npacked with technical insights, practical strategies, and robust AI patterns.\\n‚ÄîGourav Singh Bais, senior data scientist and\\nsenior technical content writer, Allianz Services\\nPrototyping generative AI apps is easy‚Äîshipping them is hard. The\\nstrategies and tools in Learning LangChain make it possible to turn ideas\\ninto modern, production-ready applications.\\n‚ÄîJames Spiteri, director of product management\\nfor security, Elastic\\nLearning LangChain provides a clear path for transforming how you build AI-powered\\napplications. By breaking down flexible architectures and robust checkpointing, it offers a\\nstrong foundation for creating reliable, production-ready AI agents at scale.\\n‚ÄîDavid O‚ÄôRegan, engineering manager for AI/ML, GitLab'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 3, 'page_label': 'ii'}, page_content='Learning LangChain helped us skip the boilerplate for debugging and monitoring.\\nThe many helpful patterns and tooling insights allowed us to move fast\\nand deploy AI apps with confidence.\\n‚ÄîChris Focke, chief AI scientist, AppFolio\\nTeaching LangChain through clear, actionable examples, this book is a gateway to\\nagentic applications that are as inspiring as Asimov‚Äôs sci-fi novels.\\n‚Äî Ilya Meyzin, SVP head of data science, Dun & Bradstreet'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 4, 'page_label': 'iii'}, page_content='Mayo Oshin and Nuno Campos\\nLearning LangChain\\nBuilding AI and LLM Applications with\\nLangChain and LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 5, 'page_label': 'iv'}, page_content='978-1-098-16728-8\\n[LSI]\\nLearning LangChain\\nby Mayo Oshin and Nuno Campos\\nCopyright ¬© 2025 Olumayowa ‚ÄúMayo‚Äù Olufemi Oshin and O‚ÄôReilly Media, Inc. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O‚ÄôReilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO‚ÄôReilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\nAcquisitions Editor: Nicole Butterfield\\nDevelopment Editor: Corbin Collins\\nProduction Editor: Clare Laylock\\nCopyeditor: nSight, Inc.\\nProofreader: Helena Stirling\\nIndexer: Judith McConville\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\nFebruary 2025:  First Edition\\nRevision History for the First Edition\\n2025-02-13: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098167288 for release details.\\nThe O‚ÄôReilly logo is a registered trademark of O‚ÄôReilly Media, Inc. Learning LangChain, the cover image,\\nand related trade dress are trademarks of O‚ÄôReilly Media, Inc.\\nThe views expressed in this work are those of the authors and do not represent the publisher‚Äôs views.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\\nof or reliance on this work. Use of the information and instructions contained in this work is at your\\nown risk. If any code samples or other technology this work contains or describes is subject to open\\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 6, 'page_label': 'v'}, page_content='Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\\n1. LLM Fundamentals with LangChain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nGetting Set Up with LangChain                                                                                       3\\nUsing LLMs in LangChain                                                                                                4\\nMaking LLM Prompts Reusable                                                                                      7\\nGetting Specific Formats out of LLMs                                                                          13\\nJSON Output                                                                                                                 14\\nOther Machine-Readable Formats with Output Parsers                                        15\\nAssembling the Many Pieces of an LLM Application                                                 16\\nUsing the Runnable Interface                                                                                     16\\nImperative Composition                                                                                             18\\nDeclarative Composition                                                                                            20\\nSummary                                                                                                                           22\\n2. RAG Part I: Indexing Your Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  23\\nThe Goal: Picking Relevant Context for LLMs                                                            24\\nEmbeddings: Converting Text to Numbers                                                                 25\\nEmbeddings Before LLMs                                                                                           25\\nLLM-Based Embeddings                                                                                             27\\nSemantic Embeddings Explained                                                                               27\\nConverting Y our Documents into Text                                                                         30\\nSplitting Y our Text into Chunks                                                                                    32\\nGenerating Text Embeddings                                                                                         36\\nStoring Embeddings in a Vector Store                                                                          39\\nGetting Set Up with PGVector                                                                                   40\\nWorking with Vector Stores                                                                                        41\\nTracking Changes to Y our Documents                                                                         44\\nIndexing Optimization                                                                                                    48\\nv'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 7, 'page_label': 'vi'}, page_content='MultiVectorRetriever                                                                                                   49\\nRAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval        53\\nColBERT: Optimizing Embeddings                                                                           54\\nSummary                                                                                                                           56\\n3. RAG Part II: Chatting with Your Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  57\\nIntroducing Retrieval-Augmented Generation                                                           57\\nRetrieving Relevant Documents                                                                                 59\\nGenerating LLM Predictions Using Relevant Documents                                     63\\nQuery Transformation                                                                                                    68\\nRewrite-Retrieve-Read                                                                                                68\\nMulti-Query Retrieval                                                                                                 71\\nRAG-Fusion                                                                                                                  74\\nHypothetical Document Embeddings                                                                       78\\nQuery Routing                                                                                                                  81\\nLogical Routing                                                                                                             81\\nSemantic Routing                                                                                                         84\\nQuery Construction                                                                                                        87\\nText-to-Metadata Filter                                                                                               87\\nText-to-SQL                                                                                                                   90\\nSummary                                                                                                                           92\\n4. Using LangGraph to Add Memory to Your Chatbot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95\\nBuilding a Chatbot Memory System                                                                             96\\nIntroducing LangGraph                                                                                                  98\\nCreating a StateGraph                                                                                                   101\\nAdding Memory to StateGraph                                                                                   105\\nModifying Chat History                                                                                                107\\nTrimming Messages                                                                                                   107\\nFiltering Messages                                                                                                      110\\nMerging Consecutive Messages                                                                               112\\nSummary                                                                                                                         114\\n5. Cognitive Architectures with LangGraph. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  115\\nArchitecture #1: LLM Call                                                                                            118\\nArchitecture #2: Chain                                                                                                  121'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 7, 'page_label': 'vi'}, page_content='Architecture #2: Chain                                                                                                  121\\nArchitecture #3: Router                                                                                                125\\nSummary                                                                                                                         133\\n6. Agent Architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  135\\nThe Plan-Do Loop                                                                                                         136\\nBuilding a LangGraph Agent                                                                                       139\\nAlways Calling a Tool First                                                                                           143\\nvi | Table of Contents'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 8, 'page_label': 'vii'}, page_content='Dealing with Many Tools                                                                                              148\\nSummary                                                                                                                         153\\n7. Agents II. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\\nReflection                                                                                                                        155\\nSubgraphs in LangGraph                                                                                              161\\nCalling a Subgraph Directly                                                                                      162\\nCalling a Subgraph with a Function                                                                        164\\nMulti-Agent Architectures                                                                                           165\\nSupervisor Architecture                                                                                            167\\nSummary                                                                                                                         170\\n8. Patterns to Make the Most of LLMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  171\\nStructured Output                                                                                                         173\\nIntermediate Output                                                                                                  176\\nStreaming LLM Output Token-by-Token                                                              178\\nHuman-in-the-Loop Modalities                                                                              179\\nMultitasking LLMs                                                                                                     186\\nSummary                                                                                                                         189\\n9. Deployment: Launching Your AI Application into Production. . . . . . . . . . . . . . . . . . . . .  191\\nPrerequisites                                                                                                                   191\\nInstall Dependencies                                                                                                  192\\nLarge Language Model                                                                                              192\\nVector Store                                                                                                                 193\\nBackend API                                                                                                               197\\nCreate a LangSmith Account                                                                                    199\\nUnderstanding the LangGraph Platform API                                                           200\\nData Models                                                                                                                201\\nFeatures                                                                                                                        202\\nDeploying Y our AI Application on LangGraph Platform                                        204\\nCreate a LangGraph API Config                                                                              204\\nTest Y our LangGraph App Locally                                                                           205\\nDeploy from the LangSmith UI                                                                               207\\nLaunch LangGraph Studio                                                                                        210\\nSecurity                                                                                                                            213'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 8, 'page_label': 'vii'}, page_content='Security                                                                                                                            213\\nSummary                                                                                                                         214\\n10. Testing: Evaluation, Monitoring, and Continuous Improvement. . . . . . . . . . . . . . . . . .  215\\nTesting Techniques Across the LLM App Development Cycle                               216\\nThe Design Stage: Self-Corrective RAG                                                                     217\\nThe Preproduction Stage                                                                                              224\\nCreating Datasets                                                                                                       224\\nTable of Contents | vii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 9, 'page_label': 'viii'}, page_content='Defining Y our Evaluation Criteria                                                                           228\\nRegression Testing                                                                                                      234\\nEvaluating an Agent‚Äôs End-to-End Performance                                                   235\\nProduction                                                                                                                      247\\nTracing                                                                                                                         247\\nCollect Feedback in Production                                                                               248\\nClassification and Tagging                                                                                        249\\nMonitoring and Fixing Errors                                                                                  249\\nSummary                                                                                                                         250\\n11. Building with LLMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\\nInteractive Chatbots                                                                                                      252\\nCollaborative Editing with LLMs                                                                                254\\nAmbient Computing                                                                                                     255\\nSummary                                                                                                                         257\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  259\\nviii | Table of Contents'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 10, 'page_label': 'ix'}, page_content='Preface\\nOn November 30, 2022, San Francisco‚Äìbased firm OpenAI publicly released\\nChatGPT‚Äîthe viral AI chatbot that can generate content, answer questions, and\\nsolve problems like a human. Within two months of its launch, ChatGPT attracted\\nover 100 million monthly active users , the fastest adoption rate of a new consumer\\ntechnology application (so far). ChatGPT is a chatbot experience powered by an\\ninstruction and dialogue-tuned version of OpenAI‚Äôs GPT-3.5 family of large language\\nmodels (LLMs). We‚Äôll get to definitions of these concepts very shortly.\\nBuilding LLM applications with or without LangChain requires the\\nuse of an LLM. In this book we will be making use of the OpenAI\\nAPI as the LLM provider we use in the code examples (pricing\\nis listed on its platform). One of the benefits of working with\\nLangChain is that you can follow along with all of these examples\\nusing either OpenAI or alternative commercial or open source\\nLLM providers.\\nThree months later, OpenAI released the ChatGPT API , giving developers access\\nto the chat and speech-to-text capabilities. This kickstarted an uncountable number\\nof new applications and technical developments under the loose umbrella term  of\\ngenerative AI.\\nBefore we define generative AI and LLMs, let‚Äôs touch on the concept of machine learn‚Äê\\ning (ML). Some computer algorithms (imagine a repeatable recipe for achievement of\\nsome predefined task, such as sorting a deck of cards) are directly written by a software\\nengineer. Other computer algorithms are instead learned from vast amounts of training\\nexamples‚Äîthe job of the software engineer shifts from writing the algorithm itself to\\nwriting the training logic that creates the algorithm. A lot of attention in the ML field\\nwent into developing algorithms for predicting any number of things, from tomorrow‚Äôs\\nweather to the most efficient delivery route for an Amazon driver.\\nix'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 11, 'page_label': 'x'}, page_content='With the advent of LLMs and other generative models (such as diffusion models for\\ngenerating images, which we don‚Äôt cover in this book), those same ML techniques are\\nnow applied to the problem of generating new content, such as a new paragraph of\\ntext or drawing, that is at the same time unique and informed by examples in the\\ntraining data. LLMs in particular are generative models dedicated to generating text.\\nLLMs have two other differences from previous ML algorithms:\\n‚Ä¢ They are trained on much larger amounts of data; training one of these models‚Ä¢\\nfrom scratch would be very costly.\\n‚Ä¢ They are more versatile.‚Ä¢\\nThe same text generation model can be used for summarization, translation, classifi‚Äê\\ncation, and so forth, whereas previous ML models were usually trained and used for a\\nspecific task.\\nThese two differences conspire to make the job of the software engineer shift once\\nmore, with increasing amounts of time dedicated to working out how to get an LLM\\nto work for their use case. And that‚Äôs what LangChain is all about.\\nBy the end of 2023, competing LLMs emerged, including  Anthropic‚Äôs Claude and\\nGoogle‚Äôs Bard (later renamed Gemini), providing even wider access to these new\\ncapabilities. And subsequently, thousands of successful startups and major enterprises\\nhave incorporated generative AI APIs to build applications for various use cases,\\nranging from customer support chatbots to writing and debugging code.\\nOn October 22, 2022, Harrison Chase published the first commit  on GitHub for the\\nLangChain open source library. LangChain started from the realization that the most\\ninteresting LLM applications needed to use LLMs together with ‚Äúother sources of\\ncomputation or knowledge‚Äù. For instance, you can try to get an LLM to generate the\\nanswer to this question:\\nHow many balls are left after splitting 1,234 balls evenly among 123 people?\\nY ou‚Äôll likely be disappointed by its math prowess. However, if you pair it up with a\\ncalculator function, you can instead instruct the LLM to reword the question into an\\ninput that a calculator could handle:\\n1,234 % 123\\nThen you can pass that to a calculator function and get an accurate answer to your\\noriginal question. LangChain was the first (and, at the time of writing, the largest)\\nlibrary to provide such building blocks and the tooling to reliably combine them into\\nlarger applications. Before discussing what it takes to build compelling applications\\nwith these new tools, let‚Äôs get more familiar with LLMs and LangChain.\\nx | Preface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 12, 'page_label': 'xi'}, page_content='1 Tom B. Brown et al., ‚ÄúLanguage Models Are Few-Shot Learners‚Äù, arXiv, July 22, 2020.\\n2 Xiang Zhang et al., ‚ÄúDon‚Äôt Trust ChatGPT When Y our Question Is Not in English: A Study of Multilingual\\nAbilities and Types of LLMs‚Äù, Proceedings of the 2023 Conference on Empirical Methods in Natural Lan‚Äê\\nguage Processing, December 6‚Äì10, 2023.\\nBrief Primer on LLMs\\nIn layman‚Äôs terms, LLMs are trained algorithms that receive  text input and predict\\nand generate humanlike text output. Essentially, they behave like the familiar auto‚Äê\\ncomplete feature found on many smartphones, but taken to an extreme.\\nLet‚Äôs break down the term large language model:\\n‚Ä¢ Large refers to the size of these models in terms of training data and parameters‚Ä¢\\nused during the learning process. For example, OpenAI‚Äôs GPT-3 model contains\\n175 billion parameters, which were learned from training on 45 terabytes of text\\ndata.1 Parameters in a neural network model are made up of the numbers that\\ncontrol the output of each neuron and the relative weight of its connections with\\nits neighboring neurons. (Exactly which neurons are connected to which other\\nneurons varies for each neural network architecture and is beyond the scope of\\nthis book.)\\n‚Ä¢ Language model refers to a computer algorithm trained to receive written text‚Ä¢\\n(in English or other languages) and produce output also as written text (in the\\nsame language or a different one). These are neural networks , a type of ML\\nmodel which resembles a stylized conception of the human brain, with the final\\noutput resulting from the combination of the individual outputs of many simple\\nmathematical functions, called neurons, and their interconnections. If many of\\nthese neurons are organized in specific ways, with the right training process and\\nthe right training data, this produces a model that is capable of interpreting the\\nmeaning of individual words and sentences, which makes it possible to use them\\nfor generating plausible, readable, written text.\\nBecause of the prevalence of English in the training data, most models are better\\nat English than they are at other languages with a smaller number of speakers. By\\n‚Äúbetter‚Äù we mean it is easier to get them to produce desired outputs in English. There\\nare LLMs designed for multilingual output, such as BLOOM, that use a larger pro‚Äê\\nportion of training data in other languages. Curiously, the difference in performance\\nbetween languages isn‚Äôt as large as might be expected, even in LLMs trained on a\\npredominantly English training corpus. Researchers have found that LLMs are able to\\ntransfer some of their semantic understanding to other languages.2\\nPreface | xi'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 13, 'page_label': 'xii'}, page_content='Put together, large language models  are instances of big, general-purpose language\\nmodels that are trained on vast amounts of text. In other words, these models have\\nlearned from patterns in large datasets of text‚Äîbooks, articles, forums, and other\\npublicly available sources‚Äîto perform general text-related tasks. These tasks include\\ntext generation, summarization, translation, classification, and more.\\nLet‚Äôs say we instruct an LLM to complete the following sentence:\\nThe capital of England is _______.\\nThe LLM will take that input text and predict the correct output answer as London.\\nThis looks like magic, but it‚Äôs not. Under the hood, the LLM estimates the probability\\nof a sequence of word(s) given a previous sequence of words.\\nTechnically speaking, the model makes predictions based on\\ntokens, not words. A token represents an atomic unit of text.\\nTokens can represent individual characters, words, subwords, or\\neven larger linguistic units, depending on the specific tokenization\\napproach used. For example, using GPT-3.5‚Äôs tokenizer (called\\ncl100k), the phrase good morning dearest friend  would consist of\\nfive tokens (using _ to show the space character):\\nGood\\nWith token ID 19045\\n_morning\\nWith token ID 6693\\n_de\\nWith token ID 409\\narest\\nWith token ID 15795\\n_friend\\nWith token ID 4333\\nUsually tokenizers are trained with the objective of having the most\\ncommon words encoded into a single token, for example, the word\\nmorning is encoded as the token 6693. Less common words, or\\nwords in other languages (usually tokenizers are trained on English\\ntext), require several tokens to encode them. For example, the word\\ndearest is encoded as tokens 409, 15795. One token spans on\\naverage four characters of text for common English text, or roughly\\nthree quarters of a word.\\nxii | Preface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 14, 'page_label': 'xiii'}, page_content='3 For more information, see Ashish Vaswani et al., ‚Äú Attention Is All Y ou Need \", arXiv, June 12, 2017.\\nThe driving engine behind LLMs‚Äô predictive power is known as the transformer\\nneural network architecture.3 The transformer architecture enables models to handle\\nsequences of data, such as sentences or lines of code, and make predictions about the\\nlikeliest next word(s) in the sequence. Transformers are designed to understand the\\ncontext of each word in a sentence by considering it in relation to every other word.\\nThis allows the model to build a comprehensive understanding of the meaning of a\\nsentence, paragraph, and so on (in other words, a sequence of words) as the joint\\nmeaning of its parts in relation to each other.\\nSo, when the model sees the sequence of words  the capital of England is , it makes\\na prediction based on similar examples it saw during its training. In the model‚Äôs\\ntraining corpus the word England (or the token(s) that represent it) would have often\\nshown up in sentences in similar places to words like France, United States, China.\\nThe word capital would figure in the training data in many sentences also containing\\nwords like England, France, and US, and words like London, Paris, Washington. This\\nrepetition during the model‚Äôs training resulted in the capacity to correctly predict that\\nthe next word in the sequence should be London.\\nThe instructions and input text you provide to the model is called a  prompt. Prompt‚Äê\\ning can have a significant impact on the quality of output from the LLM. There are\\nseveral best practices for prompt design or prompt engineering, including providing\\nclear and concise instructions with contextual examples, which we discuss later in\\nthis book. Before we go further into prompting, let‚Äôs look at some different types of\\nLLMs available for you to use.\\nThe base type, from which all the others derive, is commonly known as a pretrained\\nLLM: it has been trained on very large amounts of text (found on the internet and in\\nbooks, newspapers, code, video transcripts, and so forth) in a self-supervised fashion.\\nThis means that‚Äîunlike in supervised ML, where prior to training the researcher\\nneeds to assemble a dataset of pairs of input to expected output ‚Äîfor LLMs those\\npairs are inferred from the training data. In fact, the only feasible way to use datasets\\nthat are so large is to assemble those pairs from the training data automatically. Two\\ntechniques to do this involve having the model do the following:\\nPredict the next word\\nRemove the last word from each sentence in the training data, and that yields\\na pair of input and expected output, such as The capital of England is ___  and\\nLondon.\\nPreface | xiii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 15, 'page_label': 'xiv'}, page_content='Predict a missing word\\nSimilarly, if you take each sentence and omit a word from the middle, you now\\nhave other pairs of input and expected output, such as The ___ of England is\\nLondon and capital.\\nThese models are quite difficult to use as is, they require you to prime the response\\nwith a suitable prefix. For instance, if you want to know the capital of England, you\\nmight get a response by prompting the model with The capital of England is, but not\\nwith the more natural What is the capital of England?\\nInstruction-Tuned LLMs\\nResearchers have made pretrained LLMs easier to use by further training (additional\\ntraining applied on top of the long and costly training described in the previous\\nsection), also known as fine-tuning them on the following:\\nTask-specific datasets\\nThese are datasets of pairs of questions/answers manually assembled by research‚Äê\\ners, providing examples of desirable responses to common questions that end\\nusers might prompt the model with. For example, the dataset might contain the\\nfollowing pair: Q: What is the capital of England? A: The capital of England is\\nLondon. Unlike the pretraining datasets, these are manually assembled, so they\\nare by necessity much smaller:\\nReinforcement learning from human feedback (RLHF)\\nThrough the use of RLHF methods, those manually assembled datasets are aug‚Äê\\nmented with user feedback received on output produced by the model. For\\nexample, user A preferred The capital of England is London  to London is the\\ncapital of England as an answer to the earlier question.\\nInstruction-tuning has been key to broadening the number of people who can build\\napplications with LLMs, as they can now be prompted with instructions, often in the\\nform of questions such as, What is the capital of England?, as opposed to The capital of\\nEngland is.\\nDialogue-Tuned LLMs\\nModels tailored for dialogue or chat purposes are a further enhancement  of\\ninstruction-tuned LLMs. Different providers of LLMs use different techniques, so\\nthis is not necessarily true of all chat models, but usually this is done via the following:\\nDialogue datasets\\nThe manually assembled fine-tuning datasets are extended to include more exam‚Äê\\nples of multiturn dialogue interactions, that is, sequences of prompt-reply pairs.\\nxiv | Preface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 16, 'page_label': 'xv'}, page_content='Chat format\\nThe input and output formats of the model are given a layer of structure over\\nfreeform text, which divides text into parts associated with a role (and option‚Äê\\nally other metadata like a name). Usually, the roles available are  system (for\\ninstructions and framing of the task), user (the actual task or question), and\\nassistant (for the outputs of the model). This method evolved from  early prompt\\nengineering techniques  and makes it easier to tailor the model‚Äôs output while\\nmaking it harder for models to confuse user input with instructions. Confusing\\nuser input with prior instructions is also known as jailbreaking, which can, for\\ninstance, lead to carefully crafted prompts, possibly including trade secrets, being\\nexposed to end users.\\nFine-Tuned LLMs\\nFine-tuned LLMs are created by taking base LLMs and further training them on\\na proprietary dataset for a specific task. Technically, instruction-tuned and dialogue-\\ntuned LLMs are fine-tuned LLMs, but the term ‚Äúfine-tuned LLM‚Äù is usually used to\\ndescribe LLMs that are tuned by the developer for their specific task. For example,\\na model can be fine-tuned to accurately extract the sentiment, risk factors, and key\\nfinancial figures from a public company‚Äôs annual report. Usually, fine-tuned models\\nhave improved performance on the chosen task at the expense of a loss of generality.\\nThat is, they become less capable of answering queries on unrelated tasks.\\nThroughout the rest of this book, when we use the term  LLM, we mean instruction-\\ntuned LLMs, and for chat model  we mean dialogue-instructed LLMs, as defined\\nearlier in this section. These should be your workhorses when using LLMs‚Äîthe first\\ntools you reach for when starting a new LLM application.\\nNow let‚Äôs quickly discuss some common LLM prompting techniques before diving\\ninto LangChain.\\nBrief Primer on Prompting\\nAs we touched on earlier, the main task of the software engineer working with\\nLLMs is not to train an LLM, or even to fine-tune one (usually), but rather to take\\nan existing LLM and work out how to get it to accomplish the task you need for\\nyour application. There are commercial providers of LLMs, like OpenAI, Anthropic,\\nand Google, as well as open source LLMs ( Llama, Gemma, and others), released\\nfree-of-charge for others to build upon. Adapting an existing LLM for your task is\\ncalled prompt engineering.\\nMany prompting techniques have been developed in the past two years, and in a\\nbroad sense, this is a book about how to do prompt engineering with LangChain‚Äî\\nhow to use LangChain to get LLMs to do what you have in mind. But before we get\\nPreface | xv'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 17, 'page_label': 'xvi'}, page_content=\"into LangChain proper, it helps to go over some of these techniques first (and we\\napologize in advance if your favorite prompting technique isn‚Äôt listed here; there are\\ntoo many to cover).\\nTo follow along with this section we recommend copying these prompts to the\\nOpenAI Playground to try them yourself:\\n1. Create an account for the OpenAI API at http://platform.openai.com, which will1.\\nlet you use OpenAI LLMs programmatically, that is, using the API from your\\nPython or JavaScript code. It will also give you access to the OpenAI Playground,\\nwhere you can experiment with prompts from your web browser.\\n2. If necessary, add payment details for your new OpenAI account. OpenAI is a2.\\ncommercial provider of LLMs and charges a fee for each time you use their\\nmodels through OpenAI‚Äôs API or through Playground. Y ou can find the latest\\npricing on their website. Over the past two years, the price for using OpenAI‚Äôs\\nmodels has come down significantly as new capabilities and optimizations are\\nintroduced.\\n3. Head on over to the OpenAI Playground and you‚Äôre ready to try out the follow‚Äê3.\\ning prompts for yourself. We‚Äôll make use of the OpenAI API throughout this\\nbook.\\n4. Once you‚Äôve navigated to the Playground, you will see a panel of presets on the4.\\nright side of the screen, including your model of choice. If you look further\\ndown the panel, you will see Temperature under the ‚ÄúModel configuration‚Äù title.\\nMove the Temperature toggle from middle to left until the number shows 0.00.\\nEssentially, temperature controls the randomness of LLM output. The lower the\\ntemperature, the more deterministic the model output.\\nNow on to the prompts!\\nZero-Shot Prompting\\nThe first and most straightforward prompting technique consists of simply instruct‚Äê\\ning the LLM to perform the desired task:\\nHow old was the 30th president of the United States when his wife's mother died?\\nThis is typically what you should try first, and it will usually work for simple ques‚Äê\\ntions, especially when the answer to it was likely present in some of the training data.\\nIf we prompt OpenAI‚Äôs gpt-3.5-turbo with the preceding prompt, the following is\\nreturned:\\nThe 30th president of the United States, Calvin Coolidge, was 48 years old when\\nhis wife's mother passed away in 1926.\\nxvi | Preface\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 18, 'page_label': 'xvii'}, page_content='4 Ryan Liu et al. ‚ÄúMind Y our Step (by Step): Chain-of-Thought Can Reduce Performance on Tasks Where\\nThinking Makes Humans Worse‚Äù, arXiv, November 8, 2024.\\nY ou may get a different result from what we get. There is an\\nelement of randomness to how LLMs generate responses, and\\nOpenAI may have updated the model by the time you try it.\\nWhile the model did correctly identify the 30th president, the answer isn‚Äôt quite right.\\nOften you‚Äôll have to iterate on prompts and responses to get a reliable system. The\\nnext sections expand on how.\\nChain-of-Thought\\nA very useful iteration is to further instruct the model to take the time to think. This\\ntechnique has been found to increase performance on a variety of tasks. Curiously,\\na more recent paper 4 suggests that this technique can also reduce LLM performance\\non tasks where thinking reduces human performance. Called chain-of-thought (CoT)\\nprompting, this is usually done by prepending the prompt with instructions for the\\nLLM to describe how it could arrive at the answer:\\nThink step by step.\\nHow old was the 30th president of the United States when his wife\\'s mother died?\\nAnd now let‚Äôs see what the same model produces for this new prompt:\\n1. First, find out who the 30th president of the United States was. This can be\\ndone by researching a list of US presidents or by searching for \"30th president \\nof the United States\" online. You will find that the 30th president was Calvin \\nCoolidge.\\n2. Next, find out the year when Calvin Coolidge was the president. This can also \\nbe found by researching online or in a history book. You will find that he was \\nthe president from 1923-1929.\\n3. Now, you need to determine the year when Calvin Coolidge\\'s wife\\'s mother died. \\nThis can be done by researching the life and death of his wife, Grace Goodhue \\nCoolidge. You will find that she lived from 1879-1957.\\n4. Calculate the age of Calvin Coolidge\\'s wife\\'s mother when she died. She was \\n78 years old.\\n5. Determine the age of Calvin Coolidge when his wife\\'s mother died. Subtract \\nthe year of his birth, 1872, from the year his mother-in-law died, 1957. This \\nmeans that Calvin Coolidge was 85 years old when his wife\\'s mother passed away.\\nPreface | xvii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 19, 'page_label': 'xviii'}, page_content=\"Notice how the only change in the prompt was the addition of Think step by\\nstep, and how much of a difference that made in the model‚Äôs output. Now the\\nmodel outputs a sequence of steps you could go through when trying to answer this\\nquestion. Unfortunately, while the steps are very reasonable, the final output is still\\nincorrect, given it got some facts wrong, namely it listed the wrong birth and death\\nyears for President Coolidge‚Äôs wife‚Äôs mother. Let‚Äôs see how to improve on this.\\nRetrieval-Augmented Generation\\nRetrieval-augmented generation (RAG) consists of finding relevant pieces of text, also\\nknown as context, such as facts you‚Äô d find in an encyclopedia and including that\\ncontext in the prompt. The RAG technique can (and in real applications should) be\\ncombined with CoT, but for simplicity we‚Äôll use these techniques one at a time here.\\nHere‚Äôs the prompt including RAG:\\nContext:\\n- Calvin Coolidge (born John Calvin Coolidge Jr.; /ÀàkuÀêl…™d í/; July 4, 1872 ‚Äì \\nJanuary 5, 1933) was an American attorney and politician who served as the \\n30th president of the United States from 1923 to 1929.\\n- Grace Anna Coolidge (n√©e Goodhue; January 3, 1879 ‚Äì July 8, 1957) was the\\nwife of the 30th president of the United States, Calvin Coolidge.\\n- Grace Anna Goodhue was born on January 3, 1879, in Burlington, Vermont, the \\nonly child of Andrew Issachar Goodhue and Lemira Barrett Goodhue.\\n- Lemira A. Goodhue (Barrett) ; Birthdate: April 26, 1849 ; Birthplace: \\nBurlington, Chittenden County, VT, United States ; Death: October 24, 1929.\\nHow old was the 30th president of the United States when his wife's mother died?\\nAnd the output from the model:\\nThe 30th president of the United States, Calvin Coolidge, was 54 years old when \\nhis wife's mother, Lemira A. Goodhue, died on October 24, 1929.\\nNow we‚Äôre a lot closer to the correct answer, but as we touched on earlier, LLMs aren‚Äôt\\ngreat at out-of-the-box math. In this case, the final result of 54 years old is off by 3.\\nLet‚Äôs see how we can improve on this.\\nTool Calling\\nThe tool calling technique consists of prepending the prompt with a list of external\\nfunctions the LLM can make use of, along with descriptions of what each is good for\\nand instructions on how to signal in the output that it wants to use one (or more)\\nof these functions. Finally, you‚Äîthe developer of the application‚Äîshould parse the\\noutput and call the appropriate functions. Here‚Äôs one way to do this:\\nxviii | Preface\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 20, 'page_label': 'xix'}, page_content='Tools:\\n- calculator: This tool accepts math expressions and returns their result.\\n- search: This tool accepts search engine queries and returns the first search \\nresult.\\nIf you want to use tools to arrive at the answer, output the list of tools and\\ninputs in CSV format, with this header row `tool,input`.\\nHow old was the 30th president of the United States when his wife\\'s mother died?\\nAnd this is the output you might get:\\ntool,input\\ncalculator,2023-1892\\nsearch,\"What age was Calvin Coolidge when his mother-in-law died?\"\\nWhile the LLM correctly followed the output format instructions, the tools and\\ninputs selected aren‚Äôt the most appropriate for this question. This gets at one of\\nthe most important things to keep in mind when prompting LLMs: each prompting\\ntechnique is most useful when used in combination with (some of) the others . For\\ninstance, here we could improve on this by combining tool calling, chain-of-thought,\\nand RAG into a prompt that uses all three. Let‚Äôs see what that looks like:\\nContext:\\n- Calvin Coolidge (born John Calvin Coolidge Jr.; /ÀàkuÀêl…™d í/; July 4, 1872 ‚Äì\\nJanuary 5, 1933) was an American attorney and politician who served as the 30th\\npresident of the United States from 1923 to 1929.\\n- Grace Anna Coolidge (n√©e Goodhue; January 3, 1879 ‚Äì July 8, 1957) was the wife \\nof the 30th president of the United States, Calvin Coolidge.\\n- Grace Anna Goodhue was born on January 3, 1879, in Burlington, Vermont, the \\nonly child of Andrew Issachar Goodhue and Lemira Barrett Goodhue.\\n- Lemira A. Goodhue (Barrett) ; Birthdate: April 26, 1849 ; Birthplace: \\nBurlington, Chittenden County, VT, United States ; Death: October 24, 1929.\\nTools:\\n- calculator: This tool accepts math expressions and returns their result.\\nIf you want to use tools to arrive at the answer, output the list of tools and \\ninputs in CSV format, with this header row `tool,input`.\\nThink step by step.\\nHow old was the 30th president of the United States when his wife\\'s mother died?\\nPreface | xix'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 21, 'page_label': 'xx'}, page_content='And with this prompt, maybe after a few tries, we might get this output:\\ntool,input\\ncalculator,1929 - 1872\\nIf we parse that CSV output, and have a calculator function execute the operation\\n1929 - 1872, we finally get the right answer: 57 years.\\nAs per the previous example, by combining RAG with chain-of-thought and tool\\ncalling, you can retrieve the most relevant data to ground your model‚Äôs output, then\\nguide it step by step to ensure it uses that context effectively.\\nFew-Shot Prompting\\nFinally, we come to another very useful prompting technique: few-shot prompting.\\nThis consists of providing the LLM with examples of other questions and the correct\\nanswers, which enables the LLM to learn how to perform a new task without going\\nthrough additional training or fine-tuning. When compared to fine-tuning, few-shot\\nprompting is more flexible‚Äîyou can do it on the fly at query time‚Äîbut less pow‚Äê\\nerful, and you might achieve better performance with fine-tuning. That said, you\\nshould usually always try few-shot prompting before fine-tuning:\\nStatic few-shot prompting\\nThe most basic version of few-shot prompting is to assemble a predetermined list\\nof a small number of examples that you include in the prompt.\\nDynamic few-shot prompting\\nIf you assemble a dataset of many examples, you can instead pick the most\\nrelevant examples for each new query.\\nThe next section covers using LangChain to build applications using LLMs and these\\nprompting techniques.\\nLangChain and Why It‚Äôs Important\\nLangChain was one of the earliest open source libraries to provide LLM and prompt‚Äê\\ning building blocks and the tooling to reliably combine them into larger applications.\\nAs of writing, LangChain has amassed over 28 million monthly downloads , 99,000\\nGitHub stars, and the largest developer community in generative AI (72,000+ strong).\\nIt has enabled software engineers who don‚Äôt have an ML background to utilize the\\npower of LLMs to build a variety of apps, ranging from AI chatbots to AI agents that\\ncan reason and take action responsibly.\\nLangChain builds on the idea stressed in the preceding section: that prompting\\ntechniques are most useful when used together. To make that easier, LangChain\\nprovides simple abstractions for each major prompting technique. By abstraction we\\nxx | Preface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 22, 'page_label': 'xxi'}, page_content='mean Python and JavaScript functions and classes that encapsulate the ideas of those\\ntechniques into easy-to-use wrappers. These abstractions are designed to play well\\ntogether and to be combined into a larger LLM application.\\nFirst of all, LangChain provides integrations with the major LLM providers, both\\ncommercial (OpenAI, Anthropic, Google, and more) and open source (Llama,\\nGemma, and others). These integrations share a common interface, making it very\\neasy to try out new LLMs as they‚Äôre announced and letting you avoid being locked-in\\nto a single provider. We‚Äôll use these in Chapter 1.\\nLangChain also provides prompt template abstractions, which enable you to reuse\\nprompts more than once, separating static text in the prompt from placeholders that\\nwill be different for each time you send it to the LLM to get a completion generated.\\nWe‚Äôll talk more about these also in Chapter 1. LangChain prompts can also be stored\\nin the LangChain Hub for sharing with teammates.\\nLangChain contains many integrations with third-party services (such as Google\\nSheets, Wolfram Alpha, Zapier, just to name a few) exposed as tools, which is a\\nstandard interface for functions to be used in the tool-calling technique.\\nFor RAG, LangChain provides integrations with the major embedding models (lan‚Äê\\nguage models designed to output a numeric representation, the embedding, of the\\nmeaning of a sentence, paragraph, and so on), vector stores  (databases dedicated\\nto storing embeddings), and vector indexes  (regular databases with vector-storing\\ncapabilities). Y ou‚Äôll learn a lot more about these in Chapters 2 and 3.\\nFor CoT, LangChain (through the LangGraph library) provides agent abstractions\\nthat combine chain-of-thought reasoning and tool calling, first popularized by  the\\nReAct paper. This enables building LLM applications that do the following:\\n1. Reason about the steps to take.1.\\n2. Translate those steps into external tool calls.2.\\n3. Receive the output of those tool calls.3.\\n4. Repeat until the task is accomplished.4.\\nWe cover these in Chapters 5 through 8.\\nFor chatbot use cases, it becomes useful to keep track of previous interactions and\\nuse them when generating the response to a future interaction. This is called memory,\\nand Chapter 4 discusses using it in LangChain.\\nPreface | xxi'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 23, 'page_label': 'xxii'}, page_content='Finally, LangChain provides the tools to compose these building blocks into cohesive\\napplications. Chapters 1 through 6 talk more about this.\\nIn addition to this library, LangChain provides LangSmith‚Äîa platform to help\\ndebug, test, deploy, and monitor AI workflows‚Äîand LangGraph Platform‚Äîa plat‚Äê\\nform for deploying and scaling LangGraph agents. We cover these in Chapters 9\\nand 10.\\nWhat to Expect from This Book\\nWith this book, we hope to convey the excitement and possibility of adding LLMs to\\nyour software engineering toolbelt.\\nWe got into programming because we like building things, getting to the end of a\\nproject, looking at the final product and realizing there‚Äôs something new out there,\\nand we built it. Programming with LLMs is so exciting to us because it expands\\nthe set of things we can build, it makes previously hard things easy (for example,\\nextracting relevant numbers from a long text) and previously impossible things\\npossible‚Äîtry building an automated assistant a year ago and you end up with the\\nphone tree hell we all know and love from calling up customer support numbers.\\nNow with LLMs and LangChain, you can actually build pleasant assistants (or myriad\\nother applications) that chat with you and understand your intent to a very reason‚Äê\\nable degree. The difference is night and day! If that sounds exciting to you (as it does\\nto us) then you‚Äôve come to the right place.\\nIn this Preface, we‚Äôve given you a refresher on what makes LLMs tick and why exactly\\nthat gives you ‚Äúthing-building‚Äù superpowers. Having these very large ML models\\nthat understand language and can output answers written in conversational English\\n(or some other language) gives you a programmable (through prompt engineering),\\nversatile language-generation tool. By the end of the book, we hope you‚Äôll see just\\nhow powerful that can be.\\nWe‚Äôll begin with an AI chatbot customized by, for the most part, plain English\\ninstructions. That alone should be an eye-opener: you can now ‚Äúprogram‚Äù part of the\\nbehavior of your application without code.\\nThen comes the next capability: giving your chatbot access to your own documents,\\nwhich takes it from a generic assistant to one that‚Äôs knowledgeable about any area of\\nhuman knowledge for which you can find a library of written text. This will allow you\\nto have the chatbot answer questions or summarize documents you wrote, for instance.\\nAfter that, we‚Äôll make the chatbot remember your previous conversations. This will\\nimprove it in two ways: It will feel a lot more natural to have a conversation with a\\nchatbot that remembers what you have previously chatted about, and over time the\\nchatbot can be personalized to the preferences of each of its users individually.\\nxxii | Preface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 24, 'page_label': 'xxiii'}, page_content='Next, we‚Äôll use chain-of-thought and tool-calling techniques to give the chatbot the\\nability to plan and act on those plans, iteratively. This will enable it to work toward\\nmore complicated requests, such as writing a research report about a subject of your\\nchoice.\\nAs you use your chatbot for more complicated tasks, you‚Äôll feel the need to give it\\nthe tools to collaborate with you. This encompasses both giving you the ability to\\ninterrupt or authorize actions before they are taken, as well as providing the chatbot\\nwith the ability to ask for more information or clarification before acting.\\nFinally, we‚Äôll show you how to deploy your chatbot to production and discuss what\\nyou need to consider before and after taking that step, including latency, reliability,\\nand security. Then we‚Äôll show you how to monitor your chatbot in production and\\ncontinue to improve it as it is used.\\nAlong the way, we‚Äôll teach you the ins and outs of each of these techniques, so that\\nwhen you finish the book, you will have truly added a new tool (or two) to your\\nsoftware engineering toolbelt.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program\\nelements such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nThis element signifies a tip or suggestion.\\nThis element signifies a general note.\\nPreface | xxiii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 25, 'page_label': 'xxiv'}, page_content='Using Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://oreil.ly/supp-LearningLangChain.\\nIf you have a technical question or a problem using the code examples, please send\\nemail to support@oreilly.com.\\nThis book is here to help you get your job done. In general, if example code is\\noffered with this book, you may use it in your programs and documentation. Y ou\\ndo not need to contact us for permission unless you‚Äôre reproducing a significant\\nportion of the code. For example, writing a program that uses several chunks of code\\nfrom this book does not require permission. Selling or distributing examples from\\nO‚ÄôReilly books does require permission. Answering a question by citing this book\\nand quoting example code does not require permission. Incorporating a significant\\namount of example code from this book into your product‚Äôs documentation does\\nrequire permission.\\nWe appreciate, but generally do not require, attribution. An attribution usually\\nincludes the title, author, publisher, and ISBN. For example: ‚Äú Learning LangChain\\nby Mayo Oshin and Nuno Campos (O‚ÄôReilly). Copyright 2025 Olumayowa ‚ÄúMayo‚Äù\\nOlufemi Oshin, 978-1-098-16728-8. ‚Äù\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nO‚ÄôReilly Online Learning\\nFor more than 40 years, O‚ÄôReilly Media has provided technol‚Äê\\nogy and business training, knowledge, and insight to help\\ncompanies succeed.\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O‚ÄôReilly‚Äôs online learning\\nplatform gives you on-demand access to live training courses, in-depth learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO‚ÄôReilly and 200+ other publishers. For more information, visit https://oreilly.com.\\nxxiv | Preface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 26, 'page_label': 'xxv'}, page_content='How to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO‚ÄôReilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-889-8969 (in the United States or Canada)\\n707-827-7019 (international or local)\\n707-829-0104 (fax)\\nsupport@oreilly.com\\nhttps://oreilly.com/about/contact.html\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at https://oreil.ly/learning-langchain.\\nFor news and information about our books and courses, visit https://oreilly.com.\\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media.\\nWatch us on Y ouTube: https://youtube.com/oreillymedia.\\nAcknowledgments\\nWe would like to express our gratitude and appreciation to the reviewers‚ÄîRajat\\nKant Goel, Douglas Bailley, Tom Taulli, Gourav Bais, and Jacob Lee‚Äîfor providing\\nvaluable technical feedback on improving this book.\\nPreface | xxv'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 28, 'page_label': '1'}, page_content='CHAPTER 1\\nLLM Fundamentals with LangChain\\nThe Preface gave you a taste of the power of LLM prompting, where we saw firsthand\\nthe impact that different prompting techniques can have on what you get out of\\nLLMs, especially when judiciously combined. The challenge in building good LLM\\napplications is, in fact, in how to effectively construct the prompt sent to the model\\nand process the model‚Äôs prediction to return an accurate output (see Figure 1-1).\\nFigure 1-1. The challenge in making LLMs a useful part of your application\\nIf you can solve this problem, you are well on your way to building LLM applications,\\nsimple and complex alike. In this chapter, you‚Äôll learn more about how LangChain‚Äôs\\nbuilding blocks map to LLM concepts and how, when combined effectively, they\\nenable you to build LLM applications. But first, the sidebar ‚ÄúWhy LangChain?‚Äù is a\\nbrief primer on why we think it useful to use LangChain to build LLM applications.\\nWhy LangChain?\\nY ou can of course build LLM applications without LangChain. The most obvious\\nalternative is to use the software development kit (SDK)‚Äîthe package exposing the\\nmethods of their HTTP API as functions in the programming language of your\\nchoice‚Äîof the LLM provider you tried first (for example, OpenAI). We think learn‚Äê\\ning LangChain will pay off in the short term and over the long run because of the\\nfollowing factors:\\n1'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 29, 'page_label': '2'}, page_content='Prebuilt common patterns\\nLangChain comes with reference implementations of the most common LLM\\napplication patterns (we mentioned some of these in the Preface: chain-of-\\nthought, tool calling, and others). This is the quickest way to get started with\\nLLMs and might often be all you need. We‚Äô d suggest starting any new application\\nfrom these and checking whether the results out of the box are good enough for\\nyour use case. If not, then see the next item for the other half of the LangChain\\nlibraries.\\nInterchangeable building blocks\\nThese are components that can be easily swapped out for alternatives. Every\\ncomponent (an LLM, chat model, output parser, and so on‚Äîmore on these\\nshortly) follows a shared specification, which makes your application future-\\nproof. As new capabilities are released by model providers and as your needs\\nchange, you can evolve your application without rewriting it each time.\\nThroughout this book we make use of the following major components in the code\\nexamples:\\n‚Ä¢ LLM/chat model: OpenAI‚Ä¢\\n‚Ä¢ Embeddings: OpenAI‚Ä¢\\n‚Ä¢ Vector store: PGVector‚Ä¢\\nY ou can swap out each of these for any of the alternatives listed on the following\\npages:\\nChat models\\nSee the LangChain documentation. If you don‚Äôt want to use OpenAI (a commer‚Äê\\ncial API) we suggest Anthropic as a commercial alternative or Ollama as an open\\nsource one.\\nEmbeddings\\nSee the LangChain documentation. If you don‚Äôt want to use OpenAI (a commer‚Äê\\ncial API) we suggest Cohere as a commercial alternative or Ollama as an open\\nsource one.\\nVector stores\\nSee the LangChain documentation. If you don‚Äôt want to use PGVector (an open\\nsource extension to the popular SQL database Postgres) we suggest using either\\nWeaviate (a dedicated vector store) or OpenSearch (vector search features that\\nare part of a popular search database).\\nThis effort goes beyond, for instance, all LLMs having the same methods, with similar\\narguments and return values. Let‚Äôs look at the example of chat models and two\\npopular LLM providers, OpenAI and Anthropic. Both have a chat API which receives\\nchat messages (loosely defined as objects with a type string and a content string) and\\nreturns a new message generated by the model. But if you try to use both models\\n2 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 30, 'page_label': '3'}, page_content='in the same conversation, you‚Äôll immediately run into issues, as their chat message\\nformats are subtly incompatible. LangChain abstracts away these differences to enable\\nbuilding applications that are truly independent of a particular provider. For instance,\\nwith LangChain, a chatbot conversation where you use both OpenAI and Anthropic\\nmodels works.\\nFinally, as you build out your LLM applications with several of these components,\\nwe‚Äôve found it useful to have the orchestration capabilities of LangChain:\\n‚Ä¢ All major components are instrumented by the callbacks system for observability‚Ä¢\\n(more on this in Chapter 8).\\n‚Ä¢ All major components implement the same interface (more on this toward the‚Ä¢\\nend of this chapter).\\n‚Ä¢ Long-running LLM applications can be interrupted, resumed, or retried (more‚Ä¢\\non this in Chapter 6).\\nGetting Set Up with LangChain\\nTo follow along with the rest of the chapter, and the chapters to come, we recommend\\nsetting up LangChain on your computer first.\\nSee the instructions in the Preface regarding setting up an  OpenAI account and\\ncomplete these if you haven‚Äôt yet. If you prefer using a different LLM provider, see\\n‚ÄúWhy LangChain?‚Äù on page 1 for alternatives.\\nThen head over to the API Keys page on the OpenAI website (after logging in to your\\nOpenAI account), create an API key, and save it‚Äîyou‚Äôll need it soon.\\nIn this book, we‚Äôll show code examples in both Python and Java‚Äê\\nScript (JS). LangChain offers the same functionality in both lan‚Äê\\nguages, so just pick the one you‚Äôre most comfortable with and\\nfollow the respective code snippets throughout the book (the code\\nexamples for each language are equivalent).\\nFirst, some setup instructions for readers using Python:\\n1. Ensure that you have Python installed. See the instructions for your operating1.\\nsystem.\\n2. Install Jupyter if you want to run the examples in a notebook environment. Y ou2.\\ncan do this by running pip install notebook in your terminal.\\nGetting Set Up with LangChain | 3'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 31, 'page_label': '4'}, page_content='3. Install the LangChain library by running the following commands in your3.\\nterminal:\\npip install langchain langchain-openai langchain-community \\npip install langchain-text-splitters langchain-postgres\\n4. Take the OpenAI API key you generated at the beginning of this section and4.\\nmake it available in your terminal environment. Y ou can do this by running the\\nfollowing:\\nexport OPENAI_API_KEY=your-key\\n5. Don‚Äôt forget to replace your-key with the API key you generated previously.5.\\n6. Open a Jupyter notebook by running this command:6.\\njupyter notebook\\nY ou‚Äôre now ready to follow along with the Python code examples.\\nHere are the instructions for readers using JavaScript:\\n1. Take the OpenAI API key you generated at the beginning of this section and1.\\nmake it available in your terminal environment. Y ou can do this by running the\\nfollowing:\\nexport OPENAI_API_KEY=your-key\\n2. Don‚Äôt forget to replace your-key with the API key you generated previously.2.\\n3. If you want to run the examples as Node.js scripts, install Node by following the3.\\ninstructions.\\n4. Install the LangChain libraries by running the following commands in your4.\\nterminal:\\nnpm install langchain @langchain/openai @langchain/community\\nnpm install @langchain/core pg\\n5. Take each example, save it as a .js file and run it with node ./file.js.5.\\nUsing LLMs in LangChain\\nTo recap, LLMs are the driving engine behind most generative AI applications.\\nLangChain provides two simple interfaces to interact with any LLM API provider:\\n‚Ä¢ Chat models‚Ä¢\\n‚Ä¢ LLMs‚Ä¢\\nThe LLM interface simply takes a string prompt as input, sends the input to the\\nmodel provider, and then returns the model prediction as output.\\n4 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 32, 'page_label': '5'}, page_content='Let‚Äôs import LangChain‚Äôs OpenAI LLM wrapper to invoke a model prediction using a\\nsimple prompt:\\nPython\\nfrom langchain_openai.llms import OpenAI\\nmodel = OpenAI(model=\"gpt-3.5-turbo\")\\nmodel.invoke(\"The sky is\")\\nJavaScript\\nimport { OpenAI } from \"@langchain/openai\";\\nconst model = new OpenAI({ model: \"gpt-3.5-turbo\" });\\nawait model.invoke(\"The sky is\");\\nThe output:\\nBlue!\\nNotice the parameter model passed to OpenAI. This is the most com‚Äê\\nmon parameter to configure when using an LLM or chat model, the\\nunderlying model to use, as most providers offer several models with\\ndifferent trade-offs in capability and cost (usually larger models are\\nmore capable, but also more expensive and slower). See OpenAI‚Äôs\\noverview of the models they offer.\\nOther useful parameters to configure include the following, offered\\nby most providers.\\ntemperature\\nThis controls the sampling algorithm used to generate output.\\nLower values produce more predictable outputs (for example,\\n0.1), while higher values generate more creative, or unexpec‚Äê\\nted, results (such as 0.9). Different tasks will need different\\nvalues for this parameter. For instance, producing structured\\noutput usually benefits from a lower temperature, whereas\\ncreative writing tasks do better with a higher value:\\nmax_tokens\\nThis limits the size (and cost) of the output. A lower value may\\ncause the LLM to stop generating the output before getting to\\na natural end, so it may appear to have been truncated.\\nBeyond these, each provider exposes a different set of parameters.\\nWe recommend looking at the documentation for the one you\\nchoose. For an example, refer to OpenAI‚Äôs platform.\\nUsing LLMs in LangChain | 5'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 33, 'page_label': '6'}, page_content='Alternatively, the chat model interface enables back and forth conversations between\\nthe user and model. The reason why it‚Äôs a separate interface is because popular LLM\\nproviders like OpenAI differentiate messages sent to and from the model into user,\\nassistant, and system roles (here role denotes the type of content the message contains):\\nSystem role\\nUsed for instructions the model should use to answer a user question\\nUser role\\nUsed for the user‚Äôs query and any other content produced by the user\\nAssistant role\\nUsed for content generated by the model\\nThe chat model‚Äôs interface makes it easier to configure and manage conversions in your\\nAI chatbot application. Here‚Äôs an example utilizing LangChain‚Äôs ChatOpenAI model:\\nPython\\nfrom langchain_openai.chat_models import ChatOpenAI\\nfrom langchain_core.messages import HumanMessage\\nmodel = ChatOpenAI()\\nprompt = [HumanMessage(\"What is the capital of France?\")]\\nmodel.invoke(prompt)\\nJavaScript\\nimport { ChatOpenAI } from \\'@langchain/openai\\'\\nimport { HumanMessage } from \\'@langchain/core/messages\\'\\nconst model = new ChatOpenAI()\\nconst prompt = [new HumanMessage(\\'What is the capital of France?\\')]\\nawait model.invoke(prompt)\\nThe output:\\nAIMessage(content=\\'The capital of France is Paris.\\')\\nInstead of a single prompt string, chat models make use of different types of chat\\nmessage interfaces associated with each role mentioned previously. These include the\\nfollowing:\\nHumanMessage\\nA message sent from the perspective of the human, with the user role\\nAIMessage\\nA message sent from the perspective of the AI that the human is interacting with,\\nwith the assistant role\\n6 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 34, 'page_label': '7'}, page_content='SystemMessage\\nA message setting the instructions the AI should follow, with the system role\\nChatMessage\\nA message allowing for arbitrary setting of role\\nLet‚Äôs incorporate a SystemMessage instruction in our example:\\nPython\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\nfrom langchain_openai.chat_models import ChatOpenAI\\nmodel = ChatOpenAI()\\nsystem_msg = SystemMessage(\\n    \\'\\'\\'You are a helpful assistant that responds to questions with three \\n        exclamation marks.\\'\\'\\'\\n)\\nhuman_msg = HumanMessage(\\'What is the capital of France?\\')\\nmodel.invoke([system_msg, human_msg])\\nJavaScript\\nimport { ChatOpenAI } from \"@langchain/openai\";\\nimport { HumanMessage, SystemMessage } from \"@langchain/core/messages\";\\nconst model = new ChatOpenAI();\\nconst prompt = [\\n  new SystemMessage(\\n    `You are a helpful assistant that responds to questions with three \\n      exclamation marks.`,\\n  ),\\n  new HumanMessage(\"What is the capital of France?\"),\\n];\\nawait model.invoke(prompt);\\nThe output:\\nAIMessage(\\'Paris!!!\\')\\nAs you can see, the model obeyed the instruction provided in the SystemMessage even\\nthough it wasn‚Äôt present in the user‚Äôs question. This enables you to preconfigure your\\nAI application to respond in a relatively predictable manner based on the user‚Äôs input.\\nMaking LLM Prompts Reusable\\nThe previous section showed how the prompt instruction significantly influences the\\nmodel‚Äôs output. Prompts help the model understand context and generate relevant\\nanswers to queries.\\nMaking LLM Prompts Reusable | 7'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 35, 'page_label': '8'}, page_content='Here is an example of a detailed prompt:\\nAnswer the question based on the context below. If the question cannot be\\nanswered using the information provided, answer with \"I don\\'t know\".\\nContext: The most recent advancements in NLP are being driven by Large Language \\nModels (LLMs). These models outperform their smaller counterparts and have\\nbecome invaluable for developers who are creating applications with NLP \\ncapabilities. Developers can tap into these models through Hugging Face\\'s\\n`transformers` library, or by utilizing OpenAI and Cohere\\'s offerings through\\nthe `openai` and `cohere` libraries, respectively.\\nQuestion: Which model providers offer LLMs?\\nAnswer:\\nAlthough the prompt looks like a simple string, the challenge is figuring out what the\\ntext should contain and how it should vary based on the user‚Äôs input. In this example,\\nthe Context and Question values are hardcoded, but what if we wanted to pass these\\nin dynamically?\\nFortunately, LangChain provides prompt template interfaces that make it easy to\\nconstruct prompts with dynamic inputs:\\nPython\\nfrom langchain_core.prompts import PromptTemplate\\ntemplate = PromptTemplate.from_template(\"\"\"Answer the question based on the\\n    context below. If the question cannot be answered using the information \\n    provided, answer with \"I don\\'t know\".\\nContext: {context}\\nQuestion: {question}\\nAnswer: \"\"\")\\ntemplate.invoke({\\n    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \\n        Language Models (LLMs). These models outperform their smaller \\n        counterparts and have become invaluable for developers who are creating \\n        applications with NLP capabilities. Developers can tap into these \\n        models through Hugging Face\\'s `transformers` library, or by utilizing \\n        OpenAI and Cohere\\'s offerings through the `openai` and `cohere` \\n        libraries, respectively.\"\"\",\\n    \"question\": \"Which model providers offer LLMs?\"\\n})\\n8 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 36, 'page_label': '9'}, page_content='JavaScript\\nimport { PromptTemplate } from \\'@langchain/core/prompts\\'\\nconst template = PromptTemplate.fromTemplate(`Answer the question based on the \\n  context below. If the question cannot be answered using the information \\n  provided, answer with \"I don\\'t know\".\\nContext: {context}\\nQuestion: {question}\\nAnswer: `)\\nawait template.invoke({\\n  context: `The most recent advancements in NLP are being driven by Large \\n    Language Models (LLMs). These models outperform their smaller \\n    counterparts and have become invaluable for developers who are creating \\n    applications with NLP capabilities. Developers can tap into these models \\n    through Hugging Face\\'s \\\\`transformers\\\\` library, or by utilizing OpenAI \\n    and Cohere\\'s offerings through the \\\\`openai\\\\` and \\\\`cohere\\\\` libraries, \\n    respectively.`,\\n  question: \"Which model providers offer LLMs?\"\\n})\\nThe output:\\nStringPromptValue(text=\\'Answer the question based on the context below. If the \\n    question cannot be answered using the information provided, answer with \"I\\n    don\\\\\\'t know\".\\\\n\\\\nContext: The most recent advancements in NLP are being \\n    driven by Large Language Models (LLMs). These models outperform their \\n    smaller counterparts and have become invaluable for developers who are \\n    creating applications with NLP capabilities. Developers can tap into these \\n    models through Hugging Face\\\\\\'s `transformers` library, or by utilizing \\n    OpenAI and Cohere\\\\\\'s offerings through the `openai` and `cohere` libraries, \\n    respectively.\\\\n\\\\nQuestion: Which model providers offer LLMs?\\\\n\\\\nAnswer: \\')\\nThis example takes the static prompt from the previous block and makes it dynamic.\\nThe template contains the structure of the final prompt alongside the definition of\\nwhere the dynamic inputs will be inserted.\\nAs such, the template can be used as a recipe to build multiple static, specific\\nprompts. When you format the prompt with some specific values‚Äîin this case,\\ncontext and question‚Äîyou get a static prompt ready to be passed in to an LLM.\\nAs you can see, the question argument is passed dynamically via the invoke func‚Äê\\ntion. By default, LangChain prompts follow Python‚Äôs f-string syntax for defining\\ndynamic parameters‚Äîany word surrounded by curly braces, such as {question}, are\\nplaceholders for values passed in at runtime. In the previous example, {question}\\nwas replaced by ‚ÄúWhich model providers offer LLMs?‚Äù\\nMaking LLM Prompts Reusable | 9'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 37, 'page_label': '10'}, page_content='Let‚Äôs see how we‚Äô d feed this into an LLM OpenAI model using LangChain:\\nPython\\nfrom langchain_openai.llms import OpenAI\\nfrom langchain_core.prompts import PromptTemplate\\n# both `template` and `model` can be reused many times\\ntemplate = PromptTemplate.from_template(\"\"\"Answer the question based on the \\n    context below. If the question cannot be answered using the information \\n    provided, answer with \"I don\\'t know\".\\nContext: {context}\\nQuestion: {question}\\nAnswer: \"\"\")\\nmodel = OpenAI()\\n# `prompt` and `completion` are the results of using template and model once\\nprompt = template.invoke({\\n    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large\\n        Language Models (LLMs). These models outperform their smaller \\n        counterparts and have become invaluable for developers who are creating \\n        applications with NLP capabilities. Developers can tap into these \\n        models through Hugging Face\\'s `transformers` library, or by utilizing \\n        OpenAI and Cohere\\'s offerings through the `openai` and `cohere` \\n        libraries, respectively.\"\"\",\\n    \"question\": \"Which model providers offer LLMs?\"\\n})\\ncompletion = model.invoke(prompt)\\nJavaScript\\nimport { PromptTemplate } from \\'@langchain/core/prompts\\'\\nimport { OpenAI } from \\'@langchain/openai\\'\\nconst model = new OpenAI()\\nconst template = PromptTemplate.fromTemplate(`Answer the question based on the  \\n  context below. If the question cannot be answered using the information \\n  provided, answer with \"I don\\'t know\".\\nContext: {context}\\nQuestion: {question}\\nAnswer: `)\\nconst prompt = await template.invoke({\\n10 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 38, 'page_label': '11'}, page_content='context: `The most recent advancements in NLP are being driven by Large \\n    Language Models (LLMs). These models outperform their smaller \\n    counterparts and have become invaluable for developers who are creating \\n    applications with NLP capabilities. Developers can tap into these models \\n    through Hugging Face\\'s \\\\`transformers\\\\` library, or by utilizing OpenAI \\n    and Cohere\\'s offerings through the \\\\`openai\\\\` and \\\\`cohere\\\\` libraries, \\n    respectively.`,\\n  question: \"Which model providers offer LLMs?\"\\n})\\nawait model.invoke(prompt)\\nThe output:\\nHugging Face\\'s `transformers` library, OpenAI using the `openai` library, and \\nCohere using the `cohere` library offer LLMs.\\nIf you‚Äôre looking to build an AI chat application, the ChatPromptTemplate can be\\nused instead to provide dynamic inputs based on the role of the chat message:\\nPython\\nfrom langchain_core.prompts import ChatPromptTemplate\\ntemplate = ChatPromptTemplate.from_messages([\\n    (\\'system\\', \\'\\'\\'Answer the question based on the context below. If the \\n        question cannot be answered using the information provided, answer with \\n        \"I don\\\\\\'t know\".\\'\\'\\'),\\n    (\\'human\\', \\'Context: {context}\\'),\\n    (\\'human\\', \\'Question: {question}\\'),\\n])\\ntemplate.invoke({\\n    \"context\": \"\"\"The most recent advancements in NLP are being driven by Large \\n        Language Models (LLMs). These models outperform their smaller \\n        counterparts and have become invaluable for developers who are creating \\n        applications with NLP capabilities. Developers can tap into these \\n        models through Hugging Face\\'s `transformers` library, or by utilizing \\n        OpenAI and Cohere\\'s offerings through the `openai` and `cohere` \\n        libraries, respectively.\"\"\",\\n    \"question\": \"Which model providers offer LLMs?\"\\n})\\nJavaScript\\nimport { ChatPromptTemplate } from \\'@langchain/core/prompts\\'\\nconst template = ChatPromptTemplate.fromMessages([\\n  [\\'system\\', `Answer the question based on the context below. If the question \\n    cannot be answered using the information provided, answer with \"I \\n    don\\\\\\'t know\".`],\\n  [\\'human\\', \\'Context: {context}\\'],\\n  [\\'human\\', \\'Question: {question}\\'],\\n])\\nMaking LLM Prompts Reusable | 11'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 39, 'page_label': '12'}, page_content='await template.invoke({\\n  context: `The most recent advancements in NLP are being driven by Large \\n    Language Models (LLMs). These models outperform their smaller \\n    counterparts and have become invaluable for developers who are creating \\n    applications with NLP capabilities. Developers can tap into these models \\n    through Hugging Face\\'s \\\\`transformers\\\\` library, or by utilizing OpenAI \\n    and Cohere\\'s offerings through the \\\\`openai\\\\` and \\\\`cohere\\\\` libraries, \\n    respectively.`,\\n  question: \"Which model providers offer LLMs?\"\\n})\\nThe output:\\nChatPromptValue(messages=[SystemMessage(content=\\'Answer the question based on \\n    the context below. If the question cannot be answered using the information \\n    provided, answer with \"I don\\\\\\'t know\".\\'), HumanMessage(content=\"Context: \\n    The most recent advancements in NLP are being driven by Large Language \\n    Models (LLMs). These models outperform their smaller counterparts and have \\n    become invaluable for developers who are creating applications with NLP \\n    capabilities. Developers can tap into these models through Hugging Face\\\\\\'s \\n    `transformers` library, or by utilizing OpenAI and Cohere\\\\\\'s offerings \\n    through the `openai` and `cohere` libraries, respectively.\"), HumanMessage\\n    (content=\\'Question: Which model providers offer LLMs?\\')])\\nNotice how the prompt contains instructions in a SystemMessage and two instances\\nof HumanMessage that contain dynamic context and question variables. Y ou can still\\nformat the template in the same way and get back a static prompt that you can pass to\\na large language model for a prediction output:\\nPython\\nfrom langchain_openai.chat_models import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\n# both `template` and `model` can be reused many times\\ntemplate = ChatPromptTemplate.from_messages([\\n    (\\'system\\', \\'\\'\\'Answer the question based on the context below. If the \\n        question cannot be answered using the information provided, answer\\n        with \"I don\\\\\\'t know\".\\'\\'\\'),\\n    (\\'human\\', \\'Context: {context}\\'),\\n    (\\'human\\', \\'Question: {question}\\'),\\n])\\nmodel = ChatOpenAI()\\n# `prompt` and `completion` are the results of using template and model once\\nprompt = template.invoke({\\n    \"context\": \"\"\"The most recent advancements in NLP are being driven by \\n        Large Language Models (LLMs). These models outperform their smaller \\n        counterparts and have become invaluable for developers who are creating \\n        applications with NLP capabilities. Developers can tap into these \\n12 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 40, 'page_label': '13'}, page_content='models through Hugging Face\\'s `transformers` library, or by utilizing \\n        OpenAI and Cohere\\'s offerings through the `openai` and `cohere` \\n        libraries, respectively.\"\"\",\\n    \"question\": \"Which model providers offer LLMs?\"\\n})\\nmodel.invoke(prompt)\\nJavaScript\\nimport { ChatPromptTemplate } from \\'@langchain/core/prompts\\'\\nimport { ChatOpenAI } from \\'@langchain/openai\\'\\nconst model = new ChatOpenAI()\\nconst template = ChatPromptTemplate.fromMessages([\\n  [\\'system\\', `Answer the question based on the context below. If the question \\n    cannot be answered using the information provided, answer with \"I \\n    don\\\\\\'t know\".`],\\n  [\\'human\\', \\'Context: {context}\\'],\\n  [\\'human\\', \\'Question: {question}\\'],\\n])\\nconst prompt = await template.invoke({\\n  context: `The most recent advancements in NLP are being driven by Large \\n    Language Models (LLMs). These models outperform their smaller \\n    counterparts and have become invaluable for developers who are creating \\n    applications with NLP capabilities. Developers can tap into these models \\n    through Hugging Face\\'s \\\\`transformers\\\\` library, or by utilizing OpenAI \\n    and Cohere\\'s offerings through the \\\\`openai\\\\` and \\\\`cohere\\\\` libraries, \\n    respectively.`,\\n  question: \"Which model providers offer LLMs?\"\\n})\\nawait model.invoke(prompt)\\nThe output:\\nAIMessage(content=\"Hugging Face\\'s `transformers` library, OpenAI using the \\n    `openai` library, and Cohere using the `cohere` library offer LLMs.\")\\nGetting Specific Formats out of LLMs\\nPlain text outputs are useful, but there may be use cases where you need the LLM to\\ngenerate a structured output‚Äîthat is, output in a machine-readable format, such as\\nJSON, XML, CSV , or even in a programming language such as Python or JavaScript.\\nThis is very useful when you intend to hand that output off to some other piece of\\ncode, making an LLM play a part in your larger application.\\nGetting Specific Formats out of LLMs | 13'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 41, 'page_label': '14'}, page_content='JSON Output\\nThe most common format to generate with LLMs is JSON. JSON outputs can (for\\nexample) be sent over the wire to your frontend code or be saved to a database.\\nWhen generating JSON, the first task is to define the schema you want the LLM to\\nrespect when producing the output. Then, you should include that schema in the\\nprompt, along with the text you want to use as the source. Let‚Äôs see an example:\\nPython\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.pydantic_v1 import BaseModel\\nclass AnswerWithJustification(BaseModel):\\n    \\'\\'\\'An answer to the user\\'s question along with justification for the \\n        answer.\\'\\'\\'\\n    answer: str\\n    \\'\\'\\'The answer to the user\\'s question\\'\\'\\'\\n    justification: str\\n    \\'\\'\\'Justification for the answer\\'\\'\\'\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\nstructured_llm = llm.with_structured_output(AnswerWithJustification)\\nstructured_llm.invoke(\"\"\"What weighs more, a pound of bricks or a pound \\n    of feathers\"\"\")\\nJavaScript\\nimport { ChatOpenAI } from \\'@langchain/openai\\'\\nimport { z } from \"zod\";\\nconst answerSchema = z\\n  .object({\\n    answer: z.string().describe(\"The answer to the user\\'s question\"),\\n    justification: z.string().describe(`Justification for the \\n      answer`),\\n  })\\n  .describe(`An answer to the user\\'s question along with justification for \\n    the answer.`);\\nconst model = new ChatOpenAI({\\n  model: \"gpt-3.5-turbo\",\\n  temperature: 0,\\n}).withStructuredOutput(answerSchema)\\nawait model.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")\\n14 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 42, 'page_label': '15'}, page_content='The output:\\n{\\n  answer: \"They weigh the same\",\\n  justification: \"Both a pound of bricks and a pound of feathers weigh one pound. \\n    The weight is the same, but the volu\"... 42 more characters\\n}\\nSo, first define a schema. In Python, this is easiest to do with Pydantic (a library\\nused for validating data against schemas). In JS, this is easiest to do with Zod (an\\nequivalent library). The method with_structured_output will use that schema for\\ntwo things:\\n‚Ä¢ The schema will be converted to a JSONSchema object (a JSON format used to‚Ä¢\\ndescribe the shape [types, names, descriptions] of JSON data), which will be sent\\nto the LLM. For each LLM, LangChain picks the best method to do this, usually\\nfunction calling or prompting.\\n‚Ä¢ The schema will also be used to validate the output returned by the LLM before‚Ä¢\\nreturning it; this ensures the output produced respects the schema you passed in\\nexactly.\\nOther Machine-Readable Formats with Output Parsers\\nY ou can also use an LLM or chat model to produce output in other formats, such as\\nCSV or XML. This is where output parsers come in handy. Output parsers are classes\\nthat help you structure large language model responses. They serve two functions:\\nProviding format instructions\\nOutput parsers can be used to inject some additional instructions in the prompt\\nthat will help guide the LLM to output text in the format it knows how to parse.\\nValidating and parsing output\\nThe main function is to take the textual output of the LLM or chat model and\\nrender it to a more structured format, such as a list, XML, or other format. This\\ncan include removing extraneous information, correcting incomplete output,\\nand validating the parsed values.\\nHere‚Äôs an example of how an output parser works:\\nPython\\nfrom langchain_core.output_parsers import CommaSeparatedListOutputParser\\nparser = CommaSeparatedListOutputParser()\\nitems = parser.invoke(\"apple, banana, cherry\")\\nGetting Specific Formats out of LLMs | 15'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 43, 'page_label': '16'}, page_content='JavaScript\\nimport { CommaSeparatedListOutputParser } from \\'@langchain/core/output_parsers\\'\\nconst parser = new CommaSeparatedListOutputParser()\\nawait parser.invoke(\"apple, banana, cherry\")\\nThe output:\\n[\\'apple\\', \\'banana\\', \\'cherry\\']\\nLangChain provides a variety of output parsers for various use cases, including CSV ,\\nXML, and more. We‚Äôll see how to combine output parsers with models and prompts\\nin the next section.\\nAssembling the Many Pieces of an LLM Application\\nThe key components you‚Äôve learned about so far are essential building blocks of\\nthe LangChain framework. Which brings us to the critical question: How do you\\ncombine them effectively to build your LLM application?\\nUsing the Runnable Interface\\nAs you may have noticed, all the code examples used so far utilize a similar interface\\nand the invoke() method to generate outputs from the model (or prompt template,\\nor output parser). All components have the following:\\n‚Ä¢ There is a common interface with these methods:‚Ä¢\\n‚Äî invoke: transforms a single input into an output‚Äî\\n‚Äî batch: efficiently transforms multiple inputs into multiple outputs‚Äî\\n‚Äî stream: streams output from a single input as it‚Äôs produced‚Äî\\n‚Ä¢ There are built-in utilities for retries, fallbacks, schemas, and runtime‚Ä¢\\nconfigurability.\\n‚Ä¢ In Python, each of the three methods have asyncio equivalents.‚Ä¢\\nAs such, all components behave the same way, and the interface learned for one of\\nthem applies to all:\\nPython\\nfrom langchain_openai.llms import ChatOpenAI\\nmodel = ChatOpenAI()\\ncompletion = model.invoke(\\'Hi there!\\') \\n# Hi!\\n16 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 44, 'page_label': '17'}, page_content=\"completions = model.batch(['Hi there!', 'Bye!'])\\n# ['Hi!', 'See you!']\\nfor token in model.stream('Bye!'):\\n    print(token)\\n    # Good\\n    # bye\\n    # !\\nJavaScript\\nimport { ChatOpenAI } from '@langchain/openai'\\nconst model = new ChatOpenAI()\\nconst completion = await model.invoke('Hi there!') \\n// Hi!\\nconst completions = await model.batch(['Hi there!', 'Bye!'])\\n// ['Hi!', 'See you!']\\nfor await (const token of await model.stream('Bye!')) {\\n  console.log(token)\\n  // Good\\n  // bye\\n  // !\\n}\\nIn this example, you see how the three main methods work:\\n‚Ä¢ invoke() takes a single input and returns a single output.‚Ä¢\\n‚Ä¢ batch() takes a list of outputs and returns a list of outputs.‚Ä¢\\n‚Ä¢ stream() takes a single input and returns an iterator of parts of the output as‚Ä¢\\nthey become available.\\nIn some cases, where the underlying component doesn‚Äôt support iterative output,\\nthere will be a single part containing all output.\\nY ou can combine these components in two ways:\\nImperative\\nCall your components directly, for example, with model.invoke(...)\\nDeclarative\\nUse LangChain Expression Language (LCEL), as covered in an upcoming section\\nTable 1-1 summarizes their differences, and we‚Äôll see each in action next.\\nAssembling the Many Pieces of an LLM Application | 17\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 45, 'page_label': '18'}, page_content='Table 1-1. The main differences between imperative and declarative composition.\\n Imperative Declarative\\nSyntax All of Python or JavaScript LCEL\\nParallel execution Python: with threads or coroutines\\nJavaScript: with Promise.all\\nAutomatic\\nStreaming With yield keyword Automatic\\nAsync execution With async functions Automatic\\nImperative Composition\\nImperative composition is just a fancy name for writing the code you‚Äôre used to\\nwriting, composing these components into functions and classes. Here‚Äôs an example\\ncombining prompts, models, and output parsers:\\nPython\\nfrom langchain_openai.chat_models import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import chain\\n# the building blocks\\ntemplate = ChatPromptTemplate.from_messages([\\n    (\\'system\\', \\'You are a helpful assistant.\\'),\\n    (\\'human\\', \\'{question}\\'),\\n])\\nmodel = ChatOpenAI()\\n# combine them in a function\\n# @chain decorator adds the same Runnable interface for any function you write\\n@chain\\ndef chatbot(values):\\n    prompt = template.invoke(values)\\n    return model.invoke(prompt)\\n# use it\\nchatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})\\nJavaScript\\nimport {ChatOpenAI} from \\'@langchain/openai\\'\\nimport {ChatPromptTemplate} from \\'@langchain/core/prompts\\'\\nimport {RunnableLambda} from \\'@langchain/core/runnables\\'\\n// the building blocks\\nconst template = ChatPromptTemplate.fromMessages([\\n18 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 46, 'page_label': '19'}, page_content='[\\'system\\', \\'You are a helpful assistant.\\'],\\n  [\\'human\\', \\'{question}\\'],\\n])\\nconst model = new ChatOpenAI()\\n// combine them in a function\\n// RunnableLambda adds the same Runnable interface for any function you write\\nconst chatbot = RunnableLambda.from(async values => {\\n  const prompt = await template.invoke(values)\\n  return await model.invoke(prompt)\\n})\\n// use it\\nawait chatbot.invoke({\\n  \"question\": \"Which model providers offer LLMs?\"\\n})\\nThe output:\\nAIMessage(content=\"Hugging Face\\'s `transformers` library, OpenAI using the \\n    `openai` library, and Cohere using the `cohere` library offer LLMs.\")\\nThe preceding is a complete example of a chatbot, using a prompt and chat model. As\\nyou can see, it uses familiar Python syntax and supports any custom logic you might\\nwant to add in that function.\\nOn the other hand, if you want to enable streaming or async support, you‚Äô d have to\\nmodify your function to support it. For example, streaming support can be added as\\nfollows:\\nPython\\n@chain\\ndef chatbot(values):\\n    prompt = template.invoke(values)\\n    for token in model.stream(prompt):\\n        yield token\\nfor part in chatbot.stream({\\n    \"question\": \"Which model providers offer LLMs?\"\\n}):\\n    print(part)\\nJavaScript\\nconst chatbot = RunnableLambda.from(async function* (values) {\\n  const prompt = await template.invoke(values)\\n  for await (const token of await model.stream(prompt)) {\\n    yield token\\n  }\\n})\\nAssembling the Many Pieces of an LLM Application | 19'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 47, 'page_label': '20'}, page_content='for await (const token of await chatbot.stream({\\n  \"question\": \"Which model providers offer LLMs?\"\\n})) {\\n  console.log(token)\\n}\\nThe output:\\nAIMessageChunk(content=\"Hugging\")\\nAIMessageChunk(content=\" Face\\'s\")\\nAIMessageChunk(content=\" `transformers`\")\\n...\\nSo, either in JS or Python, you can enable streaming for your custom function by\\nyielding the values you want to stream and then calling it with stream.\\nFor asynchronous execution, you‚Äô d rewrite your function like this:\\nPython\\n@chain\\nasync def chatbot(values):\\n    prompt = await template.ainvoke(values)\\n    return await model.ainvoke(prompt)\\nawait chatbot.ainvoke({\"question\": \"Which model providers offer LLMs?\"})\\n# > AIMessage(content=\"\"\"Hugging Face\\'s `transformers` library, OpenAI using\\n    the `openai` library, and Cohere using the `cohere` library offer LLMs.\"\"\")\\nThis one applies to Python only, as asynchronous execution is the only option in\\nJavaScript.\\nDeclarative Composition\\nLCEL is a declarative language for composing LangChain components. LangChain\\ncompiles LCEL compositions to an optimized execution plan, with automatic paralleli‚Äê\\nzation, streaming, tracing, and async support.\\nLet‚Äôs see the same example using LCEL:\\nPython\\nfrom langchain_openai.chat_models import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\n# the building blocks\\ntemplate = ChatPromptTemplate.from_messages([\\n    (\\'system\\', \\'You are a helpful assistant.\\'),\\n    (\\'human\\', \\'{question}\\'),\\n])\\n20 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 48, 'page_label': '21'}, page_content='model = ChatOpenAI()\\n# combine them with the | operator\\nchatbot = template | model\\n# use it\\nchatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})\\nJavaScript\\nimport { ChatOpenAI } from \\'@langchain/openai\\'\\nimport { ChatPromptTemplate } from \\'@langchain/core/prompts\\'\\nimport { RunnableLambda } from \\'@langchain/core/runnables\\'\\n// the building blocks\\nconst template = ChatPromptTemplate.fromMessages([\\n  [\\'system\\', \\'You are a helpful assistant.\\'],\\n  [\\'human\\', \\'{question}\\'],\\n])\\nconst model = new ChatOpenAI()\\n// combine them in a function\\nconst chatbot = template.pipe(model)\\n// use it\\nawait chatbot.invoke({\\n  \"question\": \"Which model providers offer LLMs?\"\\n})\\nThe output:\\nAIMessage(content=\"Hugging Face\\'s `transformers` library, OpenAI using the \\n    `openai` library, and Cohere using the `cohere` library offer LLMs.\")\\nCrucially, the last line is the same between the two examples‚Äîthat is, you use the\\nfunction and the LCEL sequence in the same way, with invoke/stream/batch. And\\nin this version, you don‚Äôt need to do anything else to use streaming:\\nPython\\nchatbot = template | model\\nfor part in chatbot.stream({\\n    \"question\": \"Which model providers offer LLMs?\"\\n}):\\n    print(part)\\n    # > AIMessageChunk(content=\"Hugging\")\\n    # > AIMessageChunk(content=\" Face\\'s\")\\nAssembling the Many Pieces of an LLM Application | 21'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 49, 'page_label': '22'}, page_content='# > AIMessageChunk(content=\" `transformers`\")\\n    # ...\\nJavaScript\\nconst chatbot = template.pipe(model)\\nfor await (const token of await chatbot.stream({\\n  \"question\": \"Which model providers offer LLMs?\"\\n})) {\\n  console.log(token)\\n}\\nAnd, for Python only, it‚Äôs the same for using asynchronous methods:\\nPython\\nchatbot = template | model\\nawait chatbot.ainvoke({\\n    \"question\": \"Which model providers offer LLMs?\"\\n})\\nSummary\\nIn this chapter, you‚Äôve learned about the building blocks and key components neces‚Äê\\nsary to build LLM applications using LangChain. LLM applications are essentially\\na chain consisting of the large language model to make predictions, the prompt\\ninstruction(s) to guide the model toward a desired output, and an optional output\\nparser to transform the format of the model‚Äôs output.\\nAll LangChain components share the same interface with invoke, stream, and batch\\nmethods to handle various inputs and outputs. They can either be combined and\\nexecuted imperatively by calling them directly or declaratively using LCEL.\\nThe imperative approach is useful if you intend to write a lot of custom logic, whereas\\nthe declarative approach is useful for simply assembling existing components with\\nlimited customization.\\nIn Chapter 2, you‚Äôll learn how to provide external data to your AI chatbot as context\\nso that you can build an LLM application that enables you to ‚Äúchat‚Äù with your data.\\n22 | Chapter 1: LLM Fundamentals with LangChain'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 50, 'page_label': '23'}, page_content='CHAPTER 2\\nRAG Part I: Indexing Your Data\\nIn the previous chapter, you learned about the important building blocks used to\\ncreate an LLM application using LangChain. Y ou also built a simple AI chatbot\\nconsisting of a prompt sent to the model and the output generated by the model. But\\nthere are major limitations to this simple chatbot.\\nWhat if your use case requires knowledge that the model wasn‚Äôt trained on? For\\nexample, let‚Äôs say you want to use AI to ask questions about a company, but the\\ninformation is contained in a private PDF or other type of document. While we‚Äôve\\nseen model providers enriching their training datasets to include more and more of\\nthe world‚Äôs public information (no matter what format it is stored in), two major\\nlimitations continue to exist in LLM‚Äôs knowledge corpus:\\nPrivate data\\nInformation that isn‚Äôt publicly available is, by definition, not included in the\\ntraining data of LLMs.\\nCurrent events\\nTraining an LLM is a costly and time-consuming process that can span multiple\\nyears, with data-gathering being one of the first steps. This results in what is\\ncalled the knowledge cutoff, or a date beyond which the LLM has no knowledge\\nof real-world events; usually this would be the date the training set was finalized.\\nThis can be anywhere from a few months to a few years into the past, depending\\non the model in question.\\nIn either case, the model will most likely hallucinate (find misleading or false\\ninformation) and respond with inaccurate information. Adapting the prompt won‚Äôt\\nresolve the issue either because it relies on the model‚Äôs current knowledge.\\n23'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 51, 'page_label': '24'}, page_content='The Goal: Picking Relevant Context for LLMs\\nIf the only private/current data you needed for your LLM use case was one to\\ntwo pages of text, this chapter would be a lot shorter: all you‚Äô d need to make that\\ninformation available to the LLM is to include that entire text in every single prompt\\nyou sent to the model.\\nThe challenge in making data available to LLMs is first and foremost a quantity\\nproblem. Y ou have more information than can fit in each prompt you send to the\\nLLM. Which small subset of your large collection of text do you include each time\\nyou call the model? Or in other words, how do you pick (with the aid of the model)\\nwhich text is most relevant to answer each question?\\nIn this chapter and the next, you‚Äôll learn how to overcome this challenge in two steps:\\n1. Indexing your documents, that is, preprocessing them in a way where your1.\\napplication can easily find the most relevant ones for each question\\n2. Retrieving this external data from the index and using it as context for the LLM to2.\\ngenerate an accurate output based on your data\\nThis chapter focuses on indexing, the first step, which involves preprocessing your\\ndocuments into a format that can be understood and searched with LLMs. This\\ntechnique is called retrieval-augmented generation (RAG). But before we begin, let‚Äôs\\ndiscuss why your documents require preprocessing.\\nLet‚Äôs assume you would like to use LLMs to analyze the financial performance and\\nrisks in Tesla‚Äôs 2022 annual report, which is stored as text in PDF format. Y our goal\\nis to be able to ask a question like ‚ÄúWhat key risks did Tesla face in 2022?‚Äù and get a\\nhumanlike response based on context from the risk factors section of the document.\\nBreaking it down, there are four key steps (shown in Figure 2-1) that you‚Äô d need to\\ntake in order to achieve this goal:\\n1. Extract the text from the document.1.\\n2. Split the text into manageable chunks.2.\\n3. Convert the text into numbers that computers can understand.3.\\n4. Store these number representations of your text somewhere that makes it easy4.\\nand fast to retrieve the relevant sections of your document to answer a given\\nquestion.\\n24 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 52, 'page_label': '25'}, page_content='Figure 2-1. Four key steps to preprocess your documents for LLM usage\\nFigure 2-1 illustrates the flow of this preprocessing and transformation of your docu‚Äê\\nments, a process known as ingestion. Ingestion is simply the process of converting\\nyour documents into numbers that computers can understand and analyze, and\\nstoring them in a special type of database for efficient retrieval. These numbers\\nare formally known as embeddings, and this special type of database is known as a\\nvector store. Let‚Äôs look a little more closely at what embeddings are and why they‚Äôre\\nimportant, starting with something simpler than LLM-powered embeddings.\\nEmbeddings: Converting Text to Numbers\\nEmbedding refers to representing text as a (long) sequence of numbers. This is a\\nlossy representation‚Äîthat is, you can‚Äôt recover the original text from these number\\nsequences, so you usually store both the original text and this numeric representation.\\nSo, why bother? Because you gain the flexibility and power that comes with working\\nwith numbers: you can do math on words! Let‚Äôs see why that‚Äôs exciting.\\nEmbeddings Before LLMs\\nLong before LLMs, computer scientists were using embeddings‚Äîfor instance, to\\nenable full-text search capabilities in websites or to classify emails as spam. Let‚Äôs see\\nan example:\\n1. Take these three sentences:1.\\n‚Ä¢ What a sunny day.‚Ä¢\\n‚Ä¢ Such bright skies today.‚Ä¢\\n‚Ä¢ I haven‚Äôt seen a sunny day in weeks.‚Ä¢\\n2. List all unique words in them: what, a, sunny, day, such, bright, and so on.2.\\n3. For each sentence, go word by word and assign the number 0 if not present, 1 if3.\\nused once in the sentence, 2 if present twice, and so on.\\nEmbeddings: Converting Text to Numbers | 25'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 53, 'page_label': '26'}, page_content='Table 2-1 shows the result.\\nTable 2-1. Word embeddings for three sentences\\nWord What a sunny day. Such bright skies today. I haven‚Äôt seen a sunny day in weeks.\\nwhat 1 0 0\\na 1 0 1\\nsunny 1 0 1\\nday 1 0 1\\nsuch 0 1 0\\nbright 0 1 0\\nskies 0 1 0\\ntoday 0 1 0\\nI 0 0 1\\nhaven‚Äôt 0 0 1\\nseen 0 0 1\\nin 0 0 1\\nweeks 0 0 1\\nIn this model, the embedding for I haven‚Äôt seen a sunny day in weeks  is the sequence\\nof numbers 0 1 1 1 0 0 0 0 1 1 1 1 1 . This is called the bag-of-words model, and these\\nembeddings are also called sparse embeddings (or sparse vectors‚Äî vector is another\\nword for a sequence of numbers), because a lot of the numbers will be 0. Most\\nEnglish sentences use only a very small subset of all existing English words.\\nY ou can successfully use this model for:\\nKeyword search\\nY ou can find which documents contain a given word or words.\\nClassification of documents\\nY ou can calculate embeddings for a collection of examples previously labeled\\nas email spam or not spam, average them out, and obtain average word frequen‚Äê\\ncies for each of the classes (spam or not spam). Then, each new document is\\ncompared to those averages and classified accordingly.\\nThe limitation here is that the model has no awareness of meaning, only of the actual\\nwords used. For instance, the embeddings for sunny day and bright skies look very\\ndifferent. In fact they have no words in common, even though we know they have\\nsimilar meaning. Or, in the email classification problem, a would-be spammer can\\ntrick the filter by replacing common ‚Äúspam words‚Äù with their synonyms.\\nIn the next section, we‚Äôll see how semantic embeddings address this limitation by\\nusing numbers to represent the meaning of the text, instead of the exact words found\\nin the text.\\n26 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 54, 'page_label': '27'}, page_content='1 Arvind Neelakantan et al., ‚ÄúText and Code Embeddings by Contrastive Pre-Training‚Äù, arXiv, January 21, 2022.\\nLLM-Based Embeddings\\nWe‚Äôre going to skip over all the ML developments that came in between and jump\\nstraight to LLM-based embeddings. Just know that there was a gradual evolution\\nfrom the simple method outlined in the previous section to the sophisticated method\\ndescribed in this one.\\nY ou can think of embedding models as an offshoot from the training process of\\nLLMs. If you remember from the Preface, the LLM training process (learning from\\nvast amounts of written text) enables LLMs to complete a prompt (or input) with the\\nmost appropriate continuation (output). This capability stems from an understanding\\nof the meaning of words and sentences in the context of the surrounding text, learned\\nfrom how words are used together in the training texts. This understanding of the\\nmeaning (or semantics) of the prompt can be extracted as a numeric representation\\n(or embedding) of the input text, and can be used directly for some very interesting\\nuse cases too.\\nIn practice, most embedding models are trained for that purpose alone, following\\nsomewhat similar architectures and training processes as LLMs, as that is more\\nefficient and results in higher-quality embeddings.1\\nAn embedding model then is an algorithm that takes a piece of text and outputs\\na numerical representation of its meaning‚Äîtechnically, a long list of floating-point\\n(decimal) numbers, usually somewhere between 100 and 2,000 numbers, or  dimen‚Äê\\nsions. These are also called dense embeddings, as opposed to the sparse embeddings of\\nthe previous section, as here usually all dimensions are different from 0.\\nDifferent models produce different numbers and different sizes of\\nlists. All of these are specific to each model; that is, even if the\\nsize of the lists matches, you cannot compare embeddings from\\ndifferent models. Combining embeddings from different models\\nshould always be avoided.\\nSemantic Embeddings Explained\\nConsider these three words: lion, pet, and dog. Intuitively, which pair of these words\\nshare similar characteristics to each other at first glance? The obvious answer is pet\\nand dog. But computers do not have the ability to tap into this intuition or nuanced\\nunderstanding of the English language. In order for a computer to differentiate\\nbetween a lion, pet, or dog, you need to be able to translate them into the language of\\ncomputers, which is numbers.\\nEmbeddings: Converting Text to Numbers | 27'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 55, 'page_label': '28'}, page_content='Figure 2-2 illustrates converting each word into hypothetical number representations\\nthat retain their meaning.\\nFigure 2-2. Semantic representations of words\\nFigure 2-2 shows each word alongside its corresponding semantic embedding. Note\\nthat the numbers themselves have no particular meaning, but instead the sequences\\nof numbers for two words (or sentences) that are close in meaning should be closer\\nthan those of unrelated words. As you can see, each number is a floating-point value,\\nand each of them represents a semantic dimension. Let‚Äôs see what we mean by closer:\\nIf we plot these vectors in a three-dimensional space, it could look like Figure 2-3.\\nFigure 2-3. Plot of word vectors in a multidimensional space\\nFigure 2-3 shows the pet and dog vectors are closer to each other in distance than the\\nlion plot. We can also observe that the angles between each plot varies depending on\\nhow similar they are. For example, the words pet and lion have a wider angle between\\none another than the pet and dog do, indicating more similarities shared by the latter\\nword pairs. The narrower the angle or shorter the distance between two vectors, the\\ncloser their similarities.\\n28 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 56, 'page_label': '29'}, page_content='One effective way to calculate the degree of similarity between two vectors in a\\nmultidimensional space is called cosine similarity. Cosine similarity computes the dot\\nproduct of vectors and divides it by the product of their magnitudes to output a\\nnumber between ‚Äì1 and 1, where 0 means the vectors share no correlation, ‚Äì1 means\\nthey are absolutely dissimilar, and 1 means they are absolutely similar. So, in the case\\nof our three words here, the cosine similarity between pet and dog could be 0.75, but\\nbetween pet and lion it might be 0.1.\\nThe ability to convert sentences into embeddings  that capture semantic meaning and\\nthen perform calculations to find semantic similarities between different sentences\\nenables us to get an LLM to find the most relevant documents to answer questions\\nabout a large body of text like our Tesla PDF document. Now that you understand the\\nbig picture, let‚Äôs revisit the first step (indexing) of preprocessing your document.\\nOther Uses for Embeddings\\nThese sequences of numbers and vectors have a number of interesting properties:\\n‚Ä¢ As you learned earlier, if you think of a vector as describing a point in high-‚Ä¢\\ndimensional space, points that are closer together have more similar meanings,\\nso a distance function can be used to measure similarity.\\n‚Ä¢ Groups of points close together can be said to be related; therefore, a clustering‚Ä¢\\nalgorithm can be used to identify topics (or clusters of points) and classify new\\ninputs into one of those topics.\\n‚Ä¢ If you average out multiple embeddings, the average embedding can be said‚Ä¢\\nto represent the overall meaning of that group; that is, you can embed a long\\ndocument (for instance, this book) by:\\n1. Embedding each page separately1.\\n2. Taking the average of the embeddings of all pages as the book embedding2.\\n‚Ä¢ Y ou can ‚Äútravel‚Äù the ‚Äúmeaning‚Äù space by using the elementary math operations‚Ä¢\\nof addition and subtraction: for instance, the operation king ‚Äì man + woman\\n= queen. If you take the meaning (or semantic embedding) of king, subtract\\nthe meaning of man, presumably you arrive at the more abstract meaning of\\nmonarch, at which point, if you add the meaning of woman, you‚Äôve arrived close\\nto the meaning (or embedding) of the word queen.\\n‚Ä¢ There are models that can produce embeddings for nontext content, for instance,‚Ä¢\\nimages, videos, and sounds, in addition to text. This enables, for instance, finding\\nimages that are most similar or relevant for a given sentence.\\nEmbeddings: Converting Text to Numbers | 29'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 57, 'page_label': '30'}, page_content='We won‚Äôt explore all of these attributes in this book, but it‚Äôs useful to know they can be\\nused for a number of applications such as:\\nSearch\\nFinding the most relevant documents for a new query\\nClustering\\nGiven a body of documents, dividing them into groups (for instance, topics)\\nClassification\\nAssigning a new document to a previously identified group or label (for instance,\\na topic)\\nRecommendation\\nGiven a document, surfacing similar documents\\nDetecting anomalies\\nIdentifying documents that are very dissimilar from previously seen ones\\nWe hope this leaves you with some intuition that embeddings are quite versatile and\\ncan be put to good use in your future projects.\\nConverting Your Documents into Text\\nAs mentioned at the beginning of the chapter, the first step in preprocessing your\\ndocument is to convert it to text. In order to achieve this, you would need to build\\nlogic to parse and extract the document with minimal loss of quality. Fortunately,\\nLangChain provides document loaders that handle the parsing logic and enable you\\nto ‚Äúload‚Äù data from various sources into a Document class that consists of text and\\nassociated metadata.\\nFor example, consider a simple .txt file. Y ou can simply import a LangChain\\nTextLoader class to extract the text, like this:\\nPython\\nfrom langchain_community.document_loaders import TextLoader\\nloader = TextLoader(\"./test.txt\")\\nloader.load()\\nJavaScript\\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\\nconst loader = new TextLoader(\"./test.txt\");\\nconst docs = await loader.load();\\n30 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 58, 'page_label': '31'}, page_content='The output:\\n[Document(page_content=\\'text content \\\\n\\', metadata={\\'line_number\\': 0, \\'source\\': \\n    \\'./test.txt\\'})]\\nThe previous code block assumes that you have a file named test.txt in your\\ncurrent directory. Usage of all LangChain document loaders follows a similar pattern:\\n1. Start by picking the loader for your type of document from the long list of1.\\nintegrations.\\n2. Create an instance of the loader in question, along with any parameters to2.\\nconfigure it, including the location of your documents (usually a filesystem path\\nor web address).\\n3. Load the documents by calling load(), which returns a list of documents ready3.\\nto pass to the next stage (more on that soon).\\nAside from .txt files, LangChain provides document loaders for other popular file\\ntypes including .csv, .json, and Markdown, alongside integrations with popular plat‚Äê\\nforms such as Slack and Notion.\\nFor example, you can use WebBaseLoader to load HTML from web URLs and parse it\\nto text.\\nInstall the beautifulsoup4 package:\\npip install beautifulsoup4\\nPython\\nfrom langchain_community.document_loaders import WebBaseLoader\\nloader = WebBaseLoader(\"https://www.langchain.com/\")\\nloader.load()\\nJavaScript\\n// install cheerio: npm install cheerio\\nimport { \\n  CheerioWebBaseLoader \\n} from \"@langchain/community/document_loaders/web/cheerio\";\\nconst loader = new CheerioWebBaseLoader(\"https://www.langchain.com/\");\\nconst docs = await loader.load();\\nConverting Your Documents into Text | 31'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 59, 'page_label': '32'}, page_content='In the case of our Tesla PDF use case, we can utilize LangChain‚Äôs PDFLoader to extract\\ntext from the PDF document:\\nPython\\n# install the pdf parsing library\\n# pip install pypdf\\nfrom langchain_community.document_loaders import PyPDFLoader\\nloader = PyPDFLoader(\"./test.pdf\")\\npages = loader.load()\\nJavaScript\\n// install the pdf parsing library: npm install pdf-parse\\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\\nconst loader = new PDFLoader(\"./test.pdf\");\\nconst docs = await loader.load();\\nThe text has been extracted from the PDF document and stored in the Document\\nclass. But there‚Äôs a problem. The loaded document is over 100,000 characters long,\\nso it won‚Äôt fit into the context window of the vast majority of LLMs or embedding\\nmodels. In order to overcome this limitation, we need to split the Document into man‚Äê\\nageable chunks of text that we can later convert into embeddings and semantically\\nsearch, bringing us to the second step (retrieving).\\nLLMs and embedding models are designed with a hard limit on\\nthe size of input and output tokens they can handle. This limit\\nis usually called context window, and usually applies to the combi‚Äê\\nnation of input and output; that is, if the context window is 100\\n(we‚Äôll talk about units in a second), and your input measures 90,\\nthe output can be at most of length 10. Context windows are\\nusually measured in number of tokens, for instance 8,192 tokens.\\nTokens, as mentioned in the Preface, are a representation of text as\\nnumbers, with each token usually covering between three and four\\ncharacters of English text.\\nSplitting Your Text into Chunks\\nAt first glance it may seem straightforward to split a large body of text into chunks,\\nbut keeping semantically related (related by meaning) chunks of text together is a\\ncomplex process. To make it easier to split large documents into small, but still\\nmeaningful, pieces of text, LangChain provides RecursiveCharacterTextSplitter,\\nwhich does the following:\\n32 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 60, 'page_label': '33'}, page_content='1. Take a list of separators, in order of importance. By default these are:1.\\na. The paragraph separator: \\\\n\\\\na.\\nb. The line separator: \\\\nb.\\nc. The word separator: space characterc.\\n2. To respect the given chunk size, for instance, 1,000 characters, start by splitting2.\\nup paragraphs.\\n3. For any paragraph longer than the desired chunk size, split by the next separator:3.\\nlines. Continue until all chunks are smaller than the desired length, or there are\\nno additional separators to try.\\n4. Emit each chunk as a Document, with the metadata of the original docu‚Äê4.\\nment passed in and additional information about the position in the original\\ndocument.\\nLet‚Äôs see an example:\\nPython\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nloader = TextLoader(\"./test.txt\") # or any other loader\\ndocs = loader.load()\\nsplitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,\\n    chunk_overlap=200,\\n)\\nsplitted_docs = splitter.split_documents(docs)\\nJavaScript\\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\\nconst loader = new TextLoader(\"./test.txt\"); // or any other loader \\nconst docs = await loader.load();\\nconst splitter = new RecursiveCharacterTextSplitter({\\n  chunkSize: 1000,\\n  chunkOverlap: 200,\\n});\\nconst splittedDocs = await splitter.splitDocuments(docs)\\nIn the preceding code, the documents created by the document loader are split\\ninto chunks of 1,000 characters each, with some overlap between chunks of 200\\ncharacters to maintain some context. The result is also a list of documents, where\\neach document is up to 1,000 characters in length, split along the natural divisions of\\nSplitting Your Text into Chunks | 33'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 61, 'page_label': '34'}, page_content='written text‚Äîparagraphs, new lines and finally, words. This uses the structure of the\\ntext to keep each chunk a consistent, readable snippet of text.\\nRecursiveCharacterTextSplitter can also be used to split code languages and\\nMarkdown into semantic chunks. This is done by using keywords specific to each\\nlanguage as the separators, which ensures, for instance, the body of each function is\\nkept in the same chunk, instead of split between several. Usually, as programming\\nlanguages have more structure than written text, there‚Äôs less need to use overlap\\nbetween the chunks. LangChain contains separators for a number of popular lan‚Äê\\nguages, such as Python, JS, Markdown, HTML, and many more. Here‚Äôs an example:\\nPython\\nfrom langchain_text_splitters import (\\n    Language,\\n    RecursiveCharacterTextSplitter,\\n)\\nPYTHON_CODE = \"\"\"\\ndef hello_world():\\n    print(\"Hello, World!\")\\n# Call the function\\nhello_world()\\n\"\"\"\\npython_splitter = RecursiveCharacterTextSplitter.from_language(\\n    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\\n)\\npython_docs = python_splitter.create_documents([PYTHON_CODE])\\nJavaScript\\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\\nconst PYTHON_CODE = `\\ndef hello_world():\\n  print(\"Hello, World!\")\\n# Call the function\\nhello_world()\\n`;\\nconst pythonSplitter = RecursiveCharacterTextSplitter.fromLanguage(\"python\", {\\n  chunkSize: 50,\\n  chunkOverlap: 0,\\n});\\nconst pythonDocs = await pythonSplitter.createDocuments([PYTHON_CODE]);\\nThe output:\\n[Document(page_content=\\'def hello_world():\\\\n    print(\"Hello, World!\")\\'),\\n    Document(page_content=\\'# Call the function\\\\nhello_world()\\')]\\n34 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 62, 'page_label': '35'}, page_content='Notice how we‚Äôre still using RecursiveCharacterTextSplitter as before, but now\\nwe‚Äôre creating an instance of it for a specific language, using the from_language\\nmethod. This one accepts the name of the language, and the usual parameters for\\nchunk size, and so on. Also notice we are now using the method create_documents,\\nwhich accepts a list of strings, rather than the list of documents we had before. This\\nmethod is useful when the text you want to split doesn‚Äôt come from a document\\nloader, so you have only the raw text strings.\\nY ou can also use the optional second argument to create_documents in order to pass\\na list of metadata to associate with each text string. This metadata list should have the\\nsame length as the list of strings and will be used to populate the metadata field of\\neach Document returned.\\nLet‚Äôs see an example for Markdown text, using the metadata argument as well:\\nPython\\nmarkdown_text = \"\"\"\\n# LangChain\\n‚ö° Building applications with LLMs through composability ‚ö°\\n## Quick Install\\n```bash\\npip install langchain\\n```\\nAs an open source project in a rapidly developing field, we are extremely open \\n    to contributions.\\n\"\"\"\\nmd_splitter = RecursiveCharacterTextSplitter.from_language(\\n    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\\n)\\nmd_docs = md_splitter.create_documents([markdown_text], \\n    [{\"source\": \"https://www.langchain.com\"}])\\nJavaScript\\nconst markdownText = `\\n# LangChain\\n‚ö° Building applications with LLMs through composability ‚ö°\\n## Quick Install\\n\\\\`\\\\`\\\\`bash\\npip install langchain\\n\\\\`\\\\`\\\\`\\nSplitting Your Text into Chunks | 35'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 63, 'page_label': '36'}, page_content='As an open source project in a rapidly developing field, we are extremely \\n  open to contributions.\\n`;\\nconst mdSplitter = RecursiveCharacterTextSplitter.fromLanguage(\"markdown\", {\\n  chunkSize: 60,\\n  chunkOverlap: 0,\\n});\\nconst mdDocs = await mdSplitter.createDocuments([markdownText], \\n  [{\"source\": \"https://www.langchain.com\"}]);\\nThe output:\\n[Document(page_content=\\'# LangChain\\', \\n    metadata={\"source\": \"https://www.langchain.com\"}),\\n Document(page_content=\\'‚ö° Building applications with LLMs through composability \\n    ‚ö°\\', metadata={\"source\": \"https://www.langchain.com\"}),\\n Document(page_content=\\'## Quick Install\\\\n\\\\n```bash\\', \\n    metadata={\"source\": \"https://www.langchain.com\"}),\\n Document(page_content=\\'pip install langchain\\', \\n    metadata={\"source\": \"https://www.langchain.com\"}),\\n Document(page_content=\\'```\\', metadata={\"source\": \"https://www.langchain.com\"}),\\n Document(page_content=\\'As an open source project in a rapidly developing field, \\n    we\\', metadata={\"source\": \"https://www.langchain.com\"}),\\n Document(page_content=\\'are extremely open to contributions.\\', \\n    metadata={\"source\": \"https://www.langchain.com\"})]\\nNotice two things:\\n‚Ä¢ The text is split along the natural stopping points in the Markdown document;‚Ä¢\\nfor instance, the heading goes into one chunk, the line of text under it in a\\nseparate chunk, and so on.\\n‚Ä¢ The metadata we passed in the second argument is attached to each resulting‚Ä¢\\ndocument, which allows you to track, for instance, where the document came\\nfrom and where you can go to see the original.\\nGenerating Text Embeddings\\nLangChain also has an Embeddings class designed to interface with text embedding\\nmodels‚Äîincluding OpenAI, Cohere, and Hugging Face‚Äîand generate vector repre‚Äê\\nsentations of text. This class provides two methods: one for embedding documents\\nand one for embedding a query. The former takes a list of text strings as input, while\\nthe latter takes a single text string.\\n36 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 64, 'page_label': '37'}, page_content='Here‚Äôs an example of embedding a document using OpenAI‚Äôs embedding model:\\nPython\\nfrom langchain_openai import OpenAIEmbeddings\\nmodel = OpenAIEmbeddings()\\nembeddings = model.embed_documents([\\n    \"Hi there!\",\\n    \"Oh, hello!\",\\n    \"What\\'s your name?\",\\n    \"My friends call me World\",\\n    \"Hello World!\"\\n])\\nJavaScript\\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\\nconst model = new OpenAIEmbeddings();\\nconst embeddings = await embeddings.embedDocuments([\\n  \"Hi there!\",\\n  \"Oh, hello!\",\\n  \"What\\' s your name?\",\\n  \"My friends call me World\",\\n  \"Hello World!\"\\n]);\\nThe output:\\n[\\n  [\\n    -0.004845875, 0.004899438, -0.016358767, -0.024475135, -0.017341806,\\n      0.012571548, -0.019156644, 0.009036391, -0.010227379, -0.026945334,\\n      0.022861943, 0.010321903, -0.023479493, -0.0066544134, 0.007977734,\\n    0.0026371893, 0.025206111, -0.012048521, 0.012943339, 0.013094575,\\n    -0.010580265, -0.003509951, 0.004070787, 0.008639394, -0.020631202,\\n    ... 1511 more items\\n  ]\\n  [\\n      -0.009446913, -0.013253193, 0.013174579, 0.0057552797, -0.038993083,\\n      0.0077763423, -0.0260478, -0.0114384955, -0.0022683728, -0.016509168,\\n      0.041797023, 0.01787183, 0.00552271, -0.0049789557, 0.018146982,\\n      -0.01542166, 0.033752076, 0.006112323, 0.023872782, -0.016535373,\\n      -0.006623321, 0.016116094, -0.0061090477, -0.0044155475, -0.016627092,\\n    ... 1511 more items\\n  ]\\n  ... 3 more items\\n]\\nGenerating Text Embeddings | 37'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 65, 'page_label': '38'}, page_content='Notice that you can embed multiple documents at the same time; you should prefer\\nthis to embedding them one at a time, as it will be more efficient (due to how these\\nmodels are constructed). Y ou get back a list containing multiple lists of numbers‚Äî\\neach inner list is a vector or embedding, as explained in an earlier section.\\nNow let‚Äôs see an end-to-end example using the three capabilities we‚Äôve seen so far:\\n‚Ä¢ Document loaders, to convert any document to plain text‚Ä¢\\n‚Ä¢ Text splitters, to split each large document into many smaller ones‚Ä¢\\n‚Ä¢ Embeddings models, to create a numeric representation of the meaning of each‚Ä¢\\nsplit\\nHere‚Äôs the code:\\nPython\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import OpenAIEmbeddings\\n## Load the document \\nloader = TextLoader(\"./test.txt\")\\ndoc = loader.load()\\n\"\"\"\\n[\\n    Document(page_content=\\'Document loaders\\\\n\\\\nUse document loaders to load data \\n        from a source as `Document`\\\\\\'s. A `Document` is a piece of text\\\\nand \\n        associated metadata. For example, there are document loaders for \\n        loading a simple `.txt` file, for loading the text\\\\ncontents of any web \\n        page, or even for loading a transcript of a YouTube video.\\\\n\\\\nEvery \\n        document loader exposes two methods:\\\\n1. \"Load\": load documents from \\n        the configured source\\\\n2. \"Load and split\": load documents from the \\n        configured source and split them using the passed in text \\n        splitter\\\\n\\\\nThey optionally implement:\\\\n\\\\n3. \"Lazy load\": load \\n        documents into memory lazily\\\\n\\', metadata={\\'source\\': \\'test.txt\\'})\\n]\\n\"\"\"\\n## Split the document\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,\\n    chunk_overlap=20,\\n)\\nchunks = text_splitter.split_documents(doc)\\n## Generate embeddings\\n38 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 66, 'page_label': '39'}, page_content='embeddings_model = OpenAIEmbeddings()\\nembeddings = embeddings_model.embed_documents(\\n    [chunk.page_content for chunk in chunks]\\n)\\n\"\"\"\\n[[0.0053587136790156364,\\n -0.0004999046213924885,\\n  0.038883671164512634,\\n -0.003001077566295862,\\n -0.00900818221271038, ...], ...]\\n\"\"\"\\nJavaScript\\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\\n// Load the document \\nconst loader = new TextLoader(\"./test.txt\");\\nconst docs = await loader.load();\\n// Split the document\\nconst splitter = new RecursiveCharacterTextSplitter({\\n  chunkSize: 1000,\\n  chunkOverlap: 200,\\n});\\nconst chunks = await splitter.splitDocuments(docs)\\n// Generate embeddings\\nconst model = new OpenAIEmbeddings();\\nawait embeddings.embedDocuments(chunks.map(c => c.pageContent));\\nOnce you‚Äôve generated embeddings from your documents, the next step is to store\\nthem in a special database known as a vector store.\\nStoring Embeddings in a Vector Store\\nEarlier in this chapter, we discussed the cosine similarity calculation to measure the\\nsimilarity between vectors in a vector space. A vector store is a database designed to\\nstore vectors and perform complex calculations, like cosine similarity, efficiently and\\nquickly.\\nUnlike traditional databases that specialize in storing structured data (such as JSON\\ndocuments or data conforming to the schema of a relational database), vector stores\\nhandle unstructured data, including text and images. Like traditional databases, vector\\nStoring Embeddings in a Vector Store | 39'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 67, 'page_label': '40'}, page_content='stores are capable of performing create, read, update, delete (CRUD), and search\\noperations.\\nVector stores unlock a wide variety of use cases, including scalable applications that\\nutilize AI to answer questions about large documents, as illustrated in Figure 2-4.\\nFigure 2-4. Loading, embedding, storing, and retrieving relevant docs from a vector store\\nFigure 2-4  illustrates how document embeddings are inserted into the vector store\\nand how later, when a query is sent, similar embeddings are retrieved from the vector\\nstore.\\nCurrently, there is an abundance of vector store providers to choose from, each\\nspecializing in different capabilities. Y our selection should depend on the critical\\nrequirements of your application, including multitenancy, metadata filtering capabili‚Äê\\nties, performance, cost, and scalability.\\nAlthough vector stores are niche databases built to manage vector data, there are a\\nfew disadvantages working with them:\\n‚Ä¢ Most vector stores are relatively new and may not stand the test of time.‚Ä¢\\n‚Ä¢ Managing and optimizing vector stores can present a relatively steep learning‚Ä¢\\ncurve.\\n‚Ä¢ Managing a separate database adds complexity to your application and may drain‚Ä¢\\nvaluable resources.\\nFortunately, vector store capabilities have recently been extended to PostgreSQL (a\\npopular open source relational database) via the pgvector extension. This enables\\nyou to use the same database you‚Äôre already familiar with and to power both your\\ntransactional tables (for instance your users table) as well as your vector search tables.\\nGetting Set Up with PGVector\\nTo use Postgres and PGVector you‚Äôll need to follow a few setup steps:\\n1. Ensure you have Docker installed on your computer, following the instructions1.\\nfor your operating system.\\n2. Run the following command in your terminal; it will launch a Postgres instance2.\\nin your computer running on port 6024:\\n40 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 68, 'page_label': '41'}, page_content='docker run \\\\\\n    --name pgvector-container \\\\\\n    -e POSTGRES_USER=langchain \\\\\\n    -e POSTGRES_PASSWORD=langchain \\\\\\n    -e POSTGRES_DB=langchain \\\\\\n    -p 6024:5432 \\\\\\n    -d pgvector/pgvector:pg16\\nOpen your docker dashboard containers and you should see a green running\\nstatus next to pgvector-container.\\n3. Save the connection string to use in your code; we‚Äôll need it later:3.\\npostgresql+psycopg://langchain:langchain@localhost:6024/langchain\\nWorking with Vector Stores\\nPicking up where we left off in the previous section on embeddings, now let‚Äôs see an\\nexample of loading, splitting, embedding, and storing a document in PGVector:\\nPython\\n# first, pip install langchain-postgres\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_postgres.vectorstores import PGVector\\nfrom langchain_core.documents import Document\\nimport uuid\\n# Load the document, split it into chunks\\nraw_documents = TextLoader(\\'./test.txt\\').load()\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \\n    chunk_overlap=200)\\ndocuments = text_splitter.split_documents(raw_documents)\\n# embed each chunk and insert it into the vector store\\nembeddings_model = OpenAIEmbeddings()\\nconnection = \\'postgresql+psycopg://langchain:langchain@localhost:6024/langchain\\'\\ndb = PGVector.from_documents(documents, embeddings_model, connection=connection)\\nJavaScript\\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\\nimport { PGVectorStore } from \"@langchain/community/vectorstores/pgvector\";\\nimport { v4 as uuidv4 } from \\'uuid\\';\\n// Load the document, split it into chunks\\nconst loader = new TextLoader(\"./test.txt\");\\nconst raw_docs = await loader.load();\\nconst splitter = new RecursiveCharacterTextSplitter({\\n  chunkSize: 1000,\\nStoring Embeddings in a Vector Store | 41'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 69, 'page_label': '42'}, page_content='chunkOverlap: 200,\\n});\\nconst docs = await splitter.splitDocuments(docs)\\n// embed each chunk and insert it into the vector store\\nconst embeddings_model = new OpenAIEmbeddings();\\nconst db = await PGVectorStore.fromDocuments(docs, embeddings_model, {\\n  postgresConnectionOptions: {\\n    connectionString: \\'postgresql://langchain:langchain@localhost:6024/langchain\\'\\n  }\\n})\\nNotice how we reuse the code from the previous sections to first load the documents\\nwith the loader and then split them into smaller chunks. Then, we instantiate the\\nembeddings model we want to use‚Äîin this case, OpenAI‚Äôs. Note that you could use\\nany other embeddings model supported by LangChain here.\\nNext, we have a new line of code, which creates a vector store given documents, the\\nembeddings model, and a connection string. This will do a few things:\\n‚Ä¢ Establish a connection to the Postgres instance running in your computer (see‚Ä¢\\n‚ÄúGetting Set Up with PGVector‚Äù on page 40.)\\n‚Ä¢ Run any setup necessary, such as creating tables to hold your documents and‚Ä¢\\nvectors, if this is the first time you‚Äôre running it.\\n‚Ä¢ Create embeddings for each document you passed in, using the model you chose.‚Ä¢\\n‚Ä¢ Store the embeddings, the document‚Äôs metadata, and the document‚Äôs text content‚Ä¢\\nin Postgres, ready to be searched.\\nLet‚Äôs see what it looks like to search documents:\\nPython\\ndb.similarity_search(\"query\", k=4)\\nJavaScript\\nawait pgvectorStore.similaritySearch(\"query\", 4);\\nThis method will find the most relevant documents (which you previously indexed),\\nby following this process:\\n‚Ä¢ The search query‚Äîin this case, the word query‚Äîwill be sent to the embeddings‚Ä¢\\nmodel to retrieve its embedding.\\n‚Ä¢ Then, it will run a query on Postgres to find the N (in this case 4) previously‚Ä¢\\nstored embeddings that are most similar to your query.\\n‚Ä¢ Finally, it will fetch the text content and metadata that relates to each of those‚Ä¢\\nembeddings.\\n42 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 70, 'page_label': '43'}, page_content='‚Ä¢ The model can now return a list of Document sorted by how similar they are to‚Ä¢\\nthe query‚Äîthe most similar first, the second most similar after, and so on.\\nY ou can also add more documents to an existing database. Let‚Äôs see an example:\\nPython\\nids = [str(uuid.uuid4()), str(uuid.uuid4())]\\ndb.add_documents(\\n    [\\n        Document(\\n            page_content=\"there are cats in the pond\",\\n            metadata={\"location\": \"pond\", \"topic\": \"animals\"},\\n        ),\\n        Document(\\n            page_content=\"ducks are also found in the pond\",\\n            metadata={\"location\": \"pond\", \"topic\": \"animals\"},\\n        ),\\n    ],\\n    ids=ids,\\n)\\nJavaScript\\nconst ids = [uuidv4(), uuidv4()];\\nawait db.addDocuments(\\n  [\\n    {\\n      pageContent: \"there are cats in the pond\",\\n      metadata: {location: \"pond\", topic: \"animals\"}\\n    }, \\n    {\\n      pageContent: \"ducks are also found in the pond\",\\n      metadata: {location: \"pond\", topic: \"animals\"}\\n    },\\n  ], \\n  {ids}\\n);\\nThe add_documents method we‚Äôre using here will follow a similar process to\\nfromDocuments:\\n‚Ä¢ Create embeddings for each document you passed in, using the model you chose.‚Ä¢\\n‚Ä¢ Store the embeddings, the document‚Äôs metadata, and the document‚Äôs text content‚Ä¢\\nin Postgres, ready to be searched.\\nIn this example, we are using the optional ids argument to assign identifiers to each\\ndocument, which allows us to update or delete them later.\\nStoring Embeddings in a Vector Store | 43'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 71, 'page_label': '44'}, page_content='Here‚Äôs an example of the delete operation:\\nPython\\ndb.delete(ids=[1])\\nJavaScript\\nawait db.delete({ ids: [ids[1]] })\\nThis removes the second document inserted by using its Universally Unique Identi‚Äê\\nfier (UUID). Now let‚Äôs see how to do this in a more systematic way.\\nTracking Changes to Your Documents\\nOne of the key challenges with working with vector stores is working with data that\\nregularly changes, because changes mean re-indexing. And re-indexing can lead to\\ncostly recomputations of embeddings and duplications of preexisting content.\\nFortunately, LangChain provides an indexing API to make it easy to keep your\\ndocuments in sync with your vector store. The API utilizes a class ( RecordManager)\\nto keep track of document writes into the vector store. When indexing content,\\nhashes are computed for each document and the following information is stored in\\nRecordManager:\\n‚Ä¢ The document hash (hash of both page content and metadata)‚Ä¢\\n‚Ä¢ Write time‚Ä¢\\n‚Ä¢ The source ID (each document should include information in its metadata to‚Ä¢\\ndetermine the ultimate source of this document).\\nIn addition, the indexing API provides cleanup modes to help you decide how to\\ndelete existing documents in the vector store. For example, If you‚Äôve made changes to\\nhow documents are processed before insertion or if source documents have changed,\\nyou may want to remove any existing documents that come from the same source\\nas the new documents being indexed. If some source documents have been deleted,\\nyou‚Äôll want to delete all existing documents in the vector store and replace them with\\nthe re-indexed documents.\\nThe modes are as follows:\\n‚Ä¢ None mode does not do any automatic cleanup, allowing the user to manually do‚Ä¢\\ncleanup of old content.\\n‚Ä¢ Incremental and full modes delete previous versions of the content if the‚Ä¢\\ncontent of the source document or derived documents has changed.\\n44 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 72, 'page_label': '45'}, page_content='‚Ä¢ Full mode will additionally delete any documents not included in documents‚Ä¢\\ncurrently being indexed.\\nHere‚Äôs an example of the use of the indexing API with Postgres database set up as a\\nrecord manager:\\nPython\\nfrom langchain.indexes import SQLRecordManager, index\\nfrom langchain_postgres.vectorstores import PGVector\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain.docstore.document import Document\\nconnection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\\ncollection_name = \"my_docs\"\\nembeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\\nnamespace = \"my_docs_namespace\"\\nvectorstore = PGVector(\\n    embeddings=embeddings_model,\\n    collection_name=collection_name,\\n    connection=connection,\\n    use_jsonb=True,\\n)\\nrecord_manager = SQLRecordManager(\\n    namespace,\\n    db_url=\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\",\\n)\\n# Create the schema if it doesn\\'t exist\\nrecord_manager.create_schema()\\n# Create documents\\ndocs = [\\n    Document(page_content=\\'there are cats in the pond\\', metadata={\\n        \"id\": 1, \"source\": \"cats.txt\"}),\\n    Document(page_content=\\'ducks are also found in the pond\\', metadata={\\n        \"id\": 2, \"source\": \"ducks.txt\"}),\\n]\\n# Index the documents\\nindex_1 = index(\\n    docs,\\n    record_manager,\\n    vectorstore,\\n    cleanup=\"incremental\",  # prevent duplicate documents\\n    source_id_key=\"source\",  # use the source field as the source_id\\n)\\nprint(\"Index attempt 1:\", index_1)\\nTracking Changes to Your Documents | 45'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 73, 'page_label': '46'}, page_content='# second time you attempt to index, it will not add the documents again\\nindex_2 = index(\\n    docs,\\n    record_manager,\\n    vectorstore,\\n    cleanup=\"incremental\",\\n    source_id_key=\"source\",\\n)\\nprint(\"Index attempt 2:\", index_2)\\n# If we mutate a document, the new version will be written and all old \\n# versions sharing the same source will be deleted.\\ndocs[0].page_content = \"I just modified this document!\"\\nindex_3 = index(\\n    docs,\\n    record_manager,\\n    vectorstore,\\n    cleanup=\"incremental\",\\n    source_id_key=\"source\",\\n)\\nprint(\"Index attempt 3:\", index_3)\\nJavaScript\\n/** \\n1. Ensure docker is installed and running (https://docs.docker.com/get-docker/)\\n2. Run the following command to start the postgres container:\\n   \\ndocker run \\\\\\n  --name pgvector-container \\\\\\n  -e POSTGRES_USER=langchain \\\\\\n  -e POSTGRES_PASSWORD=langchain \\\\\\n  -e POSTGRES_DB=langchain \\\\\\n  -p 6024:5432 \\\\\\n  -d pgvector/pgvector:pg16\\n3. Use the connection string below for the postgres container\\n*/\\nimport { PostgresRecordManager } from \\'@langchain/community/indexes/postgres\\';\\nimport { index } from \\'langchain/indexes\\';\\nimport { OpenAIEmbeddings } from \\'@langchain/openai\\';\\nimport { PGVectorStore } from \\'@langchain/community/vectorstores/pgvector\\';\\nimport { v4 as uuidv4 } from \\'uuid\\';\\nconst tableName = \\'test_langchain\\';\\nconst connectionString =\\n  \\'postgresql://langchain:langchain@localhost:6024/langchain\\';\\n// Load the document, split it into chunks\\n46 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 74, 'page_label': '47'}, page_content=\"const config = {\\n  postgresConnectionOptions: {\\n    connectionString,\\n  },\\n  tableName: tableName,\\n  columns: {\\n    idColumnName: 'id',\\n    vectorColumnName: 'vector',\\n    contentColumnName: 'content',\\n    metadataColumnName: 'metadata',\\n  },\\n};\\nconst vectorStore = await PGVectorStore.initialize(\\n  new OpenAIEmbeddings(),\\n  config\\n);\\n// Create a new record manager\\nconst recordManagerConfig = {\\n  postgresConnectionOptions: {\\n    connectionString,\\n  },\\n  tableName: 'upsertion_records',\\n};\\nconst recordManager = new PostgresRecordManager(\\n  'test_namespace',\\n  recordManagerConfig\\n);\\n// Create the schema if it doesn't exist\\nawait recordManager.createSchema();\\nconst docs = [\\n  {\\n    pageContent: 'there are cats in the pond',\\n    metadata: { id: uuidv4(), source: 'cats.txt' },\\n  },\\n  {\\n    pageContent: 'ducks are also found in the pond',\\n    metadata: { id: uuidv4(), source: 'ducks.txt' },\\n  },\\n];\\n// the first attempt will index both documents\\nconst index_attempt_1 = await index({\\n  docsSource: docs,\\n  recordManager,\\n  vectorStore,\\n  options: {\\n    // prevent duplicate documents by id from being indexed\\n    cleanup: 'incremental',\\nTracking Changes to Your Documents | 47\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 75, 'page_label': '48'}, page_content=\"// the key in the metadata that will be used to identify the document \\n    sourceIdKey: 'source', \\n  },\\n});\\nconsole.log(index_attempt_1);\\n// the second attempt will skip indexing because the identical documents \\n// already exist\\nconst index_attempt_2 = await index({\\n  docsSource: docs,\\n  recordManager,\\n  vectorStore,\\n  options: {\\n    cleanup: 'incremental',\\n    sourceIdKey: 'source',\\n  },\\n});\\nconsole.log(index_attempt_2);\\n// If we mutate a document, the new version will be written and all old \\n// versions sharing the same source will be deleted.\\ndocs[0].pageContent = 'I modified the first document content';\\nconst index_attempt_3 = await index({\\n  docsSource: docs,\\n  recordManager,\\n  vectorStore,\\n  options: {\\n    cleanup: 'incremental',\\n    sourceIdKey: 'source',\\n  },\\n});\\nconsole.log(index_attempt_3);\\nFirst, you create a record manager, which keeps track of which documents have been\\nindexed before. Then you use the index function to synchronize your vector store\\nwith the new list of documents. In this example, we‚Äôre using the incremental mode, so\\nany documents that have the same ID as previous ones will be replaced with the new\\nversion.\\nIndexing Optimization\\nA basic RAG indexing stage involves naive text splitting and embedding of chunks of\\na given document. However, this basic approach leads to inconsistent retrieval results\\nand a relatively high occurrence of hallucinations, especially when the data source\\ncontains images and tables.\\n48 | Chapter 2: RAG Part I: Indexing Your Data\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 76, 'page_label': '49'}, page_content='There are various strategies to enhance the accuracy and performance of the indexing\\nstage. We will cover three of them in the next sections: MultiVectorRetriever, RAP‚Äê\\nTOR, and ColBERT.\\nMultiVectorRetriever\\nA document that contains a mixture of text and tables cannot be simply split by text\\ninto chunks and embedded as context: the entire table can be easily lost. To solve this\\nproblem, we can decouple documents that we want to use for answer synthesis, from\\na reference that we want to use for the retriever. Figure 2-5 illustrates how.\\nFigure 2-5. Indexing multiple representations of a single document\\nFor example, in the case of a document that contains tables, we can first generate\\nand embed summaries of table elements, ensuring each summary contains an id\\nreference to the full raw table. Next, we store the raw referenced tables in a separate\\ndocstore. Finally, when a user‚Äôs query retrieves a table summary, we pass the entire\\nreferenced raw table as context to the final prompt sent to the LLM for answer\\nsynthesis. This approach enables us to provide the model with the full context of\\ninformation required to answer the question.\\nHere‚Äôs an example. First, let‚Äôs use the LLM to generate summaries of the documents:\\nPython\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_postgres.vectorstores import PGVector\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom pydantic import BaseModel\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI\\nIndexing Optimization | 49'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 77, 'page_label': '50'}, page_content='from langchain_core.documents import Document\\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\\nfrom langchain.storage import InMemoryStore\\nimport uuid\\nconnection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\\ncollection_name = \"summaries\"\\nembeddings_model = OpenAIEmbeddings()\\n# Load the document\\nloader = TextLoader(\"./test.txt\", encoding=\"utf-8\")\\ndocs = loader.load()\\nprint(\"length of loaded docs: \", len(docs[0].page_content))\\n# Split the document\\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nchunks = splitter.split_documents(docs)\\n# The rest of your code remains the same, starting from:\\nprompt_text = \"Summarize the following document:\\\\n\\\\n{doc}\"\\nprompt = ChatPromptTemplate.from_template(prompt_text)\\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\\nsummarize_chain = {\\n    \"doc\": lambda x: x.page_content} | prompt | llm | StrOutputParser()\\n# batch the chain across the chunks\\nsummaries = summarize_chain.batch(chunks, {\"max_concurrency\": 5})\\nNext, let‚Äôs define the vector store and docstore to store the raw summaries and their\\nembeddings:\\nPython\\n# The vectorstore to use to index the child chunks\\nvectorstore = PGVector(\\n    embeddings=embeddings_model,\\n    collection_name=collection_name,\\n    connection=connection,\\n    use_jsonb=True,\\n)\\n# The storage layer for the parent documents\\nstore = InMemoryStore()\\nid_key = \"doc_id\"\\n# indexing the summaries in our vector store, whilst retaining the original \\n# documents in our document store:\\nretriever = MultiVectorRetriever(\\n    vectorstore=vectorstore,\\n    docstore=store,\\n    id_key=id_key,\\n)\\n# Changed from summaries to chunks since we need same length as docs\\n50 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 78, 'page_label': '51'}, page_content='doc_ids = [str(uuid.uuid4()) for _ in chunks]\\n# Each summary is linked to the original document by the doc_id\\nsummary_docs = [\\n    Document(page_content=s, metadata={id_key: doc_ids[i]})\\n    for i, s in enumerate(summaries)\\n]\\n# Add the document summaries to the vector store for similarity search\\nretriever.vectorstore.add_documents(summary_docs)\\n# Store the original documents in the document store, linked to their summaries \\n# via doc_ids\\n# This allows us to first search summaries efficiently, then fetch the full \\n# docs when needed\\nretriever.docstore.mset(list(zip(doc_ids, chunks)))\\n# vector store retrieves the summaries\\nsub_docs = retriever.vectorstore.similarity_search(\\n    \"chapter on philosophy\", k=2)\\nFinally, let‚Äôs retrieve the relevant full context document based on a query:\\nPython\\n# Whereas the retriever will return the larger source document chunks:\\nretrieved_docs = retriever.invoke(\"chapter on philosophy\")\\nHere‚Äôs the full implementation in JavaScript:\\nJavaScript\\nimport * as uuid from \\'uuid\\';\\nimport { MultiVectorRetriever } from \\'langchain/retrievers/multi_vector\\';\\nimport { OpenAIEmbeddings } from \\'@langchain/openai\\';\\nimport { RecursiveCharacterTextSplitter } from \\'@langchain/textsplitters\\';\\nimport { InMemoryStore } from \\'@langchain/core/stores\\';\\nimport { TextLoader } from \\'langchain/document_loaders/fs/text\\';\\nimport { Document } from \\'@langchain/core/documents\\';\\nimport { PGVectorStore } from \\'@langchain/community/vectorstores/pgvector\\';\\nimport { ChatOpenAI } from \\'@langchain/openai\\';\\nimport { PromptTemplate } from \\'@langchain/core/prompts\\';\\nimport { RunnableSequence } from \\'@langchain/core/runnables\\';\\nimport { StringOutputParser } from \\'@langchain/core/output_parsers\\';\\nconst connectionString =\\n  \\'postgresql://langchain:langchain@localhost:6024/langchain\\';\\nconst collectionName = \\'summaries\\';\\nconst textLoader = new TextLoader(\\'./test.txt\\');\\nconst parentDocuments = await textLoader.load();\\nconst splitter = new RecursiveCharacterTextSplitter({\\n  chunkSize: 10000,\\nIndexing Optimization | 51'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 79, 'page_label': '52'}, page_content=\"chunkOverlap: 20,\\n});\\nconst docs = await splitter.splitDocuments(parentDocuments);\\nconst prompt = PromptTemplate.fromTemplate(\\n  `Summarize the following document:\\\\n\\\\n{doc}`\\n);\\nconst llm = new ChatOpenAI({ modelName: 'gpt-3.5-turbo' });\\nconst chain = RunnableSequence.from([\\n  { doc: (doc) => doc.pageContent },\\n  prompt,\\n  llm,\\n  new StringOutputParser(),\\n]);\\n// batch summarization chain across the chunks\\nconst summaries = await chain.batch(docs, {\\n  maxConcurrency: 5,\\n});\\nconst idKey = 'doc_id';\\nconst docIds = docs.map((_) => uuid.v4());\\n// create summary docs with metadata linking to the original docs\\nconst summaryDocs = summaries.map((summary, i) => {\\n  const summaryDoc = new Document({\\n    pageContent: summary,\\n    metadata: {\\n      [idKey]: docIds[i],\\n    },\\n  });\\n  return summaryDoc;\\n});\\n// The byteStore to use to store the original chunks\\nconst byteStore = new InMemoryStore();\\n// vector store for the summaries\\nconst vectorStore = await PGVectorStore.fromDocuments(\\n  docs,\\n  new OpenAIEmbeddings(),\\n  {\\n    postgresConnectionOptions: {\\n      connectionString,\\n    },\\n  }\\n);\\nconst retriever = new MultiVectorRetriever({\\n  vectorstore: vectorStore,\\n  byteStore,\\n52 | Chapter 2: RAG Part I: Indexing Your Data\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 80, 'page_label': '53'}, page_content=\"2 Parth Sarthi et al., ‚ÄúRAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval‚Äù, arXiv, January\\n31, 2024. Paper published at ICLR 2024.\\n  idKey,\\n});\\nconst keyValuePairs = docs.map((originalDoc, i) => [docIds[i], originalDoc]);\\n// Use the retriever to add the original chunks to the document store\\nawait retriever.docstore.mset(keyValuePairs);\\n// Vectorstore alone retrieves the small chunks\\nconst vectorstoreResult = await retriever.vectorstore.similaritySearch(\\n  'chapter on philosophy',\\n  2\\n);\\nconsole.log(`summary: ${vectorstoreResult[0].pageContent}`);\\nconsole.log(\\n  `summary retrieved length: ${vectorstoreResult[0].pageContent.length}`\\n);\\n// Retriever returns larger chunk result\\nconst retrieverResult = await retriever.invoke('chapter on philosophy');\\nconsole.log(\\n  `multi-vector retrieved chunk length: ${retrieverResult[0].pageContent.length}`\\n);\\nRAPTOR: Recursive Abstractive Processing for\\nTree-Organized Retrieval\\nRAG systems need to handle lower-level questions that reference specific facts found\\nin a single document or higher-level questions that distill ideas that span many docu‚Äê\\nments. Handling both types of questions can be a challenge with typical k-nearest\\nneighbors (k-NN) retrieval over document chunks.\\nRecursive abstractive processing for tree-organized retrieval  (RAPTOR) is an effective\\nstrategy that involves creating document summaries that capture higher-level con‚Äê\\ncepts, embedding and clustering those documents, and then summarizing each\\ncluster.2 This is done recursively, producing a tree of summaries with increasingly\\nhigh-level concepts. The summaries and initial documents are then indexed together,\\ngiving coverage across lower-to-higher-level user questions. Figure 2-6 illustrates.\\nIndexing Optimization | 53\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 81, 'page_label': '54'}, page_content='Figure 2-6. Recursively summarizing documents\\nColBERT: Optimizing Embeddings\\nOne of the challenges of using embeddings models during the indexing stage is\\nthat they compress text into fixed-length (vector) representations that capture the\\nsemantic content of the document. Although this compression is useful for retrieval,\\nembedding irrelevant or redundant content may lead to hallucinations in the final\\nLLM output.\\nOne solution to this problem is to do the following:\\n1. Generate contextual embeddings for each token in the document and query.1.\\n2. Calculate and score similarity between each query token and all document2.\\ntokens.\\n3. Sum the maximum similarity score of each query embedding to any of the3.\\ndocument embeddings to get a score for each document.\\n54 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 82, 'page_label': '55'}, page_content='3 Keshav Santhanam et al., ‚ÄúColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction‚Äù,\\narXiv, December 2, 2021.\\nThis results in a granular and effective embedding approach for better retrieval.\\nFortunately, the embedding model called ColBERT  embodies the solution to this\\nproblem.3\\nHere‚Äôs how we can utilize ColBERT for optimal embedding of our data:\\nPython\\n# RAGatouille is a library that makes it simple to use ColBERT\\n#! pip install -U ragatouille\\nfrom ragatouille import RAGPretrainedModel\\nRAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\\nimport requests\\ndef get_wikipedia_page(title: str):\\n    \"\"\"\\n    Retrieve the full text content of a Wikipedia page.\\n    :param title: str - Title of the Wikipedia page.\\n    :return: str - Full text content of the page as raw string.\\n    \"\"\"\\n    # Wikipedia API endpoint\\n    URL = \"https://en.wikipedia.org/w/api.php\"\\n    # Parameters for the API request\\n    params = {\\n        \"action\": \"query\",\\n        \"format\": \"json\",\\n        \"titles\": title,\\n        \"prop\": \"extracts\",\\n        \"explaintext\": True,\\n    }\\n    # Custom User-Agent header to comply with Wikipedia\\'s best practices\\n    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1\"}\\n    response = requests.get(URL, params=params, headers=headers)\\n    data = response.json()\\n    # Extracting page content\\n    page = next(iter(data[\"query\"][\"pages\"].values()))\\n    return page[\"extract\"] if \"extract\" in page else None\\nfull_document = get_wikipedia_page(\"Hayao_Miyazaki\")\\n## Create an index\\nIndexing Optimization | 55'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 83, 'page_label': '56'}, page_content='RAG.index(\\n    collection=[full_document],\\n    index_name=\"Miyazaki-123\",\\n    max_document_length=180,\\n    split_documents=True,\\n)\\n#query\\nresults = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\\nresults\\n#utilize langchain retriever\\nretriever = RAG.as_langchain_retriever(k=3)\\nretriever.invoke(\"What animation studio did Miyazaki found?\")\\nBy using ColBERT, you can improve the relevancy of retrieved documents used as\\ncontext by the LLM.\\nSummary\\nIn this chapter, you‚Äôve learned how to prepare and preprocess your documents for\\nyour LLM application using various LangChain‚Äôs modules. The document loaders\\nenable you to extract text from your data source, the text splitters help you split your\\ndocument into semantically similar chunks, and the embeddings models convert your\\ntext into vector representations of their meaning.\\nSeparately, vector stores allow you to perform CRUD operations on these embed‚Äê\\ndings alongside complex calculations to compute semantically similar chunks of text.\\nFinally, indexing optimization strategies enable your AI app to improve the quality of\\nembeddings and perform accurate retrieval of documents that contain semistructured\\ndata including tables.\\nIn Chapter 3 , you‚Äôll learn how to efficiently retrieve the most similar chunks of\\ndocuments from your vector store based on your query, provide it as context  the\\nmodel can see, and then generate an accurate output.\\n56 | Chapter 2: RAG Part I: Indexing Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 84, 'page_label': '57'}, page_content='1 Patrick Lewis et al., ‚ÄúRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks‚Äù, arXiv, April 12,\\n2021.\\nCHAPTER 3\\nRAG Part II: Chatting with Your Data\\nIn the previous chapter, you learned how to process your data and create and store\\nembeddings in a vector store. In this chapter, you‚Äôll learn how to efficiently retrieve\\nthe most relevant embeddings and chunks of documents based on a user‚Äôs query.\\nThis enables you to construct a prompt that contains relevant documents as context,\\nimproving the accuracy of the LLM‚Äôs final output.\\nThis process‚Äîwhich involves embedding a user‚Äôs query, retrieving similar documents\\nfrom a data source, and then passing them as context to the prompt sent to the\\nLLM‚Äîis formally known as retrieval-augmented generation (RAG).\\nRAG is an essential component of building chat-enabled LLM apps that are accurate,\\nefficient, and up-to-date. In this chapter, you‚Äôll progress from basics to advanced\\nstrategies to build an effective RAG system for various data sources (such as vector\\nstores and databases) and data structures (structured and unstructured).\\nBut first, let‚Äôs define RAG and discuss its benefits.\\nIntroducing Retrieval-Augmented Generation\\nRAG is a technique used to enhance the accuracy of outputs generated by LLMs by\\nproviding context from external sources. The term was originally coined in a paper\\nby Meta AI researchers who discovered that RAG-enabled models are more factual\\nand specific than non-RAG models.1\\n57'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 85, 'page_label': '58'}, page_content=\"Without RAG, the LLM relies solely on its pretrained data, which may be outdated.\\nFor example, let‚Äôs ask ChatGPT a question about a current event and see its response:\\nInput\\nWhich country is the latest winner of the men‚Äôs FIFA World Cup?\\nOutput\\nThe most recent FIFA World Cup winner was France, who won the tournament in 2018.\\nThe response by the LLM is factually incorrect and outdated. The latest winner at\\nthe time of this book‚Äôs publication is Argentina, who won the World Cup in 2022.\\nWhile this example question may be trivial, LLM hallucination can have disastrous\\nconsequences if its answers are relied upon for fact-checking or important decision\\nmaking.\\nTo combat this problem, we need to provide the LLM with factual, up-to-date infor‚Äê\\nmation from which it can formulate an accurate response. Continuing on from the\\nprevious example, let‚Äôs go over to Wikipedia‚Äôs page for the FIFA World Cup, copy the\\nintroduction paragraph, and then append it as context to our prompt to ChatGPT:\\nWhich country is the latest winner of the men's FIFA World Cup?\\nSee context below.\\nThe FIFA World Cup, often called the World Cup, is an international association\\nfootball competition among the senior men's national teams of the members of\\nthe F√©d√©ration Internationale de Football Association (FIFA), the sport's \\nglobal governing body. The tournament has been held every four years since the \\ninaugural tournament in 1930, with the exception of 1942 and 1946 due to the \\nSecond World War. The reigning champions are Argentina, who won their third \\ntitle at the 2022 tournament.\\nNote that the last sentence contains the necessary context the LLM can use to provide\\nan accurate answer. Here‚Äôs the response from the LLM:\\nThe latest winner of the men's FIFA World Cup is Argentina, who won their third\\ntitle at the 2022 tournament.\\nBecause of the up-to-date additional context provided, the LLM was able to generate\\nan accurate response to the prompt. But copying and pasting relevant information\\nas context isn‚Äôt practical nor scalable for a production AI application. We need an\\nautomated system to fetch relevant information based on a user‚Äôs query, append it as\\ncontext to the prompt, and then execute the generation request to the LLM.\\n58 | Chapter 3: RAG Part II: Chatting with Your Data\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 86, 'page_label': '59'}, page_content='Retrieving Relevant Documents\\nA RAG system for an AI app typically follows three core stages:\\nIndexing\\nThis stage involves preprocessing the external data source and storing embed‚Äê\\ndings that represent the data in a vector store where they can be easily retrieved.\\nRetrieval\\nThis stage involves retrieving the relevant embeddings and data stored in the\\nvector store based on a user‚Äôs query.\\nGeneration\\nThis stage involves synthesizing the original prompt with the retrieved relevant\\ndocuments as one final prompt sent to the model for a prediction.\\nThe three basic stages look like Figure 3-1.\\nFigure 3-1. The key stages of RAG\\nThe indexing stage of this process was covered extensively in Chapter 2, where you\\nlearned how to use document loaders, text splitters, embeddings, and vector stores.\\nIntroducing Retrieval-Augmented Generation | 59'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 87, 'page_label': '60'}, page_content='Let‚Äôs run through an example from scratch again, starting with the indexing stage:\\nPython\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_postgres.vectorstores import PGVector\\n# Load the document, split it into chunks\\nraw_documents = TextLoader(\\'./test.txt\\').load()\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, \\n    chunk_overlap=200)\\ndocuments = text_splitter.split_documents(raw_documents)\\n# embed each chunk and insert it into the vector store\\nmodel = OpenAIEmbeddings()\\nconnection = \\'postgresql+psycopg://langchain:langchain@localhost:6024/langchain\\'\\ndb = PGVector.from_documents(documents, model, connection=connection)\\nJavaScript\\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\\nimport { PGVectorStore } from \"@langchain/community/vectorstores/pgvector\";\\n// Load the document, split it into chunks\\nconst loader = new TextLoader(\"./test.txt\");\\nconst raw_docs = await loader.load();\\nconst splitter = new RecursiveCharacterTextSplitter({\\n  chunkSize: 1000,\\n  chunkOverlap: 200,\\n});\\nconst docs = await splitter.splitDocuments(docs)\\n// embed each chunk and insert it into the vector store\\nconst model = new OpenAIEmbeddings();\\nconst db = await PGVectorStore.fromDocuments(docs, model, {\\n  postgresConnectionOptions: {\\n    connectionString: \\'postgresql://langchain:langchain@localhost:6024/langchain\\'\\n  }\\n})\\nChapter 2 has more details on the indexing stage.\\nThe indexing stage is now complete. In order to execute the retrieval stage, we need\\nto perform similarity search calculations‚Äîsuch as cosine similarity‚Äîbetween the\\nuser‚Äôs query and our stored embeddings, so relevant chunks of our indexed document\\nare retrieved (see Figure 3-2).\\n60 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 88, 'page_label': '61'}, page_content='Figure 3-2. An example flow of indexing documents alongside retrieval of relevant\\ndocuments from a vector store; the Hierarchical Navigable Small World (HNSW) box\\ndepicts calculating similarity of documents against the user‚Äôs query\\nFigure 3-2 illustrates the steps in the retrieval process:\\n1. Convert the user‚Äôs query into embeddings.1.\\n2. Calculate the embeddings in the vector store that are most similar to the user‚Äôs2.\\nquery.\\n3. Retrieve the relevant document embeddings and their corresponding text chunk.3.\\nIntroducing Retrieval-Augmented Generation | 61'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 89, 'page_label': '62'}, page_content='We can represent these steps programmatically using LangChain as follows:\\nPython\\n# create retriever\\nretriever = db.as_retriever()\\n# fetch relevant documents\\ndocs = retriever.invoke(\"\"\"Who are the key figures in the ancient greek \\n    history of philosophy?\"\"\")\\nJavaScript\\n// create retriever\\nconst retriever = db.asRetriever()\\n// fetch relevant documents\\nconst docs = await retriever.invoke(`Who are the key figures in the ancient \\n  greek history of philosophy?`)\\nNote that we are using a vector store method you haven‚Äôt seen before: as_retriever.\\nThis function abstracts the logic of embedding the user‚Äôs query and the underlying\\nsimilarity search calculations performed by the vector store to retrieve the relevant\\ndocuments.\\nThere is also an argument k, which determines the number of relevant documents to\\nfetch from the vector store. For example:\\nPython\\n# create retriever with k=2\\nretriever = db.as_retriever(search_kwargs={\"k\": 2})\\n# fetch the 2 most relevant documents\\ndocs = retriever.invoke(\"\"\"Who are the key figures in the ancient greek history \\n    of philosophy?\"\"\")\\nJavaScript\\n// create retriever with k=2\\nconst retriever = db.asRetriever({k: 2})\\n// fetch the 2 most relevant documents\\nconst docs = await retriever.invoke(`Who are the key figures in the ancient \\n  greek history of philosophy?`)\\nIn this example, the argument k is specified as 2. This tells the vector store to return\\nthe two most relevant documents based on the user‚Äôs query.\\nIt may seem counterintuitive to use a low k value, but retrieving more documents is\\nnot always better. The more documents are retrieved, the slower your application will\\nperform, the larger the prompt (and associated cost of generation) will be, and the\\n62 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 90, 'page_label': '63'}, page_content='greater the likelihood of retrieving chunks of text that contain irrelevant information,\\nwhich will cause the LLM to hallucinate.\\nNow that we‚Äôve completed the retrieval stage of the RAG system, let‚Äôs move on to the\\nfinal generation stage.\\nGenerating LLM Predictions Using Relevant Documents\\nOnce we‚Äôve retrieved the relevant documents based on the user‚Äôs query, the final\\nstep is to add them to the original prompt as context and then invoke the model to\\ngenerate a final output (Figure 3-3).\\nFigure 3-3. An example flow demonstrating indexing documents, retrieval of relevant\\ndocuments from a vector store, and inclusion of retrieved documents as context in the\\nLLM prompt\\nIntroducing Retrieval-Augmented Generation | 63'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 91, 'page_label': '64'}, page_content='Here‚Äôs a code example continuing on from our previous example:\\nPython\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nretriever = db.as_retriever()\\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on \\n    the following context:\\n{context}\\nQuestion: {question}\\n\"\"\")\\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\nchain = prompt | llm\\n# fetch relevant documents \\ndocs = retriever.get_relevant_documents(\"\"\"Who are the key figures in the \\n    ancient greek history of philosophy?\"\"\")\\n# run\\nchain.invoke({\"context\": docs,\"question\": \"\"\"Who are the key figures in the \\n    ancient greek history of philosophy?\"\"\"})\\nJavaScript\\nimport {ChatOpenAI} from \\'@langchain/openai\\'\\nimport {ChatPromptTemplate} from \\'@langchain/core/prompts\\'\\nconst retriever = db.asRetriever()\\nconst prompt = ChatPromptTemplate.fromTemplate(`Answer the question based only \\n  on the following context:\\n{context}\\nQuestion: {question}\\n`)\\nconst llm = new ChatOpenAI({temperature: 0, modelName: \\'gpt-3.5-turbo\\'})\\nconst chain = prompt.pipe(llm)\\n// fetch relevant documents\\nconst docs = await retriever.invoke(`Who are the key figures in the ancient \\n  greek history of philosophy?`)\\nawait chain.invoke({context: docs, question: `Who are the key figures in the \\n  ancient greek history of philosophy?`})\\n64 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 92, 'page_label': '65'}, page_content='Note the following changes:\\n‚Ä¢ We implement dynamic context and question variables into our prompt, which‚Ä¢\\nallows us to define a ChatPromptTemplate the model can use to generate a\\nresponse.\\n‚Ä¢ We define a ChatOpenAI interface to act as our LLM. Temperature is set to 0 to‚Ä¢\\neliminate the creativity in outputs from the model.\\n‚Ä¢ We create a chain to compose the prompt and LLM. A reminder: the | operator‚Ä¢\\n(or pipe method in JS) takes the output of prompt and uses it as the input to llm.\\n‚Ä¢ We invoke the chain passing in the context variable (our retrieved relevant‚Ä¢\\ndocs) and the user‚Äôs question to generate a final output.\\nWe can encapsulate this retrieval logic in a single function:\\nPython\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import chain\\nretriever = db.as_retriever()\\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on \\n    the following context:\\n{context}\\nQuestion: {question}\\n\"\"\")\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\n@chain\\ndef qa(input):\\n    # fetch relevant documents \\n    docs = retriever.get_relevant_documents(input)\\n    # format prompt\\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\\n    # generate answer\\n    answer = llm.invoke(formatted)\\n    return answer\\n# run\\nqa.invoke(\"Who are the key figures in the ancient greek history of philosophy?\")\\nJavaScript\\nimport {ChatOpenAI} from \\'@langchain/openai\\'\\nimport {ChatPromptTemplate} from \\'@langchain/core/prompts\\'\\nimport {RunnableLambda} from \\'@langchain/core/runnables\\'\\nIntroducing Retrieval-Augmented Generation | 65'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 93, 'page_label': '66'}, page_content='const retriever = db.asRetriever()\\nconst prompt = ChatPromptTemplate.fromTemplate(`Answer the question based only \\n  on the following context:\\n{context}\\nQuestion: {question}\\n`)\\nconst llm = new ChatOpenAI({temperature: 0, modelName: \\'gpt-3.5-turbo\\'})\\nconst qa = RunnableLambda.from(async input => {\\n  // fetch relevant documents\\n  const docs = await retriever.invoke(input)\\n  // format prompt\\n  const formatted = await prompt.invoke({context: docs, question: input})\\n  // generate answer\\n  const answer = await llm.invoke(formatted)\\n  return answer\\n})\\nawait qa.invoke(`Who are the key figures in the ancient greek history of \\n  philosophy?`)\\nNotice how we now have a new runnable qa function that can be called with just\\na question and takes care to first fetch the relevant docs for context, format them\\ninto the prompt, and finally generate the answer. In the Python code, the @chain\\ndecorator turns the function into a runnable chain. This notion of encapsulating\\nmultiple steps into a single function will be key to building interesting apps with\\nLLMs.\\nY ou can also return the retrieved documents for further inspection:\\nPython\\n@chain\\ndef qa(input):\\n    # fetch relevant documents \\n    docs = retriever.get_relevant_documents(input)\\n    # format prompt\\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\\n    # generate answer\\n    answer = llm.invoke(formatted)\\n    return {\"answer\": answer, \"docs\": docs}\\nJavaScript\\nconst qa = RunnableLambda.from(async input => {\\n  // fetch relevant documents\\n  const docs = await retriever.invoke(input)\\n  // format prompt\\n  const formatted = await prompt.invoke({context: docs, question: input})\\n66 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 94, 'page_label': '67'}, page_content='// generate answer\\n  const answer = await llm.invoke(formatted)\\n  return {answer, docs}\\n})\\nCongratulations! Y ou‚Äôve now built a basic RAG system to power an AI app for\\npersonal use.\\nHowever, a production-ready AI app used by multiple users requires a more\\nadvanced RAG system. In order to build a robust RAG system, we need to answer the\\nfollowing questions effectively:\\n‚Ä¢ How do we handle the variability in the quality of a user‚Äôs input?‚Ä¢\\n‚Ä¢ How do we route queries to retrieve relevant data from a variety of data sources?‚Ä¢\\n‚Ä¢ How do we transform natural language to the query language of the target data‚Ä¢\\nsource?\\n‚Ä¢ How do we optimize our indexing process, i.e., embedding, text splitting?‚Ä¢\\nNext we‚Äôll discuss the latest research-backed strategies to answer these questions\\nand build a production-ready RAG system. These strategies can be summarized in\\nFigure 3-4.\\nFigure 3-4. Effective strategies to optimize the accuracy of your RAG system\\nIntroducing Retrieval-Augmented Generation | 67'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 95, 'page_label': '68'}, page_content='2 Xinbei Ma et al., ‚ÄúQuery Rewriting for Retrieval-Augmented Large Language Models‚Äù, arXiv, October 23, 2023.\\nResearch commissioned by Microsoft Research Asia.\\nAll code blocks in the rest of this chapter use the vector store we set\\nup at the beginning of the chapter.\\nQuery Transformation\\nOne of the major problems with a basic RAG system is that it relies too heavily on\\nthe quality of a user‚Äôs query to generate an accurate output. In a production setting, a\\nuser is likely to construct their query in an incomplete, ambiguous, or poorly worded\\nmanner that leads to model hallucination.\\nQuery transformation is a subset of strategies designed to modify the user‚Äôs input to\\nanswer the first RAG problem question: How do we handle the variability in the\\nquality of a user‚Äôs input? Figure 3-5  illustrates the range of query transformation\\nstrategies, ranging from those that make a user‚Äôs input more or less abstract in order\\nto generate an accurate LLM output. The next section begins with a middle ground\\nstrategy.\\nFigure 3-5. Various methods to transform a user‚Äôs query based on the abstraction level\\nRewrite-Retrieve-Read\\nThe Rewrite-Retrieve-Read strategy proposed by a Microsoft Research team simply\\nprompts the LLM to rewrite the user‚Äôs query before performing retrieval.2 To illustrate,\\nlet‚Äôs return to the chain we built in the previous section, this time invoked with a poorly\\nworded user query:\\n68 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 96, 'page_label': '69'}, page_content='Python\\n@chain\\ndef qa(input):\\n    # fetch relevant documents \\n    docs = retriever.get_relevant_documents(input)\\n    # format prompt\\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\\n    # generate answer\\n    answer = llm.invoke(formatted)\\n    return answer\\nqa.invoke(\"\"\"Today I woke up and brushed my teeth, then I sat down to read the \\n    news. But then I forgot the food on the cooker. Who are some key figures in \\n    the ancient greek history of philosophy?\"\"\")\\nJavaScript\\nconst qa = RunnableLambda.from(async input => {\\n  // fetch relevant documents\\n  const docs = await retriever.invoke(input)\\n  // format prompt\\n  const formatted = await prompt.invoke({context: docs, question: input})\\n  // generate answer\\n  const answer = await llm.invoke(formatted)\\n  return answer\\n})\\nawait qa.invoke(`Today I woke up and brushed my teeth, then I sat down to read \\n  the news. But then I forgot the food on the cooker. Who are some key figures \\n  in the ancient greek history of philosophy?`)\\nThe output (remember: if you rerun it, your output might be different from this):\\nBased on the given context, there is no information provided.\\nThe model failed to answer the question because it was distracted by the irrelevant\\ninformation provided in the user‚Äôs query.\\nNow let‚Äôs implement the Rewrite-Retrieve-Read prompt:\\nPython\\nrewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Provide a better search \\n    query for web search engine to answer the given question, end the queries \\n    with ‚Äô**‚Äô. Question: {x} Answer:\"\"\")\\ndef parse_rewriter_output(message):\\n    return message.content.strip(\\'\"\\').strip(\"**\")\\nrewriter = rewrite_prompt | llm | parse_rewriter_output\\n@chain\\ndef qa_rrr(input):\\nQuery Transformation | 69'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 97, 'page_label': '70'}, page_content='# rewrite the query\\n    new_query = rewriter.invoke(input)\\n    # fetch relevant documents \\n    docs = retriever.get_relevant_documents(new_query)\\n    # format prompt\\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\\n    # generate answer\\n    answer = llm.invoke(formatted)\\n    return answer\\n# run\\nqa_rrr.invoke(\"\"\"Today I woke up and brushed my teeth, then I sat down to read \\n    the news. But then I forgot the food on the cooker. Who are some key \\n    figures in the ancient greek history of philosophy?\"\"\")\\nJavaScript\\nconst rewritePrompt = ChatPromptTemplate.fromTemplate(`Provide a better search \\n  query for web search engine to answer the given question, end the queries \\n  with ‚Äô**‚Äô. Question: {question} Answer:`)\\nconst rewriter = rewritePrompt.pipe(llm).pipe(message => {\\n  return message.content.replaceAll(\\'\"\\', \\'\\').replaceAll(\\'**\\')\\n})\\nconst qa = RunnableLambda.from(async input => {\\n  const newQuery = await rewriter.invoke({question: input});\\n  // fetch relevant documents\\n  const docs = await retriever.invoke(newQuery)\\n  // format prompt\\n  const formatted = await prompt.invoke({context: docs, question: input})\\n  // generate answer\\n  const answer = await llm.invoke(formatted)\\n  return answer\\n})\\nawait qa.invoke(`Today I woke up and brushed my teeth, then I sat down to read \\n  the news. But then I forgot the food on the cooker. Who are some key \\n  figures in the ancient greek history of philosophy?`)\\nThe output:\\nBased on the given context, some key figures in the ancient greek history of \\nphilosophy include: Themistocles (an Athenian statesman), Pythagoras, and Plato.\\nNotice that we have had an LLM rewrite the user‚Äôs initial distracted query into a\\nmuch clearer one, and it is that more focused query that is passed to the retriever\\nto fetch the most relevant documents. Note: this technique can be used with any\\nretrieval method, be that a vector store such as we have here or, for instance, a web\\nsearch tool. The downside of this approach is that it introduces additional latency\\ninto your chain, because now we need to perform two LLM calls in sequence.\\n70 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 98, 'page_label': '71'}, page_content='Multi-Query Retrieval\\nA user‚Äôs single query can be insufficient to capture the full scope of information\\nrequired to answer the query comprehensively. The multi-query retrieval strategy\\nresolves this problem by instructing an LLM to generate multiple queries based on a\\nuser‚Äôs initial query, executing a parallel retrieval of each query from the data source\\nand then inserting the retrieved results as prompt context to generate a final model\\noutput. Figure 3-6 illustrates.\\nFigure 3-6. Demonstration of the multi-query retrieval strategy\\nThis strategy is particularly useful for use cases where a single question may rely on\\nmultiple perspectives to provide a comprehensive answer.\\nHere‚Äôs a code example of multi-query retrieval in action:\\nPython\\nfrom langchain.prompts import ChatPromptTemplate\\nperspectives_prompt = ChatPromptTemplate.from_template(\"\"\"You are an AI language \\n    model assistant. Your task is to generate five different versions of the \\n    given user question to retrieve relevant documents from a vector database. \\n    By generating multiple perspectives on the user question, your goal is to \\n    help the user overcome some of the limitations of the distance-based \\n    similarity search. Provide these alternative questions separated by \\n    newlines. Original question: {question}\"\"\")\\ndef parse_queries_output(message):\\n    return message.content.split(\\'\\\\n\\')\\nquery_gen = perspectives_prompt | llm | parse_queries_output\\nQuery Transformation | 71'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 99, 'page_label': '72'}, page_content=\"JavaScript\\nconst perspectivesPrompt = ChatPromptTemplate.fromTemplate(`You are an AI \\n  language model assistant. Your task is to generate five different versions \\n  of the given user question to retrieve relevant documents from a vector \\n  database. By generating multiple perspectives on the user question, your \\n  goal is to help the user overcome some of the limitations of the \\n  distance-based similarity search. Provide these alternative questions \\n  separated by newlines. Original question: {question}`)\\nconst queryGen = perspectivesPrompt.pipe(llm).pipe(message => {\\n  return message.content.split('\\\\n')\\n})\\nNote that the prompt template is designed to generate variations of questions based\\non the user‚Äôs initial query.\\nNext we take the list of generated queries, retrieve the most relevant docs for each\\nof them in parallel, and then combine to get the unique union of all the retrieved\\nrelevant documents:\\nPython\\ndef get_unique_union(document_lists):\\n    # Flatten list of lists, and dedupe them\\n    deduped_docs = {\\n        doc.page_content: doc\\n        for sublist in document_lists for doc in sublist\\n    }\\n    # return a flat list of unique docs\\n    return list(deduped_docs.values())\\nretrieval_chain = query_gen | retriever.batch | get_unique_union\\nJavaScript\\nconst retrievalChain = queryGen\\n  .pipe(retriever.batch.bind(retriever))\\n  .pipe(documentLists => {\\n    const dedupedDocs = {}\\n    documentLists.flat().forEach(doc => {\\n      dedupedDocs[doc.pageContent] = doc\\n    })\\n    return Object.values(dedupedDocs)\\n  })\\nBecause we‚Äôre retrieving documents from the same retriever with multiple (related)\\nqueries, it‚Äôs likely at least some of them are repeated. Before using them as context to\\nanswer the question, we need to deduplicate them, to end up with a single instance of\\neach. Here we dedupe docs by using their content (a string) as the key in a dictionary\\n(or object in JS), because a dictionary can only contain one entry for each key. After\\n72 | Chapter 3: RAG Part II: Chatting with Your Data\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 100, 'page_label': '73'}, page_content='we‚Äôve iterated through all docs, we simply get all the dictionary values, which is now\\nfree of duplicates.\\nNotice our use as well of .batch, which runs all generated queries in parallel and\\nreturns a list of the results‚Äîin this case, a list of lists of documents, which we then\\nflatten and dedupe as described earlier.\\nThis final step is to construct a prompt, including the user‚Äôs question and combined\\nretrieved relevant documents, and a model interface to generate the prediction:\\nPython\\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based \\n    on this context:\\n{context}\\nQuestion: {question}\\n\"\"\")\\n@chain\\ndef multi_query_qa(input):\\n    # fetch relevant documents \\n    docs = retrieval_chain.invoke(input)\\n    # format prompt\\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\\n    # generate answer\\n    answer = llm.invoke(formatted)\\n    return answer\\n# run\\nmulti_query_qa.invoke(\"\"\"Who are some key figures in the ancient greek history \\n    of philosophy?\"\"\")\\nJavaScript\\nconst prompt = ChatPromptTemplate.fromTemplate(`Answer the following \\n  question based on this context:\\n{context}\\nQuestion: {question}\\n`)\\nconst multiQueryQa = RunnableLambda.from(async input => {\\n  // fetch relevant documents\\n  const docs = await retrievalChain.invoke(input)\\n  // format prompt\\n  const formatted = await prompt.invoke({context: docs, question: input})\\n  // generate answer\\n  const answer = await llm.invoke(formatted)\\n  return answer\\n})\\nQuery Transformation | 73'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 101, 'page_label': '74'}, page_content='3 Zackary Rackauckas, ‚ÄúRAG-Fusion: A New Take on Retrieval-Augmented Generation‚Äù, arXiv, February 21,\\n2024. From the International Journal on Natural Language Computing, vol. 13, no. 1 (February 2024).\\nawait multiQueryQa.invoke(`Who are some key figures in the ancient greek \\n  history of philosophy?`)\\nNotice how this isn‚Äôt that different from our previous QA chains, as all the new logic\\nfor multi-query retrieval is contained in retrieval_chain. This is key to making\\ngood use of these techniques‚Äîimplementing each technique as a standalone chain\\n(in this case, retrieval_chain), which makes it easy to adopt them and even to\\ncombine them.\\nRAG-Fusion\\nThe RAG-Fusion strategy shares similarities with the multi-query retrieval strategy,\\nexcept we will apply a final reranking step to all the retrieved documents. 3 This\\nreranking step makes use of the reciprocal rank fusion  (RRF) algorithm, which\\ninvolves combining the ranks of different search results to produce a single, unified\\nranking. By combining ranks from different queries, we pull the most relevant docu‚Äê\\nments to the top of the final list. RRF is well-suited for combining results from\\nqueries that might have different scales or distributions of scores.\\nLet‚Äôs demonstrate RAG-Fusion in code. First, we craft a prompt similar to the multi-\\nquery retrieval strategy to generate a list of queries based on the user query:\\nPython\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nprompt_rag_fusion = ChatPromptTemplate.from_template(\"\"\"You are a helpful \\n    assistant that generates multiple search queries based on a single input \\n    query. \\\\n\\n    Generate multiple search queries related to: {question} \\\\n\\n    Output (4 queries):\"\"\")\\ndef parse_queries_output(message):\\n    return message.content.split(\\'\\\\n\\')\\nllm = ChatOpenAI(temperature=0)\\nquery_gen = prompt_rag_fusion | llm | parse_queries_output\\nJavaScript\\nimport {ChatPromptTemplate} from \\'@langchain/core/prompts\\';\\nimport {ChatOpenAI} from \\'@langchain/openai\\';\\nimport {RunnableLambda} from \\'@langchain/core/runnables\\';\\n74 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 102, 'page_label': '75'}, page_content='const perspectivesPrompt = ChatPromptTemplate.fromTemplate(`You are a helpful \\n  assistant that generates multiple search queries based on a single input \\n  query. \\\\n\\n  Generate multiple search queries related to: {question} \\\\n\\n  Output (4 queries):`)\\nconst queryGen = perspectivesPrompt.pipe(llm).pipe(message => {\\n  return message.content.split(\\'\\\\n\\')\\n})\\nOnce we‚Äôve generated our queries, we fetch relevant documents for each query and\\npass them into a function to rerank (that is, reorder according to relevancy) the final\\nlist of relevant documents.\\nThe function reciprocal_rank_fusion takes a list of the search results of each query,\\nso a list of lists of documents, where each inner list of documents is sorted by their\\nrelevance to that query. The RRF algorithm then calculates a new score for each\\ndocument based on its ranks (or positions) in the different lists and sorts them to\\ncreate a final reranked list.\\nAfter calculating the fused scores, the function sorts the documents in descending\\norder of these scores to get the final reranked list, which is then returned:\\nPython\\ndef reciprocal_rank_fusion(results: list[list], k=60):\\n    \"\"\"reciprocal rank fusion on multiple lists of ranked documents \\n       and an optional parameter k used in the RRF formula\\n    \"\"\"\\n    \\n    # Initialize a dictionary to hold fused scores for each document\\n    # Documents will be keyed by their contents to ensure uniqueness\\n    fused_scores = {}\\n    documents = {}\\n    # Iterate through each list of ranked documents\\n    for docs in results:\\n        # Iterate through each document in the list,\\n        # with its rank (position in the list)\\n        for rank, doc in enumerate(docs):\\n            # Use the document contents as the key for uniqueness\\n            doc_str = doc.page_content\\n            # If the document hasn\\'t been seen yet,\\n            # - initialize score to 0\\n            # - save it for later\\n            if doc_str not in fused_scores:\\n                fused_scores[doc_str] = 0\\n                documents[doc_str] = doc\\n            # Update the score of the document using the RRF formula:\\n            # 1 / (rank + k)\\n            fused_scores[doc_str] += 1 / (rank + k)\\nQuery Transformation | 75'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 103, 'page_label': '76'}, page_content=\"# Sort the documents based on their fused scores in descending order \\n    # to get the final reranked results\\n    reranked_doc_strs = sorted(\\n        fused_scores, key=lambda d: fused_scores[d], reverse=True\\n    )\\n    # retrieve the corresponding doc for each doc_str\\n    return [\\n        documents[doc_str]\\n        for doc_str in reranked_doc_strs\\n    ]\\nretrieval_chain = generate_queries | retriever.batch | reciprocal_rank_fusion\\nJavaScript\\nfunction reciprocalRankFusion(results, k = 60) {\\n  // Initialize a dictionary to hold fused scores for each document\\n  // Documents will be keyed by their contents to ensure uniqueness\\n  const fusedScores = {}\\n  const documents = {}\\n  results.forEach(docs => {\\n    docs.forEach((doc, rank) => {\\n      // Use the document contents as the key for uniqueness\\n      const key = doc.pageContent\\n      // If the document hasn't been seen yet,\\n      // - initialize score to 0\\n      // - save it for later\\n      if (!(key in fusedScores)) {\\n        fusedScores[key] = 0\\n        documents[key] = 0\\n      }\\n      // Update the score of the document using the RRF formula:\\n      // 1 / (rank + k)\\n      fusedScores[key] += 1 / (rank + k)\\n    })\\n  })\\n  // Sort the documents based on their fused scores in descending order \\n  // to get the final reranked results\\n  const sorted = Object.entries(fusedScores).sort((a, b) => b[1] - a[1])\\n  // retrieve the corresponding doc for each key\\n  return sorted.map(([key]) => documents[key])\\n}\\nconst retrievalChain = queryGen\\n  .pipe(retriever.batch.bind(retriever))\\n  .pipe(reciprocalRankFusion)\\n76 | Chapter 3: RAG Part II: Chatting with Your Data\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 104, 'page_label': '77'}, page_content='Notice that the function also takes a k parameter, which determines how much\\ninfluence documents in each query‚Äôs result sets have over the final list of documents.\\nA higher value indicates that lower-ranked documents have more influence.\\nFinally, we combine our new retrieval chain (now using RRF) with the full chain\\nwe‚Äôve seen before:\\nPython\\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based \\n    on this context:\\n{context}\\nQuestion: {question}\\n\"\"\")\\nllm = ChatOpenAI(temperature=0)\\n@chain\\ndef multi_query_qa(input):\\n    # fetch relevant documents \\n    docs = retrieval_chain.invoke(input)\\n    # format prompt\\n    formatted = prompt.invoke({\"context\": docs, \"question\": input})\\n    # generate answer\\n    answer = llm.invoke(formatted)\\n    return answer\\nmulti_query_qa.invoke(\"\"\"Who are some key figures in the ancient greek history \\n    of philosophy?\"\"\")\\nJavaScript\\nconst rewritePrompt = ChatPromptTemplate.fromTemplate(`Answer the following \\n  question based on this context:\\n{context}\\nQuestion: {question}\\n`)\\nconst llm = new ChatOpenAI({temperature: 0})\\nconst multiQueryQa = RunnableLambda.from(async input => {\\n  // fetch relevant documents\\n  const docs = await retrievalChain.invoke(input)\\n  // format prompt\\n  const formatted = await prompt.invoke({context: docs, question: input})\\n  // generate answer\\n  const answer = await llm.invoke(formatted)\\n  return answer\\n})\\nQuery Transformation | 77'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 105, 'page_label': '78'}, page_content='4 Luyu Gao et al., ‚ÄúPrecise Zero-Shot Dense Retrieval Without Relevance Labels‚Äù, arXiv, December 20, 2022.\\nawait multiQueryQa.invoke(`Who are some key figures in the ancient greek \\n  history of philosophy?`)\\nRAG-Fusion‚Äôs strength lies in its ability to capture the user‚Äôs intended expression,\\nnavigate complex queries, and broaden the scope of retrieved documents, enabling\\nserendipitous discovery.\\nHypothetical Document Embeddings\\nHypothetical Document Embeddings  (HyDE) is a strategy that involves creating a\\nhypothetical document based on the user‚Äôs query, embedding the document, and\\nretrieving relevant documents based on vector similarity. 4 The intuition behind\\nHyDE is that an LLM-generated hypothetical document will be more similar to the\\nmost relevant documents than the original query, as shown in Figure 3-7.\\nFigure 3-7. An illustration of HyDE closer in the vector space to the document\\nembeddings than the plain query embeddings\\nFirst, define a prompt to generate a hypothetical document:\\nPython\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_openai import ChatOpenAI\\nprompt_hyde = ChatPromptTemplate.from_template(\"\"\"Please write a passage to \\n78 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 106, 'page_label': '79'}, page_content='answer the question.\\\\n Question: {question} \\\\n Passage:\"\"\")\\ngenerate_doc = (\\n    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser() \\n)\\nJavaScript\\nimport {ChatOpenAI} from \\'@langchain/openai\\'\\nimport {ChatPromptTemplate} from \\'@langchain/core/prompts\\'\\nimport {RunnableLambda} from \\'@langchain/core/runnables\\';\\nconst prompt = ChatPromptTemplate.fromTemplate(`Please write a passage to \\n  answer the question\\nQuestion: {question}\\nPassage:`)\\nconst llm = new ChatOpenAI({temperature: 0})\\nconst generateDoc = prompt.pipe(llm).pipe(msg => msg.content)\\nNext, we take the hypothetical document and use it as input to the retriever, which\\nwill generate its embedding and search for similar documents in the vector store:\\nPython\\nretrieval_chain = generate_doc | retriever \\nJavaScript\\nconst retrievalChain = generateDoc.pipe(retriever)\\nFinally, we take the retrieved documents, pass them as context to the final prompt,\\nand instruct the model to generate an output:\\nPython\\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based \\n    on this context:\\n{context}\\nQuestion: {question}\\n\"\"\")\\nllm = ChatOpenAI(temperature=0)\\n@chain\\ndef qa(input):\\n  # fetch relevant documents from the hyde retrieval chain defined earlier\\n  docs = retrieval_chain.invoke(input)\\n  # format prompt\\n  formatted = prompt.invoke({\"context\": docs, \"question\": input})\\n  # generate answer\\n  answer = llm.invoke(formatted)\\nQuery Transformation | 79'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 107, 'page_label': '80'}, page_content='return answer\\nqa.invoke(\"\"\"Who are some key figures in the ancient greek history of \\n    philosophy?\"\"\")\\nJavaScript\\nconst prompt = ChatPromptTemplate.fromTemplate(`Answer the following \\n  question based on this context:\\n{context}\\nQuestion: {question}\\n`)\\nconst llm = new ChatOpenAI({temperature: 0})\\nconst qa = RunnableLambda.from(async input => {\\n  // fetch relevant documents from the hyde retrieval chain defined earlier\\n  const docs = await retrievalChain.invoke(input)\\n  // format prompt\\n  const formatted = await prompt.invoke({context: docs, question: input})\\n  // generate answer\\n  const answer = await llm.invoke(formatted)\\n  return answer\\n})\\nawait qa.invoke(`Who are some key figures in the ancient greek history of \\n  philosophy?`)\\nTo recap what we covered in this section, query transformation consists of taking the\\nuser‚Äôs original query and doing the following:\\n‚Ä¢ Rewriting into one or more queries‚Ä¢\\n‚Ä¢ Combining the results of those queries into a single set of the most relevant‚Ä¢\\nresults\\nRewriting the query can take many forms, but it‚Äôs usually done in a similar fashion:\\ntake the user‚Äôs original query‚Äîa prompt you wrote‚Äîand ask an LLM to write a new\\nquery or queries. Some examples of typical changes made are:\\n‚Ä¢ Removing irrelevant/unrelated text from the query.‚Ä¢\\n‚Ä¢ Grounding the query with past conversation history. For instance, to make‚Ä¢\\nsense of a query such as and what about in LA,  we need to combine it with a\\nhypothetical past question about the weather in SF , to arrive at a useful query\\nsuch as weather in LA.\\n‚Ä¢ Casting a wider net for relevant documents by also fetching documents for‚Ä¢\\nrelated queries.\\n80 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 108, 'page_label': '81'}, page_content='‚Ä¢ Decomposing a complex question into multiple, simpler questions and then‚Ä¢\\nincluding results for all of them in the final prompt to generate an answer.\\nThe right rewriting strategy to use will depend on your use case.\\nNow that we‚Äôve covered the main query transformation strategies, let‚Äôs discuss the\\nsecond major question to answer in order to build a robust RAG system: How do we\\nroute queries to retrieve relevant data from multiple data sources?\\nQuery Routing\\nAlthough using a single vector store is useful, the required data may live in a variety\\nof data sources, including relational databases or other vector stores.\\nFor example, you may have two vector stores: one for LangChain Python documenta‚Äê\\ntion and another for LangChain JS documentation. Given a user‚Äôs question, we would\\nlike to route the query to the appropriate inferred data source to retrieve relevant docs.\\nQuery routing is a strategy used to forward a user‚Äôs query to the relevant data source.\\nLogical Routing\\nIn logical routing , we give the LLM knowledge of the various data sources at our\\ndisposal and then let the LLM reason which data source to apply based on the user‚Äôs\\nquery, as shown in Figure 3-8.\\nFigure 3-8. Query routing to relevant data sources\\nIn order to achieve this, we make use of function-calling models like GPT-3.5 Turbo\\nto help classify each query into one of the available routes. A function call involves\\ndefining a schema that the model can use to generate arguments of a function based\\non the query. This enables us to generate structured outputs that can be used to run\\nQuery Routing | 81'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 109, 'page_label': '82'}, page_content='other functions. The following Python code defines the schema for our router based\\non three docs for different languages:\\nPython\\nfrom typing import Literal\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_openai import ChatOpenAI\\n# Data model\\nclass RouteQuery(BaseModel):\\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\\n    datasource: Literal[\"python_docs\", \"js_docs\"] = Field(\\n        ...,\\n        description=\"\"\"Given a user question, choose which datasource would be \\n            most relevant for answering their question\"\"\",\\n    )\\n# LLM with function call \\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\nstructured_llm = llm.with_structured_output(RouteQuery)\\n# Prompt \\nsystem = \"\"\"You are an expert at routing a user question to the appropriate data \\n    source.\\nBased on the programming language the question is referring to, route it to the \\n    relevant data source.\"\"\"\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"{question}\"),\\n    ]\\n)\\n# Define router \\nrouter = prompt | structured_llm\\nJavaScript\\nimport { ChatOpenAI } from \"@langchain/openai\";\\nimport { z } from \"zod\";\\nconst routeQuery = z.object({\\n  datasource: z.enum([\"python_docs\", \"js_docs\"]).describe(`Given a user \\n    question, choose which datasource would be most relevant for answering \\n    their question`),\\n}).describe(\"Route a user query to the most relevant datasource.\")\\n82 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 110, 'page_label': '83'}, page_content='const llm = new ChatOpenAI({model: \"gpt-3.5-turbo\", temperature: 0})\\nconst structuredLlm = llm.withStructuredOutput(routeQuery, {name: \"RouteQuery\"})\\nconst prompt = ChatPromptTemplate.fromMessages([\\n  [\\'system\\', `You are an expert at routing a user question to the appropriate \\n      data source.\\nBased on the programming language the question is referring to, route it to \\n  the relevant data source.`],\\n  [\\'human\\', \\'{question}\\']\\n])\\nconst router = prompt.pipe(structuredLlm)\\nNow we invoke the LLM to extract the data source based on the predefined schema:\\nPython\\nquestion = \"\"\"Why doesn\\'t the following code work:\\nfrom langchain_core.prompts import ChatPromptTemplate\\nprompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\\nprompt.invoke(\"french\")\\n\"\"\"\\nresult = router.invoke({\"question\": question})\\nresult.datasource\\n# \"python_docs\"\\nJavaScript\\nconst question = `Why doesn\\'t the following code work:\\nfrom langchain_core.prompts import ChatPromptTemplate\\nprompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\\nprompt.invoke(\"french\")\\n`\\nawait router.invoke({ question })\\nThe output:\\n{\\n    datasource: \"python_docs\"\\n}\\nNotice how the LLM produced JSON output, conforming to the schema we defined\\nearlier. This will be useful in many other tasks.\\nQuery Routing | 83'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 111, 'page_label': '84'}, page_content='Once we‚Äôve extracted the relevant data source, we can pass the value into another\\nfunction to execute additional logic as required:\\nPython\\ndef choose_route(result):\\n    if \"python_docs\" in result.datasource.lower():\\n        ### Logic here \\n        return \"chain for python_docs\"\\n    else:\\n        ### Logic here \\n        return \"chain for js_docs\"\\nfull_chain = router | RunnableLambda(choose_route)\\nJavaScript\\nfunction chooseRoute(result) {\\n  if (result.datasource.toLowerCase().includes(\\'python_docs\\')) {\\n    return \\'chain for python_docs\\';\\n  } else {\\n    return \\'chain for js_docs\\';\\n  }\\n} \\nconst fullChain = router.pipe(chooseRoute) \\nNotice how we don‚Äôt do an exact string comparison but instead first turn the gener‚Äê\\nated output to lowercase, and then do a substring match. This makes our chain more\\nresilient to the LLM going off script and producing output that doesn‚Äôt quite conform\\nto the schema we asked for.\\nResilience to the random nature of LLM outputs is an important\\ntheme to keep in mind when building your LLM applications.\\nLogical routing is most suitable when you have a defined list of data sources from\\nwhich relevant data can be retrieved and utilized by the LLM to generate an accurate\\noutput. These can range from vector stores to databases and even APIs.\\nSemantic Routing\\nUnlike logical routing, semantic routing involves embedding various prompts that\\nrepresent various data sources alongside the user‚Äôs query and then performing vector\\nsimilarity search to retrieve the most similar prompt. Figure 3-9 illustrates.\\n84 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 112, 'page_label': '85'}, page_content='Figure 3-9. Semantic routing to improve the accuracy of retrieved documents\\nThe following is an example of semantic routing:\\nPython\\nfrom langchain.utils.math import cosine_similarity\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.runnables import chain\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\n# Two prompts\\nphysics_template = \"\"\"You are a very smart physics professor. You are great at \\n    answering questions about physics in a concise and easy-to-understand manner. \\n    When you don\\'t know the answer to a question, you admit that you don\\'t know.\\nHere is a question:\\n{query}\"\"\"\\nmath_template = \"\"\"You are a very good mathematician. You are great at answering \\n    math questions. You are so good because you are able to break down hard \\n    problems into their component parts, answer the component parts, and then \\n    put them together to answer the broader question.\\nHere is a question:\\n{query}\"\"\"\\n# Embed prompts\\nembeddings = OpenAIEmbeddings()\\nprompt_templates = [physics_template, math_template]\\nprompt_embeddings = embeddings.embed_documents(prompt_templates)\\n# Route question to prompt\\n@chain\\ndef prompt_router(query):\\n    # Embed question\\n    query_embedding = embeddings.embed_query(query)\\n    # Compute similarity\\nQuery Routing | 85'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 113, 'page_label': '86'}, page_content='similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\\n    # Pick the prompt most similar to the input question\\n    most_similar = prompt_templates[similarity.argmax()]\\n    return PromptTemplate.from_template(most_similar)\\nsemantic_router = (\\n    prompt_router\\n    | ChatOpenAI()\\n    | StrOutputParser()\\n)\\nprint(semantic_router.invoke(\"What\\'s a black hole\"))\\nJavaScript\\nimport {cosineSimilarity} from \\'@langchain/core/utils/math\\'\\nimport {ChatOpenAI, OpenAIEmbeddings} from \\'@langchain/openai\\'\\nimport {PromptTemplate} from \\'@langchain/core/prompts\\'\\nimport {RunnableLambda} from \\'@langchain/core/runnables\\';\\nconst physicsTemplate = `You are a very smart physics professor. You are great \\n  at answering questions about physics in a concise and easy-to-understand \\n  manner. When you don\\'t know the answer to a question, you admit that you \\n  don\\'t know.\\nHere is a question:\\n{query}`\\nconst mathTemplate = `You are a very good mathematician. You are great at \\n  answering math questions. You are so good because you are able to break down \\n  hard problems into their component parts, answer the component parts, and \\n  then put them together to answer the broader question.\\nHere is a question:\\n{query}`\\nconst embeddings = new OpenAIEmbeddings()\\nconst promptTemplates = [physicsTemplate, mathTemplate]\\nconst promptEmbeddings = await embeddings.embedDocuments(promptTemplates)\\nconst promptRouter = RunnableLambda.from(query => {\\n  // Embed question\\n  const queryEmbedding = await embeddings.embedQuery(query)\\n  // Compute similarity\\n  const similarities = cosineSimilarity([queryEmbedding], promptEmbeddings)[0]\\n  // Pick the prompt most similar to the input question\\n  const mostSimilar = similarities[0] > similarities[1] \\n    ? promptTemplates[0] \\n    : promptTemplates[1]\\n  return PromptTemplate.fromTemplate(mostSimilar)\\n})\\n86 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 114, 'page_label': '87'}, page_content='const semanticRouter = promptRouter.pipe(new ChatOpenAI())\\nawait semanticRouter.invoke(\"What\\'s a black hole\")\\nNow that you‚Äôve seen how to route a user‚Äôs query to the relevant data source, let‚Äôs\\ndiscuss the third major question when building a robust RAG system: ‚ÄúHow do we\\ntransform natural language to the query language of the target data source?‚Äù\\nQuery Construction\\nAs discussed earlier, RAG is an effective strategy to embed and retrieve relevant\\nunstructured data from a vector store based on a query. But most data available\\nfor use in production apps is structured and typically stored in relational databases.\\nIn addition, unstructured data embedded in a vector store also contains structured\\nmetadata that possesses important information.\\nQuery construction  is the process of transforming a natural language query into\\nthe query language of the database or data source you are interacting with. See\\nFigure 3-10.\\nFigure 3-10. Illustration of query languages for various data sources\\nFor example, consider the query: What are movies about aliens in the year 1980?\\nThis question contains an unstructured topic that can be retrieved via embeddings\\n(aliens), but it also contains potential structured components (year == 1980).\\nThe following sections dive deeper into the various forms of query construction.\\nText-to-Metadata Filter\\nMost vector stores provide the ability to limit your vector search based on metadata.\\nDuring the embedding process, we can attach metadata key-value pairs to vectors in\\nan index and then later specify filter expressions when you query the index.\\nQuery Construction | 87'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 115, 'page_label': '88'}, page_content='LangChain provides a SelfQueryRetriever that abstracts this logic and makes it\\neasier to translate natural language queries into structured queries for various data\\nsources. The self-querying utilizes an LLM to extract and execute the relevant meta‚Äê\\ndata filters based on a user‚Äôs query and predefined metadata schema:\\nPython\\nfrom langchain.chains.query_constructor.base import AttributeInfo\\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\\nfrom langchain_openai import ChatOpenAI\\nfields = [\\n    AttributeInfo(\\n        name=\"genre\",\\n        description=\"The genre of the movie\",\\n        type=\"string or list[string]\",\\n    ),\\n    AttributeInfo(\\n        name=\"year\",\\n        description=\"The year the movie was released\",\\n        type=\"integer\",\\n    ),\\n    AttributeInfo(\\n        name=\"director\",\\n        description=\"The name of the movie director\",\\n        type=\"string\",\\n    ),\\n    AttributeInfo(\\n        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\\n    ),\\n]\\ndescription = \"Brief summary of a movie\"\\nllm = ChatOpenAI(temperature=0)\\nretriever = SelfQueryRetriever.from_llm(\\n    llm, db, description, fields,\\n)\\nprint(retriever.invoke(\\n    \"What\\'s a highly rated (above 8.5) science fiction film?\"))\\nJavaScript\\nimport { ChatOpenAI } from \"@langchain/openai\";\\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\\nimport { FunctionalTranslator } from \"@langchain/core/structured_query\";\\n/**\\n * First, we define the attributes we want to be able to query on.\\n * in this case, we want to be able to query on the genre, year, director, \\n * rating, and length of the movie.\\n * We also provide a description of each attribute and the type of the attribute.\\n88 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 116, 'page_label': '89'}, page_content='* This is used to generate the query prompts.\\n */\\nconst fields = [\\n  {\\n    name: \"genre\",\\n    description: \"The genre of the movie\",\\n    type: \"string or array of strings\",\\n  },\\n  {\\n    name: \"year\",\\n    description: \"The year the movie was released\",\\n    type: \"number\",\\n  },\\n  {\\n    name: \"director\",\\n    description: \"The director of the movie\",\\n    type: \"string\",\\n  },\\n  {\\n    name: \"rating\",\\n    description: \"The rating of the movie (1-10)\",\\n    type: \"number\",\\n  },\\n  {\\n    name: \"length\",\\n    description: \"The length of the movie in minutes\",\\n    type: \"number\",\\n  },\\n];\\nconst description = \"Brief summary of a movie\";\\nconst llm = new ChatOpenAI();\\nconst attributeInfos = fields.map((field) => new AttributeInfo(field.name,  \\n  field.description, field.type));\\n  \\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\\n  llm,\\n  db,\\n  description,\\n  attributeInfo: attributeInfos,\\n  /**\\n   * We need to use a translator that translates the queries into a\\n   * filter format that the vector store can understand. LangChain provides one \\n   * here.\\n   */\\n  structuredQueryTranslator: new FunctionalTranslator(),\\n});\\nawait selfQueryRetriever.invoke(\\n  \"What\\'s a highly rated (above 8.5) science fiction film?\"\\n);\\nQuery Construction | 89'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 117, 'page_label': '90'}, page_content='5 Nitarshan Rajkumar et al., ‚ÄúEvaluating the Text-to-SQL Capabilities of Large Language Models‚Äù, arXiv, March\\n15, 2022.\\nThis results in a retriever that will take a user query, and split it into:\\n‚Ä¢ A filter to apply on the metadata of each document first‚Ä¢\\n‚Ä¢ A query to use for semantic search on the documents‚Ä¢\\nTo do this, we have to describe which fields the metadata of our documents contain;\\nthat description will be included in the prompt. The retriever will then do the\\nfollowing:\\n1. Send the query generation prompt to the LLM.1.\\n2. Parse metadata filter and rewritten search query from the LLM output.2.\\n3. Convert the metadata filter generated by the LLM to the format appropriate for3.\\nour vector store.\\n4. Issue a similarity search against the vector store, filtered to only match docu‚Äê4.\\nments whose metadata passes the generated filter.\\nText-to-SQL\\nSQL and relational databases are important sources of structured data, but they don‚Äôt\\ninteract directly with natural language. Although we can simply use the LLM to\\ntranslate a user‚Äôs query to SQL queries, there is little margin for error.\\nHere are some useful strategies for effective text to SQL translations:\\nDatabase description\\nTo ground SQL queries, an LLM must be provided with an accurate description\\nof the database. One common text-to-SQL prompt employs an idea reported in\\nthis paper and others: provide the LLM with a CREATE TABLE description for\\neach table, including column names and types. 5 We can also provide a few (for\\ninstance, three) example rows from the table.\\nFew-shot examples\\nFeeding the prompt with few-shot examples of question-query matches can\\nimprove the query generation accuracy. This can be achieved by simply append‚Äê\\ning standard static examples in the prompt to guide the agent on how it should\\nbuild queries based on questions.\\nSee Figure 3-11 for a visual of the process.\\n90 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 118, 'page_label': '91'}, page_content='Figure 3-11. A user‚Äôs query transformed to a SQL query\\nHere‚Äôs a full code example:\\nPython\\nfrom langchain_community.tools import QuerySQLDatabaseTool\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain.chains import create_sql_query_chain\\nfrom langchain_openai import ChatOpenAI\\n# replace this with the connection details of your db\\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0)\\n# convert question to sql query\\nwrite_query = create_sql_query_chain(llm, db)\\n# Execute SQL query\\nexecute_query = QuerySQLDatabaseTool(db=db)\\n# combined\\nchain = write_query | execute_query\\n# invoke the chain\\nchain.invoke(\\'How many employees are there?\\');\\nJavaScript\\nimport { ChatOpenAI } from \"@langchain/openai\";\\nimport { createSqlQueryChain } from \"langchain/chains/sql_db\";\\nimport { SqlDatabase } from \"langchain/sql_db\";\\nimport { DataSource } from \"typeorm\";\\nimport { QuerySqlTool } from \"langchain/tools/sql\";\\nconst datasource = new DataSource({\\n  type: \"sqlite\",\\n  database: \"./Chinook.db\", // here should be the details of your database\\n});\\nQuery Construction | 91'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 119, 'page_label': '92'}, page_content='const db = await SqlDatabase.fromDataSourceParams({\\n  appDataSource: datasource,\\n});\\nconst llm = new ChatOpenAI({ model: \"gpt-4\", temperature: 0 });\\n// convert question to sql query\\nconst writeQuery = await createSqlQueryChain({ llm, db, dialect: \"sqlite\" });\\n// execute query\\nconst executeQuery = new QuerySqlTool(db);\\n// combined\\nconst chain = writeQuery.pipe(executeQuery);\\n// invoke the chain\\nawait chain.invoke(\\'How many employees are there?\\');\\nWe first convert the user‚Äôs query to a SQL query appropriate to the dialect of our\\ndatabase. Then we execute that query on our database. Note that executing arbitrary\\nSQL queries on your database generated by an LLM from user input is dangerous in\\na production application. To use these ideas in production, you need to consider a\\nnumber of security measures to reduce the risk of unintended queries being run in\\nyour database. Here are some examples:\\n‚Ä¢ Run the queries on your database with a user with read-only permissions.‚Ä¢\\n‚Ä¢ The database user running the queries should have access only to the tables you‚Ä¢\\nwish to make available for querying.\\n‚Ä¢ Add a time-out to the queries run by this application; this would ensure that even‚Ä¢\\nif an expensive query is generated, it is canceled before taking up too many of\\nyour database resources.\\nThis is not an exhaustive list of security considerations. Security of LLM applications\\nis an area currently in development, with more security measures being added to the\\nrecommendations as new vulnerabilities are discovered.\\nSummary\\nThis chapter discussed various state-of-the-art strategies to efficiently retrieve the\\nmost relevant documents based on a user‚Äôs query and synthesize them with your\\nprompt to aid the LLM to generate an accurate, up-to-date output.\\nAs discussed, a robust, production-ready RAG system requires a wide range of effec‚Äê\\ntive strategies that can execute query transformation, query construction, routing,\\nand indexing optimization.\\n92 | Chapter 3: RAG Part II: Chatting with Your Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 120, 'page_label': '93'}, page_content='Query transformation enables your AI app to transform an ambiguous or malformed\\nuser query into a representative query that‚Äôs optimal for retrieval. Query construction\\nenables your AI app to convert the user‚Äôs query into the syntax of the query language\\nof the database or data source where structured data lives. Routing enables your AI\\napp to dynamically route the user‚Äôs query to retrieve relevant information from the\\nrelevant data source.\\nIn Chapter 4, we‚Äôll build on this knowledge to add memory to your AI chatbot so that\\nit can remember and learn from each interaction. This will enable users to ‚Äúchat‚Äù with\\nthe application in multiturn conversations like ChatGPT.\\nSummary | 93'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 122, 'page_label': '95'}, page_content='CHAPTER 4\\nUsing LangGraph to Add Memory\\nto Your Chatbot\\nIn Chapter 3, you learned how to provide your AI chatbot application with up-to-\\ndate and relevant context. This enables your chatbot to generate accurate responses\\nbased on the user‚Äôs input. But that‚Äôs not enough to build a production-ready applica‚Äê\\ntion. How can you enable your application to actually ‚Äúchat‚Äù back and forth with the\\nuser, while remembering prior conversations and relevant context?\\nLarge language models are stateless, which means that each time the model is promp‚Äê\\nted to generate a new response it has no memory of the prior prompt or model\\nresponse. In order to provide this historical information to the model, we need a\\nrobust memory system that will keep track of previous conversations and context.\\nThis historical information can then be included in the final prompt sent to the LLM,\\nthus giving it ‚Äúmemory. ‚Äù Figure 4-1 illustrates this.\\nFigure 4-1. Memory and retrieval used to generate context-aware answers from an LLM\\nIn this chapter, you‚Äôll learn how to build this essential memory system using Lang‚Äê\\nChain‚Äôs built-in modules to make this development process easier.\\n95'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 123, 'page_label': '96'}, page_content='Building a Chatbot Memory System\\nThere are two core design decisions behind any robust memory system:\\n‚Ä¢ How state is stored‚Ä¢\\n‚Ä¢ How state is queried‚Ä¢\\nA simple way to build a chatbot memory system that incorporates effective solutions\\nto these design decisions is to store and reuse the history of all chat interactions\\nbetween the user and the model. The state of this memory system can be:\\n‚Ä¢ Stored as a list of messages (refer to Chapter 1 to learn more about messages)‚Ä¢\\n‚Ä¢ Updated by appending recent messages after each turn‚Ä¢\\n‚Ä¢ Appended into the prompt by inserting the messages into the prompt‚Ä¢\\nFigure 4-2 illustrates this simple memory system.\\nFigure 4-2. A simple memory system utilizing chat history in prompts to generate model\\nanswers\\nHere‚Äôs a code example that illustrates a simple version of this memory system using\\nLangChain:\\nPython\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"\"\"You are a helpful assistant. Answer all questions to the best \\n        of your ability.\"\"\"),\\n    (\"placeholder\", \"{messages}\"),\\n])\\nmodel = ChatOpenAI()\\n96 | Chapter 4: Using LangGraph to Add Memory to Your Chatbot'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 124, 'page_label': '97'}, page_content='chain = prompt | model\\nchain.invoke({\\n    \"messages\": [\\n        (\"human\",\"\"\"Translate this sentence from English to French: I love \\n            programming.\"\"\"),\\n        (\"ai\", \"J\\'adore programmer.\"),\\n        (\"human\", \"What did you just say?\"),\\n    ],\\n})\\nJavaScript\\nimport {ChatPromptTemplate} from \\'@langchain/core/prompts\\'\\nimport {ChatOpenAI} from \\'@langchain/openai\\'\\nconst prompt = ChatPromptTemplate.fromMessages([\\n  [\"system\", `You are a helpful assistant. Answer all questions to the best \\n    of your ability.`],\\n  [\"placeholder\", \"{messages}\"],\\n])\\nconst model = new ChatOpenAI()\\nconst chain = prompt.pipe(model)\\nawait chain.invoke({\\n  \"messages\": [\\n    [\"human\",`Translate this sentence from English to French: I love \\n      programming.`],\\n    [\"ai\", \"J\\'adore programmer.\"],\\n    [\"human\", \"What did you just say?\"],\\n  ],\\n})\\nThe output:\\nI said, \"J\\'adore programmer,\" which means \"I love programming\" in French.\\nNote how the incorporation of the previous conversation in the chain enabled the\\nmodel to answer the follow-up question in a context-aware manner.\\nWhile this is simple and it works, when taking your application to production, you‚Äôll\\nface some more challenges related to managing memory at scale, such as:\\n‚Ä¢ Y ou‚Äôll need to update the memory after every interaction, atomically (i.e., don‚Äôt‚Ä¢\\nrecord only the question or only the answer in the case of failure).\\n‚Ä¢ Y ou‚Äôll want to store these memories in durable storage, such as a relational‚Ä¢\\ndatabase.\\nBuilding a Chatbot Memory System | 97'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 125, 'page_label': '98'}, page_content='‚Ä¢ Y ou‚Äôll want to control how many and which messages are stored for later, and‚Ä¢\\nhow many of these are used for new interactions.\\n‚Ä¢ Y ou‚Äôll want to inspect and modify this state (for now, just a list of messages)‚Ä¢\\noutside a call to an LLM.\\nWe‚Äôll now introduce some better tooling, which will help with this and all later\\nchapters.\\nIntroducing LangGraph\\nFor the remainder of this chapter and the following chapters, we‚Äôll start to make\\nuse of LangGraph, an open source library authored by LangChain. LangGraph was\\ndesigned to enable developers to implement multiactor, multistep, stateful cognitive\\narchitectures, called graphs. That‚Äôs a lot of words packed into a short sentence; let‚Äôs\\ntake them one at a time. Figure 4-3 illustrates the multiactor aspect.\\nFigure 4-3. From single-actor applications to multiactor applications\\nA team of specialists can build something together that none of them could build\\nalone. The same is true of LLM applications: an LLM prompt (great for answer gener‚Äê\\nation and task planning and many more things) is much more powerful when paired\\nup with a search engine (best at finding current facts), or even when paired with\\ndifferent LLM prompts. We have seen developers build some amazing applications,\\nlike Perplexity or Arc Search , when they combine those two building blocks (and\\nothers) in novel ways.\\n98 | Chapter 4: Using LangGraph to Add Memory to Your Chatbot'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 126, 'page_label': '99'}, page_content='And just as a human team needs more coordination than one person working by\\nthemselves, an application with multiple actors needs a coordination layer to do these\\nthings:\\n‚Ä¢ Define the actors involved (the nodes in a graph) and how they hand off work to‚Ä¢\\neach other (the edges in that graph).\\n‚Ä¢ Schedule execution of each actor at the appropriate time‚Äîin parallel if needed‚Äî‚Ä¢\\nwith deterministic results.\\nFigure 4-4 illustrates the multistep dimension.\\nFigure 4-4. From multiactor to multistep applications\\nAs each actor hands off work to another (for example, an LLM prompt asking a\\nsearch tool for the results of a given search query), we need to make sense of the\\nback-and-forth between multiple actors. We need to know what order it happens\\nin, how many times each actor is called, and so on. To do this, we can model the\\ninteraction between the actors as happening across multiple discrete steps in time.\\nWhen one actor hands off work to another actor, it results in the scheduling of the\\nnext step of the computation, and so on, until no more actors hand off work to\\nothers, and the final result is reached.\\nIntroducing LangGraph | 99'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 127, 'page_label': '100'}, page_content='Figure 4-5 illustrates the stateful aspect.\\nFigure 4-5. From multistep to stateful applications\\nCommunication across steps requires tracking some state‚Äîotherwise, when you call\\nthe LLM actor the second time, you‚Äô d get the same result as the first time. It is very\\nhelpful to pull this state out of each of the actors and have all actors collaborate on\\nupdating a single central state. With a single central state, we can:\\n‚Ä¢ Snapshot and store the central state during or after each computation.‚Ä¢\\n‚Ä¢ Pause and resume execution, which makes it easy to recover from errors.‚Ä¢\\n‚Ä¢ Implement human-in-the-loop controls (more on this in Chapter 8).‚Ä¢\\nEach graph is then made up of the following:\\nState\\nThe data received from outside the application, modified and produced by the\\napplication while it‚Äôs running.\\nNodes\\nEach step to be taken. Nodes are simply Python/JS functions, which receive the\\ncurrent state as input and can return an update to that state (that is, they can add\\nto it and modify or remove existing data).\\n100 | Chapter 4: Using LangGraph to Add Memory to Your Chatbot'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 128, 'page_label': '101'}, page_content='Edges\\nThe connections between nodes. Edges determine the path taken from the first\\nnode to the last, and they can be fixed (that is, after Node B, always visit node D)\\nor conditional (evaluate a function to decide the next node to visit after node C).\\nLangGraph offers utilities to visualize these graphs and numerous features to debug\\ntheir workings while in development. These graphs can then easily be deployed to\\nserve production workloads at high scale.\\nIf you followed the instructions in Chapter 1 , you‚Äôll already have LangGraph\\ninstalled. If not, you can install it by running one of the following commands in\\nyour terminal:\\nPython\\npip install langgraph\\nJavaScript\\nnpm i @langchain/langgraph\\nTo help get you familiar with using LangGraph, we‚Äôll create a simple chatbot using\\nLangGraph, which is a great example of the LLM call architecture with a single use of\\nan LLM. This chatbot will respond directly to user messages. Though simple, it does\\nillustrate the core concepts of building with LangGraph.\\nCreating a StateGraph\\nStart by creating a StateGraph. We‚Äôll add a node to represent the LLM call:\\nPython\\nfrom typing import Annotated, TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nclass State(TypedDict):\\n    # Messages have the type \"list\". The `add_messages` \\n    # function in the annotation defines how this state should \\n    # be updated (in this case, it appends new messages to the \\n    # list, rather than replacing the previous messages)\\nmessages: Annotated[list, add_messages]\\nbuilder = StateGraph(State)\\nCreating a StateGraph | 101'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 129, 'page_label': '102'}, page_content='JavaScript\\nimport {\\n  StateGraph,\\n  StateType,\\n  Annotation,\\n  messagesStateReducer,\\n  START, END\\n} from \\'@langchain/langgraph\\'\\nconst State = {\\n  /**\\n  * The State defines three things:\\n  * 1. The structure of the graph\\'s state (which \"channels\" are available to\\n  * read/write)\\n  * 2. The default values for the state\\'s channels\\n  * 3. The reducers for the state\\'s channels. Reducers are functions that\\n  * determine how to apply updates to the state. Below, new messages are\\n  * appended to the messages array.\\n  */\\n  messages: Annotation({\\n    reducer: messagesStateReducer,\\n    default: () => []\\n  }),\\n}\\nconst builder = new StateGraph(State)\\nThe first thing you do when you define a graph is define the state\\nof the graph. The state consists of the shape, or schema, of the\\ngraph state, as well as reducer functions that specify how to apply\\nupdates to the state. In this example, the state is a dictionary with\\na single key: messages. The messages key is annotated with the\\nadd_messages reducer function, which tells LangGraph to append\\nnew messages to the existing list, rather than overwrite it. State\\nkeys without an annotation will be overwritten by each update,\\nstoring the most recent value. Y ou can write your own reducer\\nfunctions, which are simply functions that receive as arguments‚Äî\\nargument 1 is the current state, and argument 2 is the next value\\nbeing written to the state‚Äîand should return the next state, that\\nis, the result of merging the current state with the new value. The\\nsimplest example is a function that appends the next value to a list\\nand returns that list.\\n102 | Chapter 4: Using LangGraph to Add Memory to Your Chatbot'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 130, 'page_label': '103'}, page_content='So now our graph knows two things:\\n‚Ä¢ Every node we define will receive the current State as input and return a value‚Ä¢\\nthat updates that state.\\n‚Ä¢ messages will be appended to the current list, rather than directly overwritten.‚Ä¢\\nThis is communicated via the prebuilt add_messages function in the Annotated\\nsyntax in the Python example or the reducer function for the JavaScript example.\\nNext, add the chatbot node. Nodes represent units of work. They are typically just\\nfunctions:\\nPython\\nfrom langchain_openai import ChatOpenAI\\nmodel = ChatOpenAI()\\ndef chatbot(state: State):\\n    answer = model.invoke(state[\"messages\"])\\n    return {\"messages\": [answer]}\\n# The first argument is the unique node name\\n# The second argument is the function or Runnable to run\\nbuilder.add_node(\"chatbot\", chatbot)\\nJavaScript\\nimport {ChatOpenAI} from \\'@langchain/openai\\'\\nimport {\\n  AIMessage,\\n  SystemMessage,\\n  HumanMessage\\n} from \"@langchain/core/messages\";\\nconst model = new ChatOpenAI()\\nasync function chatbot(state) {\\n  const answer = await model.invoke(state.messages)\\n  return {\"messages\": answer}\\n}\\nbuilder = builder.addNode(\\'chatbot\\', chatbot)\\nThis node receives the current state, does one LLM call, and then returns an update\\nto the state containing the new message produced by the LLM. The add_messages\\nreducer appends this message to the messages already in the state.\\nCreating a StateGraph | 103'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 131, 'page_label': '104'}, page_content=\"And finally let‚Äôs add the edges:\\nPython\\nbuilder.add_edge(START, 'chatbot')\\nbuilder.add_edge('chatbot', END)\\ngraph = builder.compile()\\nJavaScript\\nbuilder = builder\\n  .addEdge(START, 'chatbot')\\n  .addEdge('chatbot', END)\\nlet graph = builder.compile()\\nThis does a few things:\\n‚Ä¢ It tells the graph where to start its work each time you run it.‚Ä¢\\n‚Ä¢ This instructs the graph where it should exit (this is optional, as LangGraph will‚Ä¢\\nstop execution once there‚Äôs no more nodes to run).\\n‚Ä¢ It compiles the graph into a runnable object, with the familiar invoke and stream‚Ä¢\\nmethods.\\nWe can also draw a visual representation of the graph:\\nPython\\ngraph.get_graph().draw_mermaid_png()\\nJavaScript\\nawait graph.getGraph().drawMermaidPng()\\nThe graph we just made looks like Figure 4-6.\\nFigure 4-6. A simple chatbot\\n104 | Chapter 4: Using LangGraph to Add Memory to Your Chatbot\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 132, 'page_label': '105'}, page_content='Y ou can run it with the familiar stream() method you‚Äôve seen in earlier chapters:\\nPython\\ninput = {\"messages\": [HumanMessage(\\'hi!)]}\\nfor chunk in graph.stream(input):\\n    print(chunk)\\nJavaScript\\nconst input = {messages: [new HumanMessage(\\'hi!)]}\\nfor await (const chunk of await graph.stream(input)) {\\n  console.log(chunk)\\n}\\nThe output:\\n{ \"chatbot\": { \"messages\": [AIMessage(\"How can I help you?\")] } }\\nNotice how the input to the graph was in the same shape as the State object\\nwe defined earlier; that is, we sent in a list of messages in the messages key of a\\ndictionary. In addition, the stream function streams the full value of the state after\\neach step of the graph.\\nAdding Memory to StateGraph\\nLangGraph has built-in persistence, which is used in the same way for the simplest\\ngraph to the most complex. Let‚Äôs see what it looks like to apply it to this first archi‚Äê\\ntecture. We‚Äôll recompile our graph, now attaching a checkpointer, which is a storage\\nadapter for LangGraph. LangGraph ships with a base class that any user can subclass\\nto create an adapter for their favorite database; at the time of writing, LangGraph\\nships with several adapters maintained by LangChain:\\n‚Ä¢ An in-memory adapter, which we‚Äôll use for our examples here‚Ä¢\\n‚Ä¢ A SQLite adapter, using the popular in-process database, appropriate for local‚Ä¢\\napps and testing\\n‚Ä¢ A Postgres adapter, optimized for the popular relational database and appropriate‚Ä¢\\nfor large-scale applications.\\nMany developers have written adapters for other database systems, such as Redis or\\nMySQL:\\nPython\\nfrom langgraph.checkpoint.memory import MemorySaver\\ngraph = builder.compile(checkpointer=MemorySaver())\\nAdding Memory to StateGraph | 105'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 133, 'page_label': '106'}, page_content='JavaScript\\nimport {MemorySaver} from \\'@langchain/langgraph\\'\\nconst graph = builder.compile({ checkpointer: new MemorySaver() })\\nThis returns a runnable object with the same methods as the one used in the previous\\ncode block. But now, it stores the state at the end of each step, so every invocation\\nafter the first doesn‚Äôt start from a blank slate. Any time the graph is called, it starts by\\nusing the checkpointer to fetch the most recent saved state, if any, and combines the\\nnew input with the previous state. And only then does it execute the first nodes.\\nLet‚Äôs see the difference in action:\\nPython\\nthread1 = {\"configurable\": {\"thread_id\": \"1\"}}\\nresult_1 = graph.invoke(\\n    { \"messages\": [HumanMessage(\"hi, my name is Jack!\")] }, \\n    thread1\\n)\\n// { \"chatbot\": { \"messages\": [AIMessage(\"How can I help you, Jack?\")] } }\\nresult_2 = graph.invoke(\\n    { \"messages\": [HumanMessage(\"what is my name?\")] }, \\n    thread1\\n)\\n// { \"chatbot\": { \"messages\": [AIMessage(\"Your name is Jack\")] } }\\nJavaScript\\nconst thread1 = {configurable: {thread_id: \\'1\\'}}\\nconst result_1 = await graph.invoke(\\n  { \"messages\": [new HumanMessage(\"hi, my name is Jack!\")] },\\n  thread1\\n)\\n// { \"chatbot\": { \"messages\": [AIMessage(\"How can I help you, Jack?\")] } }\\nconst result_2 = await graph.invoke(\\n  { \"messages\": [new HumanMessage(\"what is my name?\")] },\\n  thread1\\n)\\n// { \"chatbot\": { \"messages\": [AIMessage(\"Your name is Jack\")] } }\\nNotice the object thread1, which identifies the current interaction as belonging to a\\nparticular history of interactions‚Äîwhich are called threads in LangGraph. Threads\\nare created automatically when first used. Any string is a valid identifier for a thread\\n(usually, Universally Unique Identifiers [UUIDs] are used). The existence of threads\\nhelps you achieve an important milestone in your LLM application; it can now be\\nused by multiple users with independent conversations that are never mixed up.\\n106 | Chapter 4: Using LangGraph to Add Memory to Your Chatbot'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 134, 'page_label': '107'}, page_content=\"As before, the chatbot node is first called with a single message (the one we just\\npassed in) and returns another message, both of which are then saved in the state.\\nThe second time we execute the graph on the same thread, the chatbot node is called\\nwith three messages, the two saved from the first execution, and the next question\\nfrom the user. This is the essence of memory: the previous state is still there, which\\nmakes it possible, for instance, to answer questions about something said before (and\\ndo many more interesting things, of which we will see more later).\\nY ou can also inspect and update the state directly; let‚Äôs see how:\\nPython\\ngraph.get_state(thread1)\\nJavaScript\\nawait graph.getState(thread1)\\nThis returns the current state of this thread.\\nAnd you can update the state like this:\\nPython\\ngraph.update_state(thread1, [HumanMessage('I like LLMs!)])\\nJavaScript\\nawait graph.updateState(thread1, [new HumanMessage('I like LLMs!)])\\nThis would add one more message to the list of messages in the state, to be used the\\nnext time you invoke the graph on this thread.\\nModifying Chat History\\nIn many cases, the chat history messages aren‚Äôt in the best state or format to generate\\nan accurate response from the model. To overcome this problem, we can modify the\\nchat history in three main ways: trimming, filtering, and merging messages.\\nTrimming Messages\\nLLMs have limited context windows; in other words, there is a maximum number\\nof tokens that LLMs can receive as a prompt. As such, the final prompt sent to\\nthe model shouldn‚Äôt exceed that limit (particular to each mode), as models will\\neither refuse an overly long prompt or truncate it. In addition, excessive prompt\\ninformation can distract the model and lead to hallucination.\\nModifying Chat History | 107\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 135, 'page_label': '108'}, page_content='An effective solution to this problem is to limit the number of messages that are\\nretrieved from chat history and appended to the prompt. In practice, we need only to\\nload and store the most recent messages. Let‚Äôs use an example chat history with some\\npreloaded messages.\\nFortunately, LangChain provides the built-in trim_messages helper that incorporates\\nvarious strategies to meet these requirements. For example, the trimmer helper ena‚Äê\\nbles specifying how many tokens we want to keep or remove from chat history.\\nHere‚Äôs an example that retrieves the last max_tokens in the list of messages by setting\\na strategy parameter to \"last\":\\nPython\\nfrom langchain_core.messages import SystemMessage, trim_messages\\nfrom langchain_openai import ChatOpenAI\\ntrimmer = trim_messages(\\n    max_tokens=65,\\n    strategy=\"last\",\\n    token_counter=ChatOpenAI(model=\"gpt-4o\"),\\n    include_system=True,\\n    allow_partial=False,\\n    start_on=\"human\",\\n)\\nmessages = [\\n    SystemMessage(content=\"you\\'re a good assistant\"),\\n    HumanMessage(content=\"hi! I\\'m bob\"),\\n    AIMessage(content=\"hi!\"),\\n    HumanMessage(content=\"I like vanilla ice cream\"),\\n    AIMessage(content=\"nice\"),\\n    HumanMessage(content=\"what\\'s 2 + 2\"),\\n    AIMessage(content=\"4\"),\\n    HumanMessage(content=\"thanks\"),\\n    AIMessage(content=\"no problem!\"),\\n    HumanMessage(content=\"having fun?\"),\\n    AIMessage(content=\"yes!\"),\\n]\\ntrimmer.invoke(messages)\\nJavaScript\\nimport {\\n  AIMessage,\\n  HumanMessage,\\n  SystemMessage,\\n  trimMessages,\\n} from \"@langchain/core/messages\";\\nimport { ChatOpenAI } from \"@langchain/openai\";\\n108 | Chapter 4: Using LangGraph to Add Memory to Your Chatbot'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 136, 'page_label': '109'}, page_content='const trimmer = trimMessages({\\n  maxTokens: 65,\\n  strategy: \"last\",\\n  tokenCounter: new ChatOpenAI({ modelName: \"gpt-4o\" }),\\n  includeSystem: true,\\n  allowPartial: false,\\n  startOn: \"human\",\\n});\\nconst messages = [\\n  new SystemMessage(\"you\\'re a good assistant\"),\\n  new HumanMessage(\"hi! I\\'m bob\"),\\n  new AIMessage(\"hi!\"),\\n  new HumanMessage(\"I like vanilla ice cream\"),\\n  new AIMessage(\"nice\"),\\n  new HumanMessage(\"what\\'s 2 + 2\"),\\n  new AIMessage(\"4\"),\\n  new HumanMessage(\"thanks\"),\\n  new AIMessage(\"no problem!\"),\\n  new HumanMessage(\"having fun?\"),\\n  new AIMessage(\"yes!\"),\\n]\\nconst trimmed = await trimmer.invoke(messages);\\nThe output:\\n[SystemMessage(content=\"you\\'re a good assistant\"),\\n HumanMessage(content=\\'what\\'s 2 + 2\\'),\\n AIMessage(content=\\'4\\'),\\n HumanMessage(content=\\'thanks\\'),\\n AIMessage(content=\\'no problem!\\'),\\n HumanMessage(content=\\'having fun?\\'),\\n AIMessage(content=\\'yes!\\')]\\nNote the following:\\n‚Ä¢ The parameter strategy controls whether to start from the beginning or the end‚Ä¢\\nof the list. Usually, you‚Äôll want to prioritize the most recent messages and cut older\\nmessages if they don‚Äôt fit. That is, start from the end of the list. For this behavior,\\nchoose the value last. The other available option is first, which would prioritize\\nthe oldest messages and cut more recent messages if they don‚Äôt fit.\\n‚Ä¢ The token_counter is an LLM or chat model, which will be used to count tokens‚Ä¢\\nusing the tokenizer appropriate to that model.\\n‚Ä¢ We can add the parameter include_system=True to ensure that the trimmer‚Ä¢\\nkeeps the system message.\\n‚Ä¢ The parameter allow_partial determines whether to cut the last message‚Äôs con‚Äê‚Ä¢\\ntent to fit within the limit. In our example, we set this to false, which completely\\nremoves the message that would send the total over the limit.\\nModifying Chat History | 109'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 137, 'page_label': '110'}, page_content='‚Ä¢ The parameter start_on=\"human\" ensures that we never remove an AIMessage‚Ä¢\\n(that is, a response from the model) without also removing a corresponding\\nHumanMessage (the question for that response).\\nFiltering Messages\\nAs the list of chat history messages grows, a wider variety of types, subchains, and\\nmodels may be utilized. LangChain‚Äôs filter_messages helper makes it easier to filter\\nthe chat history messages by type, ID, or name.\\nHere‚Äôs an example where we filter for human messages:\\nPython\\nfrom langchain_core.messages import (\\n    AIMessage,\\n    HumanMessage,\\n    SystemMessage,\\n    filter_messages,\\n)\\nmessages = [\\n    SystemMessage(\"you are a good assistant\", id=\"1\"),\\n    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\\n    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\\n    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),\\n    AIMessage(\"real output\", id=\"5\", name=\"alice\"),\\n]\\nfilter_messages(messages, include_types=\"human\")\\nJavaScript\\nimport {\\n  HumanMessage,\\n  SystemMessage,\\n  AIMessage,\\n  filterMessages,\\n} from \"@langchain/core/messages\";\\nconst messages = [\\n  new SystemMessage({content: \"you are a good assistant\", id: \"1\"}),\\n  new HumanMessage({content: \"example input\", id: \"2\", name: \"example_user\"}),\\n  new AIMessage({content: \"example output\", id: \"3\", name: \"example_assistant\"}),\\n  new HumanMessage({content: \"real input\", id: \"4\", name: \"bob\"}),\\n  new AIMessage({content: \"real output\", id: \"5\", name: \"alice\"}),\\n];\\nfilterMessages(messages, { includeTypes: [\"human\"] });\\n110 | Chapter 4: Using LangGraph to Add Memory to Your Chatbot'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 138, 'page_label': '111'}, page_content='The output:\\n[HumanMessage(content=\\'example input\\', name=\\'example_user\\', id=\\'2\\'),\\n HumanMessage(content=\\'real input\\', name=\\'bob\\', id=\\'4\\')]\\nLet‚Äôs try another example where we filter to exclude users and IDs, and include\\nmessage types:\\nPython\\nfilter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])\\n\"\"\"\\n[SystemMessage(content=\\'you are a good assistant\\', id=\\'1\\'),\\nHumanMessage(content=\\'real input\\', name=\\'bob\\', id=\\'4\\'),\\nAIMessage(content=\\'real output\\', name=\\'alice\\', id=\\'5\\')]\\n\"\"\"\\nfilter_messages(\\n    messages, \\n    include_types=[HumanMessage, AIMessage], \\n    exclude_ids=[\"3\"]\\n)\\n\"\"\"\\n[HumanMessage(content=\\'example input\\', name=\\'example_user\\', id=\\'2\\'),\\n HumanMessage(content=\\'real input\\', name=\\'bob\\', id=\\'4\\'),\\n AIMessage(content=\\'real output\\', name=\\'alice\\', id=\\'5\\')]\\n\"\"\"\\nJavaScript\\nfilterMessages(\\n  messages, \\n  { excludeNames: [\"example_user\", \\n  \"example_assistant\"] }\\n);\\n/*\\n[SystemMessage(content=\\'you are a good assistant\\', id=\\'1\\'),\\nHumanMessage(content=\\'real input\\', name=\\'bob\\', id=\\'4\\'),\\nAIMessage(content=\\'real output\\', name=\\'alice\\', id=\\'5\\')]\\n*/\\nfilterMessages(messages, { includeTypes: [\"human\", \"ai\"], excludeIds: [\"3\"] });\\n/*\\n[HumanMessage(content=\\'example input\\', name=\\'example_user\\', id=\\'2\\'),\\n HumanMessage(content=\\'real input\\', name=\\'bob\\', id=\\'4\\'),\\n AIMessage(content=\\'real output\\', name=\\'alice\\', id=\\'5\\')]\\n*/\\nModifying Chat History | 111'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 139, 'page_label': '112'}, page_content='The filter_messages helper can also be used imperatively or declaratively, making it\\neasy to compose with other components in a chain:\\nPython\\nmodel = ChatOpenAI()\\nfilter_ = filter_messages(exclude_names=[\"example_user\", \"example_assistant\"])\\nchain = filter_ | model\\nJavaScript\\nconst model = new ChatOpenAI()\\nconst filter = filterMessages({\\n  excludeNames: [\"example_user\", \"example_assistant\"]\\n})\\nconst chain = filter.pipe(model)\\nMerging Consecutive Messages\\nCertain models don‚Äôt support inputs, including consecutive messages of the same type\\n(for instance, Anthropic chat models). LangChain‚Äôs merge_message_runs utility makes\\nit easy to merge consecutive messages of the same type:\\nPython\\nfrom langchain_core.messages import (\\n    AIMessage,\\n    HumanMessage,\\n    SystemMessage,\\n    merge_message_runs,\\n)\\nmessages = [\\n    SystemMessage(\"you\\'re a good assistant.\"),\\n    SystemMessage(\"you always respond with a joke.\"),\\n    HumanMessage(\\n        [{\"type\": \"text\", \"text\": \"i wonder why it\\'s called langchain\"}]\\n    ),\\n    HumanMessage(\"and who is harrison chasing anyway\"),\\n    AIMessage(\\n        \\'\\'\\'Well, I guess they thought \"WordRope\" and \"SentenceString\" just \\n        didn\\\\\\'t have the same ring to it!\\'\\'\\'\\n    ),\\n    AIMessage(\"\"\"Why, he\\'s probably chasing after the last cup of coffee in the \\n        office!\"\"\"),\\n]\\nmerge_message_runs(messages)\\n112 | Chapter 4: Using LangGraph to Add Memory to Your Chatbot'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 140, 'page_label': '113'}, page_content='JavaScript\\nimport {\\n  HumanMessage,\\n  SystemMessage,\\n  AIMessage,\\n  mergeMessageRuns,\\n} from \"@langchain/core/messages\";\\nconst messages = [\\n  new SystemMessage(\"you\\'re a good assistant.\"),\\n  new SystemMessage(\"you always respond with a joke.\"),\\n  new HumanMessage({\\n    content: [{ type: \"text\", text: \"i wonder why it\\'s called langchain\" }],\\n  }),\\n  new HumanMessage(\"and who is harrison chasing anyway\"),\\n  new AIMessage(\\n    `Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\\\\\'t \\n      have the same ring to it!`\\n  ),\\n  new AIMessage(\\n    \"Why, he\\'s probably chasing after the last cup of coffee in the office!\"\\n  ),\\n];\\nmergeMessageRuns(messages);\\nThe output:\\n[SystemMessage(content=\"you\\'re a good assistant.\\\\nyou always respond with a \\n    joke.\"),\\n HumanMessage(content=[{\\'type\\': \\'text\\', \\'text\\': \"i wonder why it\\'s called\\n    langchain\"}, \\'and who is harrison chasing anyway\\']),\\n AIMessage(content=\\'Well, I guess they thought \"WordRope\" and \"SentenceString\" \\n    just didn\\\\\\'t have the same ring to it!\\\\nWhy, he\\\\\\'s probably chasing after \\n    the last cup of coffee in the office!\\')]\\nNotice that if the contents of one of the messages to merge is a list of content blocks,\\nthen the merged message will have a list of content blocks. And if both messages to\\nmerge have string contents, then those are concatenated with a newline character.\\nThe merge_message_runs helper can be used imperatively or declaratively, making it\\neasy to compose with other components in a chain:\\nPython\\nmodel = ChatOpenAI()\\nmerger = merge_message_runs()\\nchain = merger | model\\nModifying Chat History | 113'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 141, 'page_label': '114'}, page_content='JavaScript\\nconst model = new ChatOpenAI()\\nconst merger = mergeMessageRuns()\\nconst chain = merger.pipe(model)\\nSummary\\nThis chapter covered the fundamentals of building a simple memory system that\\nenables your AI chatbot to remember its conversations with a user. We discussed\\nhow to automate the storage and updating of chat history using LangGraph to make\\nthis easier. We also discussed the importance of modifying chat history and explored\\nvarious strategies to trim, filter, and summarize chat messages.\\nIn Chapter 5, you‚Äôll learn how to enable your AI chatbot to do more than just chat\\nback: for instance, your new model will be able to make decisions, pick actions, and\\nreflect on its past outputs.\\n114 | Chapter 4: Using LangGraph to Add Memory to Your Chatbot'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 142, 'page_label': '115'}, page_content='CHAPTER 5\\nCognitive Architectures with LangGraph\\nSo far, we‚Äôve looked at the most common features of LLM applications:\\n‚Ä¢ Prompting techniques in the Preface and Chapter 1‚Ä¢\\n‚Ä¢ RAG in Chapters 2 and 3‚Ä¢\\n‚Ä¢ Memory in Chapter 4‚Ä¢\\nThe next question should be: How do we assemble these pieces into a coherent\\napplication that achieves the goal we set out to solve? To draw a parallel with the\\nworld of bricks and mortar, a swimming pool and a one-story house are built of\\nthe same materials, but obviously serve very different purposes. What makes them\\nuniquely suited to their different purposes is the plan for how those materials are\\ncombined‚Äîthat is, their architecture. The same is true when building LLM applica‚Äê\\ntions. The most important decisions you have to make are how to assemble the\\ndifferent components you have at your disposal (such as RAG, prompting techniques,\\nmemory) into something that achieves your purpose.\\nBefore we look at specific architectures, let‚Äôs walk through an example. Any LLM\\napplication you might build will start from a purpose: what the app is designed to do.\\nLet‚Äôs say you want to build an email assistant‚Äîan LLM application that reads your\\nemails before you do and aims to reduce the amount of emails you need to look at.\\nThe application might do this by archiving a few uninteresting ones, directly replying\\nto some, and marking others as deserving of your attention later.\\n115'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 143, 'page_label': '116'}, page_content='1 Theodore R. Sumers et al., ‚ÄúCognitive Architectures for Language Agents‚Äù, arXiv, September 5, 2023, updated\\nMarch 15, 2024.\\nY ou probably also would want the app to be bound by some constraints in its action.\\nListing those constraints helps tremendously, as they will help inform the search for\\nthe right architecture. Chapter 8 covers these constraints in more detail and how to\\nwork with them. For this hypothetical email assistant, let‚Äôs say we‚Äô d like it to do the\\nfollowing:\\n‚Ä¢ Minimize the number of times it interrupts you (after all, the whole point is to‚Ä¢\\nsave time).\\n‚Ä¢ Avoid having your email correspondents receive a reply that you‚Äô d never have‚Ä¢\\nsent yourself.\\nThis hints at the key trade-off often faced when building LLM apps: the trade-off\\nbetween agency (or the capacity to act autonomously) and  reliability (or the degree to\\nwhich you can trust its outputs). Intuitively, the email assistant will be more useful\\nif it takes more actions without your involvement, but if you take it too far, it will\\ninevitably send emails you wish it hadn‚Äôt.\\nOne way to describe the degree of autonomy of an LLM application is to evaluate how\\nmuch of the behavior of the application is determined by an LLM (versus code):\\n‚Ä¢ Have an LLM decide the output of a step (for instance, write a draft reply to an‚Ä¢\\nemail).\\n‚Ä¢ Have an LLM decide the next step to take (for instance, for a new email, decide‚Ä¢\\nbetween the three actions it can take on an email: archive, reply, or mark for\\nreview).\\n‚Ä¢ Have an LLM decide what steps are available to take (for instance, have the‚Ä¢\\nLLM write code that executes a dynamic action you didn‚Äôt preprogram into the\\napplication).\\nWe can classify a number of popular recipes for building LLM applications based on\\nwhere they fall in this spectrum of autonomy, that is, which of the three tasks just\\nmentioned are handled by an LLM and which remain in the hands of the developer\\nor user. These recipes can be called cognitive architectures. In the artificial intelligence\\nfield, the term cognitive architecture has long been used to denote models of human\\nreasoning (and their implementations in computers). An LLM cognitive architecture\\n(the term was first applied to LLMs, to our knowledge, in a paper 1) can be defined\\nas a recipe for the steps to be taken by an LLM application (see Figure 5-1). A step\\nis, for instance, retrieval of relevant documents (RAG), or calling an LLM with a\\nchain-of-thought prompt.\\n116 | Chapter 5: Cognitive Architectures with LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 144, 'page_label': '117'}, page_content='Figure 5-1. Cognitive architectures for LLM applications\\nNow let‚Äôs look at each of the major architectures, or recipes, that you can use when\\nbuilding your application (as shown in Figure 5-1):\\n0: Code\\nThis is not an LLM cognitive architecture (hence we numbered it 0), as it doesn‚Äôt\\nuse LLMs at all. Y ou can think of this as regular software you‚Äôre used to writing.\\nThe first interesting architecture (for this book, at any rate) is actually the next\\none.\\n1: LLM call\\nThis is the majority of the examples we‚Äôve seen in the book so far, with one LLM\\ncall only. This is useful mostly when it‚Äôs part of a larger application that makes\\nuse of an LLM for achieving a specific task, such as translating or summarizing a\\npiece of text.\\n2: Chain\\nThe next level up, so to speak, comes with the use of multiple LLM calls in a\\npredefined sequence. For instance, a text-to-SQL application (which receives as\\ninput from the user a natural language description of some calculation to make\\nover a database) could make use of two LLM calls in sequence:\\nOne LLM call to generate a SQL query, from the natural language query, pro‚Äê\\nvided by the user, and a description of the database contents, provided by the\\ndeveloper.\\nCognitive Architectures with LangGraph | 117'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 145, 'page_label': '118'}, page_content='And another LLM call to write an explanation of the query appropriate for a\\nnontechnical user, given the query generated in the previous call. This one could\\nthen be used to enable the user to check if the generated query matches his\\nrequest.\\n3: Router\\nThis next step comes from using the LLM to define the sequence of steps to\\ntake. That is, whereas the chain architecture always executes a static sequence\\nof steps (however many) determined by the developer, the router architecture is\\ncharacterized by using an LLM to choose between certain predefined steps. An\\nexample would be a RAG application with multiple indexes of documents from\\ndifferent domains, with the following steps:\\n1. An LLM call to pick which of the available indexes to use, given the user-1.\\nsupplied query and the developer-supplied description of the indexes.\\n2. A retrieval step that queries the chosen index for the most relevant docu‚Äê2.\\nments for the user query.\\n3. Another LLM call to generate an answer, given the user-supplied query and3.\\nthe list of relevant documents fetched from the index.\\nThat‚Äôs as far as we‚Äôll go in this chapter. We will talk about each of these architectures\\nin turn. The next chapters discuss the agentic architectures, which make even more\\nuse of LLMs. But first let‚Äôs talk about some better tooling to help us on this journey.\\nArchitecture #1: LLM Call\\nAs an example of the LLM call architecture, we‚Äôll return to the chatbot we created in\\nChapter 4. This chatbot will respond directly to user messages.\\nStart by creating a StateGraph, to which we‚Äôll add a node to represent the LLM call:\\nPython\\nfrom typing import Annotated, TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\nfrom langchain_openai import ChatOpenAI\\nmodel = ChatOpenAI()\\nclass State(TypedDict):\\n    # Messages have the type \"list\". The `add_messages` \\n    # function in the annotation defines how this state should \\n    # be updated (in this case, it appends new messages to the \\n    # list, rather than replacing the previous messages)\\n    messages: Annotated[list, add_messages]\\n118 | Chapter 5: Cognitive Architectures with LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 146, 'page_label': '119'}, page_content='def chatbot(state: State):\\n    answer = model.invoke(state[\"messages\"])\\n    return {\"messages\": [answer]}\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"chatbot\", chatbot)\\nbuilder.add_edge(START, \\'chatbot\\')\\nbuilder.add_edge(\\'chatbot\\', END)\\ngraph = builder.compile()\\nJavaScript\\nimport {\\n  StateGraph,\\n  Annotation,\\n  messagesStateReducer,\\n  START, END\\n} from \\'@langchain/langgraph\\'\\nimport {ChatOpenAI} from \\'@langchain/openai\\'\\nconst model = new ChatOpenAI()\\nconst State = {\\n  /**\\n  * The State defines three things:\\n  * 1. The structure of the graph\\'s state (which \"channels\" are available to \\n  * read/write)\\n  * 2. The default values for the state\\'s channels\\n  * 3. The reducers for the state\\'s channels. Reducers are functions that \\n  * determine how to apply updates to the state. Below, new messages are \\n  * appended to the messages array.\\n  */\\n  messages: Annotation({\\n    reducer: messagesStateReducer,\\n    default: () => []\\n  }),\\n}\\nasync function chatbot(state) {\\n  const answer = await model.invoke(state.messages)\\n  return {\"messages\": answer}\\n}\\nconst builder = new StateGraph(State)\\n  .addNode(\\'chatbot\\', chatbot)\\n  .addEdge(START, \\'chatbot\\')\\n  .addEdge(\\'chatbot\\', END)\\nconst graph = builder.compile()\\nArchitecture #1: LLM Call | 119'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 147, 'page_label': '120'}, page_content='We can also draw a visual representation of the graph:\\nPython\\ngraph.get_graph().draw_mermaid_png()\\nJavaScript\\nawait graph.getGraph().drawMermaidPng()\\nThe graph we just made looks like Figure 5-2.\\nFigure 5-2. The LLM call architecture\\nY ou can run it with the familiar stream() method you‚Äôve seen in earlier chapters:\\nPython\\ninput = {\"messages\": [HumanMessage(\\'hi!)]}\\nfor chunk in graph.stream(input):\\n    print(chunk)\\nJavaScript\\nconst input = {messages: [new HumanMessage(\\'hi!)]}\\nfor await (const chunk of await graph.stream(input)) {\\n  console.log(chunk)\\n}\\nThe output:\\n{ \"chatbot\": { \"messages\": [AIMessage(\"How can I help you?\")] } }\\nNotice how the input to the graph was in the same shape as the State object\\nwe defined earlier; that is, we sent in a list of messages in the messages key of a\\ndictionary.\\nThis is the simplest possible architecture for using an LLM, which is not to say that it\\nshould never be used. Here are some examples of where you might see it in action in\\npopular products, among many others:\\n120 | Chapter 5: Cognitive Architectures with LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 148, 'page_label': '121'}, page_content='2 Tal Ridnik et al., ‚ÄúCode Generation with AlphaCodium: From Prompt Engineering to Flow Engineering‚Äù,\\narXiv, January 16, 2024.\\n‚Ä¢ AI-powered features such as summarize and translate (such as you can find in‚Ä¢\\nNotion, a popular writing software) can be powered by a single LLM call.\\n‚Ä¢ Simple SQL query generation can be powered by a single LLM call, depending on‚Ä¢\\nthe UX and target user the developer has in mind.\\nArchitecture #2: Chain\\nThis next architecture extends on all that by using multiple LLM calls, in a predefined\\nsequence (that is, different invocations of the application do the same sequence of\\nLLM calls, albeit with different inputs and results).\\nLet‚Äôs take as an example a text-to-SQL application, which receives as input from the\\nuser a natural language description of some calculation to make over a database. We\\nmentioned earlier that this could be achieved with a single LLM call, to generate\\na SQL query, but we can create a more sophisticated application by making use of\\nmultiple LLM calls in sequence. Some authors call this architecture flow engineering.2\\nFirst let‚Äôs describe the flow in words:\\n1. One LLM call to generate a SQL query from the natural language query, pro‚Äê1.\\nvided by the user, and a description of the database contents, provided by the\\ndeveloper.\\n2. Another LLM call to write an explanation of the query appropriate for a nontech‚Äê2.\\nnical user, given the query generated in the previous call. This one could then be\\nused to enable the user to check if the generated query matches his request.\\nY ou could also extend this even further (but we won‚Äôt do that here) with additional\\nsteps to be taken after the preceding two:\\n3. Executes the query against the database, which returns a two-dimensional table.3.\\n4. Uses a third LLM call to summarize the query results into a textual answer to the4.\\noriginal user question.\\nAnd now let‚Äôs implement this with LangGraph:\\nPython\\nfrom typing import Annotated, TypedDict\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\nfrom langchain_openai import ChatOpenAI\\nArchitecture #2: Chain | 121'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 149, 'page_label': '122'}, page_content='from langgraph.graph import END, START, StateGraph\\nfrom langgraph.graph.message import add_messages\\n# useful to generate SQL query\\nmodel_low_temp = ChatOpenAI(temperature=0.1)\\n# useful to generate natural language outputs\\nmodel_high_temp = ChatOpenAI(temperature=0.7)\\nclass State(TypedDict):\\n    # to track conversation history\\n    messages: Annotated[list, add_messages]\\n    # input\\n    user_query: str\\n    # output\\n    sql_query: str\\n    sql_explanation: str\\nclass Input(TypedDict):\\n    user_query: str\\nclass Output(TypedDict):\\n    sql_query: str\\n    sql_explanation: str\\ngenerate_prompt = SystemMessage(\\n    \"\"\"You are a helpful data analyst who generates SQL queries for users based \\n    on their questions.\"\"\"\\n)\\ndef generate_sql(state: State) -> State:\\n    user_message = HumanMessage(state[\"user_query\"])\\n    messages = [generate_prompt, *state[\"messages\"], user_message]\\n    res = model_low_temp.invoke(messages)\\n    return {\\n        \"sql_query\": res.content,\\n        # update conversation history\\n        \"messages\": [user_message, res],\\n    }\\nexplain_prompt = SystemMessage(\\n    \"You are a helpful data analyst who explains SQL queries to users.\"\\n)\\ndef explain_sql(state: State) -> State:\\n    messages = [\\n        explain_prompt,\\n        # contains user\\'s query and SQL query from prev step\\n        *state[\"messages\"],\\n    ]\\n    res = model_high_temp.invoke(messages)\\n    return {\\n122 | Chapter 5: Cognitive Architectures with LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 150, 'page_label': '123'}, page_content='\"sql_explanation\": res.content,\\n        # update conversation history\\n        \"messages\": res,\\n    }\\nbuilder = StateGraph(State, input=Input, output=Output)\\nbuilder.add_node(\"generate_sql\", generate_sql)\\nbuilder.add_node(\"explain_sql\", explain_sql)\\nbuilder.add_edge(START, \"generate_sql\")\\nbuilder.add_edge(\"generate_sql\", \"explain_sql\")\\nbuilder.add_edge(\"explain_sql\", END)\\ngraph = builder.compile()\\nJavaScript\\nimport {\\n  HumanMessage,\\n  SystemMessage\\n} from \"@langchain/core/messages\";\\nimport { ChatOpenAI } from \"@langchain/openai\";\\nimport {\\n  StateGraph,\\n  Annotation,\\n  messagesStateReducer,\\n  START,\\n  END,\\n} from \"@langchain/langgraph\";\\n// useful to generate SQL query\\nconst modelLowTemp = new ChatOpenAI({ temperature: 0.1 });\\n// useful to generate natural language outputs\\nconst modelHighTemp = new ChatOpenAI({ temperature: 0.7 });\\nconst annotation = Annotation.Root({\\n  messages: Annotation({ reducer: messagesStateReducer, default: () => [] }),\\n  user_query: Annotation(),\\n  sql_query: Annotation(),\\n  sql_explanation: Annotation(),\\n});\\nconst generatePrompt = new SystemMessage(\\n  `You are a helpful data analyst who generates SQL queries for users based on \\n    their questions.`\\n);\\nasync function generateSql(state) {\\n  const userMessage = new HumanMessage(state.user_query);\\n  const messages = [generatePrompt, ...state.messages, userMessage];\\n  const res = await modelLowTemp.invoke(messages);\\n  return {\\n    sql_query: res.content as string,\\n    // update conversation history\\nArchitecture #2: Chain | 123'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 151, 'page_label': '124'}, page_content='messages: [userMessage, res],\\n  };\\n}\\nconst explainPrompt = new SystemMessage(\\n  \"You are a helpful data analyst who explains SQL queries to users.\"\\n);\\nasync function explainSql(state) {\\n  const messages = [explainPrompt, ...state.messages];\\n  const res = await modelHighTemp.invoke(messages);\\n  return {\\n    sql_explanation: res.content as string,\\n    // update conversation history\\n    messages: res,\\n  };\\n}\\nconst builder = new StateGraph(annotation)\\n  .addNode(\"generate_sql\", generateSql)\\n  .addNode(\"explain_sql\", explainSql)\\n  .addEdge(START, \"generate_sql\")\\n  .addEdge(\"generate_sql\", \"explain_sql\")\\n  .addEdge(\"explain_sql\", END);\\nconst graph = builder.compile();\\nThe visual representation of the graph is shown in Figure 5-3.\\nFigure 5-3. The chain architecture\\n124 | Chapter 5: Cognitive Architectures with LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 152, 'page_label': '125'}, page_content='Here‚Äôs an example of inputs and outputs:\\nPython\\ngraph.invoke({\\n  \"user_query\": \"What is the total sales for each product?\"\\n})\\nJavaScript\\nawait graph.invoke({\\n  user_query: \"What is the total sales for each product?\"\\n})\\nThe output:\\n{\\n  \"sql_query\": \"SELECT product_name, SUM(sales_amount) AS total_sales\\\\nFROM \\n      sales\\\\nGROUP BY product_name;\",\\n  \"sql_explanation\": \"This query will retrieve the total sales for each product \\n      by summing up the sales_amount column for each product and grouping the\\n      results by product_name.\",\\n}\\nFirst, the generate_sql node is executed, which populates the sql_query key in the\\nstate (which will be part of the final output) and updates the messages key with the\\nnew messages. Then the explain_sql node runs, taking the SQL query generated in\\nthe previous step and populating the sql_explanation key in the state. At this point,\\nthe graph finishes running, and the output is returned to the caller.\\nNote also the use of separate input and output schemas when creating the State\\nGraph. This lets you customize which parts of the state are accepted as input from the\\nuser and which are returned as the final output. The remaining state keys are used\\nby the graph nodes internally to keep intermediate state and are made available to the\\nuser as part of the streaming output produced by stream().\\nArchitecture #3: Router\\nThis next architecture moves up the autonomy ladder by assigning to LLMs the next\\nof the responsibilities we outlined before: deciding the next step to take. That is,\\nwhereas the chain architecture always executes a static sequence of steps (however\\nmany), the router architecture is characterized by using an LLM to choose between\\ncertain predefined steps.\\nLet‚Äôs use the example of a RAG application with access to multiple indexes of docu‚Äê\\nments from different domains (refer to Chapter 2 for more on indexing). Usually\\nyou can extract better performance from LLMs by avoiding the inclusion of irrelevant\\ninformation in the prompt. Therefore, in building this application, we should try to\\npick the right index to use for each query and use only that one. The key development\\nArchitecture #3: Router | 125'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 153, 'page_label': '126'}, page_content='in this architecture is to use an LLM to make this decision, effectively using an LLM to\\nevaluate each incoming query and decide which index it should use for that particular\\nquery.\\nBefore the advent of LLMs, the usual way of solving this problem\\nwould be to build a classifier model using ML techniques and\\na dataset mapping example user queries to the right index. This\\ncould prove quite challenging, as it requires the following:\\n‚Ä¢ Assembling that dataset by hand‚Ä¢\\n‚Ä¢ Generating enough features (quantitative attributes) from each‚Ä¢\\nuser query to enable training a classifier for the task\\nLLMs, given their encoding of human language, can effectively\\nserve as this classifier with zero, or very few, examples or additional\\ntraining.\\nFirst, let‚Äôs describe the flow in words:\\n1. An LLM call to pick which of the available indexes to use, given the user-supplied1.\\nquery, and the developer-supplied description of the indexes\\n2. A retrieval step that queries the chosen index for the most relevant documents2.\\nfor the user query\\n3. Another LLM call to generate an answer, given the user-supplied query and the3.\\nlist of relevant documents fetched from the index\\nAnd now let‚Äôs implement it with LangGraph:\\nPython\\nfrom typing import Annotated, Literal, TypedDict\\nfrom langchain_core.documents import Document\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\nfrom langchain_core.vectorstores.in_memory import InMemoryVectorStore\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langgraph.graph import END, START, StateGraph\\nfrom langgraph.graph.message import add_messages\\nembeddings = OpenAIEmbeddings()\\n# useful to generate SQL query\\nmodel_low_temp = ChatOpenAI(temperature=0.1)\\n# useful to generate natural language outputs\\nmodel_high_temp = ChatOpenAI(temperature=0.7)\\nclass State(TypedDict):\\n126 | Chapter 5: Cognitive Architectures with LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 154, 'page_label': '127'}, page_content='# to track conversation history\\n    messages: Annotated[list, add_messages]\\n    # input\\n    user_query: str\\n    # output\\n    domain: Literal[\"records\", \"insurance\"]\\n    documents: list[Document]\\n    answer: str\\nclass Input(TypedDict):\\n    user_query: str\\nclass Output(TypedDict):\\n    documents: list[Document]\\n    answer: str\\n# refer to Chapter 2 on how to fill a vector store with documents\\nmedical_records_store = InMemoryVectorStore.from_documents([], embeddings)\\nmedical_records_retriever = medical_records_store.as_retriever()\\ninsurance_faqs_store = InMemoryVectorStore.from_documents([], embeddings)\\ninsurance_faqs_retriever = insurance_faqs_store.as_retriever()\\nrouter_prompt = SystemMessage(\\n    \"\"\"You need to decide which domain to route the user query to. You have two \\n        domains to choose from:\\n          - records: contains medical records of the patient, such as \\n          diagnosis, treatment, and prescriptions.\\n          - insurance: contains frequently asked questions about insurance \\n          policies, claims, and coverage.\\nOutput only the domain name.\"\"\"\\n)\\ndef router_node(state: State) -> State:\\n    user_message = HumanMessage(state[\"user_query\"])\\n    messages = [router_prompt, *state[\"messages\"], user_message]\\n    res = model_low_temp.invoke(messages)\\n    return {\\n        \"domain\": res.content,\\n        # update conversation history\\n        \"messages\": [user_message, res],\\n    }\\ndef pick_retriever(\\n    state: State,\\n) -> Literal[\"retrieve_medical_records\", \"retrieve_insurance_faqs\"]:\\n    if state[\"domain\"] == \"records\":\\n        return \"retrieve_medical_records\"\\n    else:\\n        return \"retrieve_insurance_faqs\"\\nArchitecture #3: Router | 127'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 155, 'page_label': '128'}, page_content='def retrieve_medical_records(state: State) -> State:\\n    documents = medical_records_retriever.invoke(state[\"user_query\"])\\n    return {\\n        \"documents\": documents,\\n    }\\ndef retrieve_insurance_faqs(state: State) -> State:\\n    documents = insurance_faqs_retriever.invoke(state[\"user_query\"])\\n    return {\\n        \"documents\": documents,\\n    }\\nmedical_records_prompt = SystemMessage(\\n    \"\"\"You are a helpful medical chatbot who answers questions based on the \\n        patient\\'s medical records, such as diagnosis, treatment, and \\n        prescriptions.\"\"\"\\n)\\ninsurance_faqs_prompt = SystemMessage(\\n    \"\"\"You are a helpful medical insurance chatbot who answers frequently asked \\n        questions about insurance policies, claims, and coverage.\"\"\"\\n)\\ndef generate_answer(state: State) -> State:\\n    if state[\"domain\"] == \"records\":\\n        prompt = medical_records_prompt\\n    else:\\n        prompt = insurance_faqs_prompt\\n    messages = [\\n        prompt,\\n        *state[\"messages\"],\\n        HumanMessage(f\"Documents: {state[\"documents\"]}\"),\\n    ]\\n    res = model_high_temp.invoke(messages)\\n    return {\\n        \"answer\": res.content,\\n        # update conversation history\\n        \"messages\": res,\\n    }\\nbuilder = StateGraph(State, input=Input, output=Output)\\nbuilder.add_node(\"router\", router_node)\\nbuilder.add_node(\"retrieve_medical_records\", retrieve_medical_records)\\nbuilder.add_node(\"retrieve_insurance_faqs\", retrieve_insurance_faqs)\\nbuilder.add_node(\"generate_answer\", generate_answer)\\nbuilder.add_edge(START, \"router\")\\nbuilder.add_conditional_edges(\"router\", pick_retriever)\\nbuilder.add_edge(\"retrieve_medical_records\", \"generate_answer\")\\nbuilder.add_edge(\"retrieve_insurance_faqs\", \"generate_answer\")\\nbuilder.add_edge(\"generate_answer\", END)\\ngraph = builder.compile()\\n128 | Chapter 5: Cognitive Architectures with LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 156, 'page_label': '129'}, page_content='JavaScript\\nimport {\\n  HumanMessage,\\n  SystemMessage\\n} from \"@langchain/core/messages\";\\nimport {\\n  ChatOpenAI,\\n  OpenAIEmbeddings\\n} from \"@langchain/openai\";\\nimport {\\n  MemoryVectorStore\\n} from \"langchain/vectorstores/memory\";\\nimport {\\n  DocumentInterface\\n} from \"@langchain/core/documents\";\\nimport {\\n  StateGraph,\\n  Annotation,\\n  messagesStateReducer,\\n  START,\\n  END,\\n} from \"@langchain/langgraph\";\\nconst embeddings = new OpenAIEmbeddings();\\n// useful to generate SQL query\\nconst modelLowTemp = new ChatOpenAI({ temperature: 0.1 });\\n// useful to generate natural language outputs\\nconst modelHighTemp = new ChatOpenAI({ temperature: 0.7 });\\nconst annotation = Annotation.Root({\\n  messages: Annotation({ reducer: messagesStateReducer, default: () => [] }),\\n  user_query: Annotation(),\\n  domain: Annotation(),\\n  documents: Annotation(),\\n  answer: Annotation(),\\n});\\n// refer to Chapter 2 on how to fill a vector store with documents\\nconst medicalRecordsStore = await MemoryVectorStore.fromDocuments(\\n  [],\\n  embeddings\\n);\\nconst medicalRecordsRetriever = medicalRecordsStore.asRetriever();\\nconst insuranceFaqsStore = await MemoryVectorStore.fromDocuments(\\n  [],\\n  embeddings\\n);\\nconst insuranceFaqsRetriever = insuranceFaqsStore.asRetriever();\\nconst routerPrompt = new SystemMessage(\\n  `You need to decide which domain to route the user query to. You have two \\nArchitecture #3: Router | 129'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 157, 'page_label': '130'}, page_content='domains to choose from:\\n        - records: contains medical records of the patient, such as diagnosis, \\n        treatment, and prescriptions.\\n        - insurance: contains frequently asked questions about insurance \\n        policies, claims, and coverage.\\nOutput only the domain name.`\\n);\\nasync function routerNode(state) {\\n  const userMessage = new HumanMessage(state.user_query);\\n  const messages = [routerPrompt, ...state.messages, userMessage];\\n  const res = await modelLowTemp.invoke(messages);\\n  return {\\n    domain: res.content as \"records\" | \"insurance\",\\n    // update conversation history\\n    messages: [userMessage, res],\\n  };\\n}\\nfunction pickRetriever(state) {\\n  if (state.domain === \"records\") {\\n    return \"retrieve_medical_records\";\\n  } else {\\n    return \"retrieve_insurance_faqs\";\\n  }\\n}\\nasync function retrieveMedicalRecords(state) {\\n  const documents = await medicalRecordsRetriever.invoke(state.user_query);\\n  return {\\n    documents,\\n  };\\n}\\nasync function retrieveInsuranceFaqs(state) {\\n  const documents = await insuranceFaqsRetriever.invoke(state.user_query);\\n  return {\\n    documents,\\n  };\\n}\\nconst medicalRecordsPrompt = new SystemMessage(\\n  `You are a helpful medical chatbot who answers questions based on the \\n    patient\\'s medical records, such as diagnosis, treatment, and \\n    prescriptions.`\\n);\\nconst insuranceFaqsPrompt = new SystemMessage(\\n  `You are a helpful medical insurance chatbot who answers frequently asked \\n    questions about insurance policies, claims, and coverage.`\\n);\\n130 | Chapter 5: Cognitive Architectures with LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 158, 'page_label': '131'}, page_content='async function generateAnswer(state) {\\n  const prompt =\\n    state.domain === \"records\" ? medicalRecordsPrompt : insuranceFaqsPrompt;\\n  const messages = [\\n    prompt,\\n    ...state.messages,\\n    new HumanMessage(`Documents: ${state.documents}`),\\n  ];\\n  const res = await modelHighTemp.invoke(messages);\\n  return {\\n    answer: res.content as string,\\n    // update conversation history\\n    messages: res,\\n  };\\n}\\nconst builder = new StateGraph(annotation)\\n  .addNode(\"router\", routerNode)\\n  .addNode(\"retrieve_medical_records\", retrieveMedicalRecords)\\n  .addNode(\"retrieve_insurance_faqs\", retrieveInsuranceFaqs)\\n  .addNode(\"generate_answer\", generateAnswer)\\n  .addEdge(START, \"router\")\\n  .addConditionalEdges(\"router\", pickRetriever)\\n  .addEdge(\"retrieve_medical_records\", \"generate_answer\")\\n  .addEdge(\"retrieve_insurance_faqs\", \"generate_answer\")\\n  .addEdge(\"generate_answer\", END);\\nconst graph = builder.compile();\\nThe visual representation is shown in Figure 5-4.\\nFigure 5-4. The router architecture\\nArchitecture #3: Router | 131'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 159, 'page_label': '132'}, page_content='Notice how this is now starting to become more useful, as it shows the two\\npossible paths through the graph, through retrieve_medical_records or through\\nretrieve_insurance_faqs, and that for both of those, we first visit the router node\\nand finish by visiting the generate_answer node. These two possible paths were\\nimplemented through the use of a conditional edge, implemented in the function\\npick_retriever, which maps the domain picked by the LLM to one of the two nodes\\nmentioned earlier. The conditional edge is shown in Figure 5-4 as dotted lines from\\nthe source node to the destination nodes.\\nAnd now for example inputs and outputs, this time with streaming output:\\nPython\\ninput = {\\n    \"user_query\": \"Am I covered for COVID-19 treatment?\"\\n}\\nfor c in graph.stream(input):\\n    print(c)\\nJavaScript\\nconst input = {\\n  user_query: \"Am I covered for COVID-19 treatment?\"\\n}\\nfor await (const chunk of await graph.stream(input)) {\\nconsole.log(chunk)\\n}\\nThe output  (the actual answer is not shown, since it would depend on your\\ndocuments):\\n{\\n    \"router\": {\\n        \"messages\": [\\n            HumanMessage(content=\"Am I covered for COVID-19 treatment?\"),\\n            AIMessage(content=\"insurance\"),\\n        ],\\n        \"domain\": \"insurance\",\\n    }\\n}\\n{\\n    \"retrieve_insurance_faqs\": {\\n        \"documents\": [...]\\n    }\\n}\\n{\\n    \"generate_answer\": {\\n        \"messages\": AIMessage(\\n            content=\"...\",\\n        ),\\n        \"answer\": \"...\",\\n132 | Chapter 5: Cognitive Architectures with LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 160, 'page_label': '133'}, page_content='}\\n}\\nThis output stream contains the values returned by each node that ran during this\\nexecution of the graph. Let‚Äôs take it one at a time. The top-level key in each dictionary\\nis the name of the node, and the value for that key is what that node returned:\\n1. The router node returned an update to messages (this would allow us to easily1.\\ncontinue this conversation using the memory technique described earlier), and\\nthe domain the LLM picked for this user‚Äôs query, in this case insurance.\\n2. Then the pick_retriever function ran and returned the name of the next node2.\\nto run, based on the domain identified by the LLM call in the previous step.\\n3. Then the retrieve_insurance_faqs node ran, returning a set of relevant docu‚Äê3.\\nments from that index. This means that on the drawing of the graph seen earlier,\\nwe took the left path, as decided by the LLM.\\n4. Finally, the generate_answer node ran, which took those documents and the4.\\noriginal user query and produced an answer to the question, which was written\\nto the state (along with a final update to the messages key).\\nSummary\\nThis chapter talked about the key trade-off when building LLM applications: agency\\nversus oversight. The more autonomous an LLM application is, the more it can do‚Äî\\nbut that raises the need for more mechanisms of control over its actions. We moved\\non to different cognitive architectures that strike different balances between agency\\nand oversight.\\nChapter 6 talks about the most powerful of the cognitive architectures we‚Äôve seen so\\nfar: the agent architecture.\\nSummary | 133'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 162, 'page_label': '135'}, page_content='CHAPTER 6\\nAgent Architecture\\nBuilding on the architectures described in Chapter 5, this chapter will cover what is\\nperhaps the most important of all current LLM architectures, the agent architecture.\\nFirst, we introduce what makes LLM agents unique, then we show how to build them\\nand how to extend them for common use cases.\\nIn the artificial intelligence field, there is a long history of creating (intelligent) agents,\\nwhich can be most simply defined as ‚Äúsomething that acts, ‚Äù in the words of Stuart\\nRussell and Peter Norvig in their Artificial Intelligence (Pearson, 2020) textbook. The\\nword acts actually carries a little more meaning than meets the eye:\\n‚Ä¢ Acting requires some capacity for deciding what to do.‚Ä¢\\n‚Ä¢ Deciding what to do implies having access to more than one possible course of‚Ä¢\\naction. After all, a decision without options is no decision at all.\\n‚Ä¢ In order to decide, the agent also needs access to information about the external‚Ä¢\\nenvironment (anything outside of the agent itself).\\nSo an agentic LLM application must be one that uses an LLM to pick from one or\\nmore possible courses of action, given some context about the current state of the\\nworld or some desired next state. These attributes are usually implemented by mixing\\ntwo prompting techniques we first met in the Preface:\\nTool calling\\nInclude a list of external functions that the LLM can make use of in your prompt\\n(that is, the actions it can decide to take) and provide instructions on how to\\nformat its choice in the output it generates. Y ou‚Äôll see in a moment what this\\nlooks like in the prompt.\\n135'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 163, 'page_label': '136'}, page_content='Chain-of-thought\\nResearchers have found that LLMs ‚Äúmake better decisions‚Äù when given instruc‚Äê\\ntions to reason about complex problems by breaking them down into granular\\nsteps to be taken in sequence. This is usually done either by adding instructions\\nalong the lines of ‚Äúthink step by step‚Äù or including examples of questions and\\ntheir decomposition into several steps/actions.\\nHere‚Äôs an example prompt using both tool calling and chain-of-thought:\\nTools:\\nsearch: this tool accepts a web search query and returns the top results.\\ncalculator: this tool accepts math expressions and returns their result.\\nIf you want to use tools to arrive at the answer, output the list of tools and\\ninputs in CSV format, with the header row: tool,input.\\nThink step by step; if you need to make multiple tool calls to arrive at the\\nanswer, return only the first one.\\nHow old was the 30th president of the United States when he died?\\ntool,input\\nAnd the output, when run against gpt-3.5-turbo at temperature 0 (to ensure the\\nLLM follows the desired output format, CSV) and newline as the stop sequence\\n(which instructs the LLM to stop producing output when it reaches this character).\\nThis makes the LLM produce a single action (as expected, given the prompt asked for\\nthis):\\nsearch,30th president of the United States\\nThe most recent LLMs and chat models have been fine-tuned to improve their\\nperformance for tool-calling and chain-of-thought applications, removing the need\\nfor adding specific instructions to the prompt:\\nadd example prompt and output for tool-calling model\\nThe Plan-Do Loop\\nWhat makes the agent architecture different from the architectures discussed in\\nChapter 5 is a concept we haven‚Äôt covered yet: the LLM-driven loop.\\nEvery programmer has encountered loops in their code before. By loop, we mean\\nrunning the same code multiple times until a stop condition is hit. The key to the\\nagent architecture is to have an LLM control the stop condition‚Äîthat is, decide when\\nto stop looping.\\n136 | Chapter 6: Agent Architecture'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 164, 'page_label': '137'}, page_content='What we‚Äôll run in this loop will be some variation of the following:\\n‚Ä¢ Planning an action or actions‚Ä¢\\n‚Ä¢ Executing said action(s)‚Ä¢\\nPicking up on the example in the previous section, we‚Äôll next run the search tool with\\nthe input 30th president of the United States, which produces this output:\\nCalvin Coolidge (born John Calvin Coolidge Jr.; /ÀàkuÀêl…™d í/; July 4, 1872 ‚Äì January \\n5, 1933) was an American attorney and politician who served as the 30th president \\nof the United States from 1923 to 1929. John Calvin Coolidge Jr.\\nAnd then we‚Äôll rerun the prompt, with a small addition:\\nTools:\\nsearch: this tool accepts a web search query and returns the top results.\\ncalculator: this tool accepts math expressions and returns their result.\\noutput: this tool ends the interaction. Use it when you have the final answer.\\nIf you want to use tools to arrive at the answer, output the list of tools and \\ninputs in CSV format, with this header row: tool,input\\nThink step by step; if you need to make multiple tool calls to arrive at \\nthe answer, return only the first one.\\nHow old was the 30th president of the United States when he died?\\ntool,input\\nsearch,30th president of the United States\\nsearch: Calvin Coolidge (born John Calvin Coolidge Jr.; /ÀàkuÀêl…™d í/; July 4, 1872 ‚Äì \\nJanuary 5, 1933) was an American attorney and politician who served as the 30th \\npresident of the United States from 1923 to 1929. John Calvin Coolidge Jr.\\ntool,input\\nAnd the output:\\ncalculator,1933 - 1872\\nNotice we added two things:\\n‚Ä¢ An ‚Äúoutput‚Äù tool‚Äîwhich the LLM should use when it has found the final answer,‚Ä¢\\nand which we‚Äô d use as the signal to stop the loop.\\n‚Ä¢ The result of the tool call from the preceding iteration, simply with the name of‚Ä¢\\nthe tool and its (text) output. This is included in order to allow the LLM to move\\non to the next step in the interaction. In other words, we‚Äôre telling the LLM, ‚ÄúHey,\\nwe got the results you asked for, what do you want to do next?‚Äù\\nThe Plan-Do Loop | 137'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 165, 'page_label': '138'}, page_content='Let‚Äôs continue with a third iteration:\\nTools:\\nsearch: this tool accepts a web search query and returns the top results.\\ncalculator: this tool accepts math expressions and returns their result.\\nIf you want to use tools to arrive at the answer, output the list of tools and \\ninputs in CSV format, with this header row: tool,input.\\noutput: this tool ends the interaction. Use it when you have the final answer.\\nThink step by step; if you need to make multiple tool calls to arrive at \\nthe answer, return only the first one.\\nHow old was the 30th president of the United States when he died?\\ntool,input\\nsearch,30th president of the United States\\nsearch: Calvin Coolidge (born John Calvin Coolidge Jr.; /ÀàkuÀêl…™d í/; July 4, 1872 ‚Äì \\nJanuary 5, 1933) was an American attorney and politician who served as the 30th \\npresident of the United States from 1923 to 1929. John Calvin Coolidge Jr.\\ntool,input\\ncalculator,1933-1872\\ncalculator: 61\\ntool, input\\nAnd the output:\\noutput, 61\\nWith the result from the calculator tool, the LLM now has enough information\\nto provide the final answer, so it picked the output tool and chose ‚Äú61‚Äù as the final\\nanswer.\\nThis is what makes the agent architecture so useful‚Äîthe LLM is given the agency\\nto decide. The next step is to arrive at an answer and decide how many steps to\\ntake‚Äîthat is, when to stop.\\nThis architecture, called ReAct, was first proposed by Shunyu Y ao et al. The rest\\nof this chapter explores how to improve the performance of the agent architecture,\\nmotivated by the email assistant example from Chapter 5.\\nBut first, let‚Äôs see what it looks like to implement the basic agent architecture using a\\nchat model and LangGraph.\\n138 | Chapter 6: Agent Architecture'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 166, 'page_label': '139'}, page_content='Building a LangGraph Agent\\nFor this example, we need to install additional dependencies for the search tool we\\nchose to use, DuckDuckGo. To install it for Python:\\nPython\\npip install duckduckgo-search\\nAnd for JS, we also need to install a dependency for the calculator tool:\\nJavaScript\\nnpm i duck-duck-scrape expr-eval\\nWith that complete, let‚Äôs get into the actual code to implement the agent architecture:\\nPython\\nimport ast\\nfrom typing import Annotated, TypedDict\\nfrom langchain_community.tools import DuckDuckGoSearchRun\\nfrom langchain_core.tools import tool\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import START, StateGraph\\nfrom langgraph.graph.message import add_messages\\nfrom langgraph.prebuilt import ToolNode, tools_condition\\n@tool\\ndef calculator(query: str) -> str:\\n    \"\"\"A simple calculator tool. Input should be a mathematical expression.\"\"\"\\n    return ast.literal_eval(query)\\nsearch = DuckDuckGoSearchRun()\\ntools = [search, calculator]\\nmodel = ChatOpenAI(temperature=0.1).bind_tools(tools)\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\ndef model_node(state: State) -> State:\\n    res = model.invoke(state[\"messages\"])\\n    return {\"messages\": res}\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"model\", model_node)\\nbuilder.add_node(\"tools\", ToolNode(tools))\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_conditional_edges(\"model\", tools_condition)\\nbuilder.add_edge(\"tools\", \"model\")\\ngraph = builder.compile()\\nBuilding a LangGraph Agent | 139'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 167, 'page_label': '140'}, page_content='JavaScript\\nimport {\\n  DuckDuckGoSearch\\n} from \"@langchain/community/tools/duckduckgo_search\";\\nimport {\\n  Calculator\\n} from \"@langchain/community/tools/calculator\";\\nimport {\\n  StateGraph,\\n  Annotation,\\n  messagesStateReducer,\\n  START,\\n} from \"@langchain/langgraph\";\\nimport {\\n  ToolNode,\\n  toolsCondition\\n} from \"@langchain/langgraph/prebuilt\";\\nconst search = new DuckDuckGoSearch();\\nconst calculator = new Calculator();\\nconst tools = [search, calculator];\\nconst model = new ChatOpenAI({\\n  temperature: 0.1\\n}).bindTools(tools);\\nconst annotation = Annotation.Root({\\n  messages: Annotation({\\n    reducer: messagesStateReducer,\\n    default: () => []\\n  }),\\n});\\nasync function modelNode(state) {\\n  const res = await model.invoke(state.messages);\\n  return { messages: res };\\n}\\nconst builder = new StateGraph(annotation)\\n  .addNode(\"model\", modelNode)\\n  .addNode(\"tools\", new ToolNode(tools))\\n  .addEdge(START, \"model\")\\n  .addConditionalEdges(\"model\", toolsCondition)\\n  .addEdge(\"tools\", \"model\");\\nconst graph = builder.compile();\\nThe visual representation is shown in Figure 6-1.\\n140 | Chapter 6: Agent Architecture'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 168, 'page_label': '141'}, page_content='Figure 6-1. The agent architecture\\nA few things to notice here:\\n‚Ä¢ We‚Äôre using two tools in this example: a search tool and a calculator tool, but you‚Ä¢\\ncould easily add more or replace the ones we used. In the Python example, you\\nalso see an example of creating a custom tool.\\n‚Ä¢ We‚Äôve used two convenience functions that ship with LangGraph. ToolNode‚Ä¢\\nserves as a node in our graph; it executes the tool calls requested in the latest AI\\nmessage found in the state and returns a ToolMessage with the results of each.\\nToolNode also handles exceptions raised by tools‚Äîusing the error message to\\nbuild a ToolMessage that is then passed to the LLM‚Äîwhich may decide what to\\ndo with the error.\\n‚Ä¢ tools_condition serves as a conditional edge function that looks at the latest‚Ä¢\\nAI message in the state and routes to the tools node if there are any tools to\\nexecute. Otherwise, it ends the graph.\\n‚Ä¢ Finally, notice that this graph loops between the model and tools nodes. That is,‚Ä¢\\nthe model itself is in charge of deciding when to end the computation, which is a\\nkey attribute of the agent architecture. Whenever we code a loop in LangGraph,\\nwe‚Äôll likely want to use a conditional edge, as that allows you to define the stop\\ncondition when the graph should exit the loop and stop executing.\\nNow let‚Äôs see how it does in the previous example:\\nPython\\ninput = {\\n    \"messages\": [\\n        HumanMessage(\"\"\"How old was the 30th president of the United States \\n            when he died?\"\"\")\\n    ]\\n}\\nfor c in graph.stream(input):\\n    print(c)\\nBuilding a LangGraph Agent | 141'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 169, 'page_label': '142'}, page_content='JavaScript\\nconst input = {\\n  messages: [\\n    HumanMessage(`How old was the 30th president of the United States when he \\n      died?`)\\n  ]\\n}\\nfor await (const c of await graph.stream(input)) {\\n  console.log(c)\\n}\\nThe output:\\n{\\n    \"model\": {\\n        \"messages\": AIMessage(\\n            content=\"\",\\n            tool_calls=[\\n                {\\n                    \"name\": \"duckduckgo_search\",\\n                    \"args\": {\\n                        \"query\": \"30th president of the United States age at \\n                            death\"\\n                    },\\n                    \"id\": \"call_ZWRbPmjvo0fYkwyo4HCYUsar\",\\n                    \"type\": \"tool_call\",\\n                }\\n            ],\\n        )\\n    }\\n}\\n{\\n    \"tools\": {\\n        \"messages\": [\\n            ToolMessage(\\n                content=\"Calvin Coolidge (born July 4, 1872, Plymouth, Vermont, \\n                    U.S.‚Äîdied January 5, 1933, Northampton, Massachusetts) was \\n                    the 30th president of the United States (1923-29). Coolidge \\n                    acceded to the presidency after the death in office of \\n                    Warren G. Harding, just as the Harding scandals were coming \\n                    to light....\",\\n                name=\"duckduckgo_search\",\\n                tool_call_id=\"call_ZWRbPmjvo0fYkwyo4HCYUsar\",\\n            )\\n        ]\\n    }\\n}\\n{\\n    \"model\": {\\n        \"messages\": AIMessage(\\n            content=\"Calvin Coolidge, the 30th president of the United States, \\n                died on January 5, 1933, at the age of 60.\",\\n142 | Chapter 6: Agent Architecture'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 170, 'page_label': '143'}, page_content=')\\n    }\\n}\\nWalking through this output:\\n1. First the model node executed and decided to call the duckduckgo_search tool,1.\\nwhich led the conditional edge to route us to the tools node after.\\n2. The ToolNode executed the search tool and got the search results printed above,2.\\nwhich actually contain the answer ‚Äú Age and Y ear of Death . January 5, 1933 (aged\\n60)‚Äù .\\n3. The model tool was called again, this time with the search results as the latest3.\\nmessage, and produced the final answer (with no more tool calls); therefore, the\\nconditional edge ended the graph.\\nNext, let‚Äôs look at a few useful extensions to this basic agent architecture, customizing\\nboth planning and tool calling.\\nAlways Calling a Tool First\\nIn the standard agent architecture, the LLM is always called upon to decide what\\ntool to call next. This arrangement has a clear advantage: it gives the LLM ultimate\\nflexibility to adapt the behavior of the application to each user query that comes in.\\nBut this flexibility comes at a cost: unpredictability. If, for instance, you, the developer\\nof the application, know that the search tool should always be called first, that can\\nactually be beneficial to your application:\\n1. It will reduce overall latency, as it will skip the first LLM call that would generate1.\\nthat request to call the search tool.\\n2. It will prevent the LLM from erroneously deciding it doesn‚Äôt need to call the2.\\nsearch tool for some user queries.\\nOn the other hand, if your application doesn‚Äôt have a clear rule of the kind ‚Äúyou\\nshould always call this tool first, ‚Äù introducing such a constraint would actually make\\nyour application worse.\\nLet‚Äôs see what it looks like to do this:\\nPython\\nimport ast\\nfrom typing import Annotated, TypedDict\\nfrom uuid import uuid4\\nfrom langchain_community.tools import DuckDuckGoSearchRun\\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolCall\\nAlways Calling a Tool First | 143'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 171, 'page_label': '144'}, page_content='from langchain_core.tools import tool\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import START, StateGraph\\nfrom langgraph.graph.message import add_messages\\nfrom langgraph.prebuilt import ToolNode, tools_condition\\n@tool\\ndef calculator(query: str) -> str:\\n    \"\"\"A simple calculator tool. Input should be a mathematical expression.\"\"\"\\n    return ast.literal_eval(query)\\nsearch = DuckDuckGoSearchRun()\\ntools = [search, calculator]\\nmodel = ChatOpenAI(temperature=0.1).bind_tools(tools)\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\ndef model_node(state: State) -> State:\\n    res = model.invoke(state[\"messages\"])\\n    return {\"messages\": res}\\ndef first_model(state: State) -> State:\\n    query = state[\"messages\"][-1].content\\n    search_tool_call = ToolCall(\\n        name=\"duckduckgo_search\", args={\"query\": query}, id=uuid4().hex\\n    )\\n    return {\"messages\": AIMessage(content=\"\", tool_calls=[search_tool_call])}\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"first_model\", first_model)\\nbuilder.add_node(\"model\", model_node)\\nbuilder.add_node(\"tools\", ToolNode(tools))\\nbuilder.add_edge(START, \"first_model\")\\nbuilder.add_edge(\"first_model\", \"tools\")\\nbuilder.add_conditional_edges(\"model\", tools_condition)\\nbuilder.add_edge(\"tools\", \"model\")\\ngraph = builder.compile()\\nJavaScript\\nimport {\\n  DuckDuckGoSearch\\n} from \"@langchain/community/tools/duckduckgo_search\";\\nimport {\\n  Calculator\\n} from \"@langchain/community/tools/calculator\";\\nimport {\\n  AIMessage,\\n} from \"@langchain/core/messages\";\\nimport {\\n144 | Chapter 6: Agent Architecture'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 172, 'page_label': '145'}, page_content='StateGraph,\\n  Annotation,\\n  messagesStateReducer,\\n  START,\\n} from \"@langchain/langgraph\";\\nimport {\\n  ToolNode,\\n  toolsCondition\\n} from \"@langchain/langgraph/prebuilt\";\\nconst search = new DuckDuckGoSearch();\\nconst calculator = new Calculator();\\nconst tools = [search, calculator];\\nconst model = new ChatOpenAI({ temperature: 0.1 }).bindTools(tools);\\nconst annotation = Annotation.Root({\\n  messages: Annotation({ reducer: messagesStateReducer, default: () => [] }),\\n});\\nasync function firstModelNode(state) {\\n  const query = state.messages[state.messages.length - 1].content;\\n  const searchToolCall = {\\n    name: \"duckduckgo_search\",\\n    args: { query },\\n    id: Math.random().toString(),\\n  };\\n  return {\\n    messages: [new AIMessage({ content: \"\", tool_calls: [searchToolCall] })],\\n  };\\n}\\nasync function modelNode(state) {\\n  const res = await model.invoke(state.messages);\\n  return { messages: res };\\n}\\nconst builder = new StateGraph(annotation)\\n  .addNode(\"first_model\", firstModelNode)\\n  .addNode(\"model\", modelNode)\\n  .addNode(\"tools\", new ToolNode(tools))\\n  .addEdge(START, \"first_model\")\\n  .addEdge(\"first_model\", \"tools\")\\n  .addEdge(\"tools\", \"model\")\\n  .addConditionalEdges(\"model\", toolsCondition);\\nconst graph = builder.compile();\\nThe visual representation is shown in Figure 6-2.\\nAlways Calling a Tool First | 145'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 173, 'page_label': '146'}, page_content='Figure 6-2. Modifying the agent architecture to always call a specific tool first\\nNotice the differences compared to the previous section:\\n‚Ä¢ Now, we start all invocations by calling first_model, which doesn‚Äôt call an LLM‚Ä¢\\nat all. It just creates a tool call for the search tool, using the user‚Äôs message\\nverbatim as the search query. The previous architecture would have the LLM\\ngenerate this tool call (or some other response it deemed better).\\n‚Ä¢ After that, we proceed to tools, which is identical to the previous example, and‚Ä¢\\nfrom there we proceed to the agent node as before.\\nNow let‚Äôs see some example output, for the same query as before:\\nPython\\ninput = {\\n    \"messages\": [\\n        HumanMessage(\"\"\"How old was the 30th president of the United States \\n            when he died?\"\"\")\\n    ]\\n}\\nfor c in graph.stream(input):\\nprint(c)\\nJavaScript\\nconst input = {\\n  messages: [\\n    HumanMessage(`How old was the 30th president of the United States when he \\n        died?`)\\n  ]\\n146 | Chapter 6: Agent Architecture'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 174, 'page_label': '147'}, page_content='}\\nfor await (const c of await graph.stream(input)) {\\n  console.log(c)\\n}\\nThe output:\\n{\\n    \"first_model\": {\\n        \"messages\": AIMessage(\\n            content=\"\",\\n            tool_calls=[\\n                {\\n                    \"name\": \"duckduckgo_search\",\\n                    \"args\": {\\n                        \"query\": \"How old was the 30th president of the United \\n                            States when he died?\"\\n                    },\\n                    \"id\": \"9ed4328dcdea4904b1b54487e343a373\",\\n                    \"type\": \"tool_call\",\\n                }\\n            ],\\n        )\\n    }\\n}\\n{\\n    \"tools\": {\\n        \"messages\": [\\n            ToolMessage(\\n                content=\"Calvin Coolidge (born July 4, 1872, Plymouth, Vermont, \\n                    U.S.‚Äîdied January 5, 1933, Northampton, Massachusetts) was \\n                    the 30th president of the United States (1923-29). Coolidge \\n                    acceded to the presidency after the death in office of \\n                    Warren G. Harding, just as the Harding scandals were coming \\n                    to light....\",\\n                name=\"duckduckgo_search\",\\n                tool_call_id=\"9ed4328dcdea4904b1b54487e343a373\",\\n            )\\n        ]\\n    }\\n}\\n{\\n    \"model\": {\\n        \"messages\": AIMessage(\\n            content=\"Calvin Coolidge, the 30th president of the United States, \\n                was born on July 4, 1872, and died on January 5, 1933. To \\n                calculate his age at the time of his death, we can subtract his \\n                birth year from his death year. \\\\n\\\\nAge at death = Death year - \\n                Birth year\\\\nAge at death = 1933 - 1872\\\\nAge at death = 61 \\n                years\\\\n\\\\nCalvin Coolidge was 61 years old when he died.\",\\n        )\\n    }\\n}\\nAlways Calling a Tool First | 147'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 175, 'page_label': '148'}, page_content='This time, we skipped the initial LLM call. We first went to first_model node, which\\ndirectly returned a tool call for the search tool. From there we went to the previous\\nflow‚Äîthat is, we executed the search tool and finally went back to the model node to\\ngenerate the final answer.\\nNext let‚Äôs go over what you can do when you have many tools you want to make\\navailable to the LLM.\\nDealing with Many Tools\\nLLMs are far from perfect, and they currently struggle more when given multiple\\nchoices or excessive information in a prompt. These limitations also extend to the\\nplanning of the next action to take. When given many tools (say, more than 10) the\\nplanning performance (that is, the LLM‚Äôs ability to choose the right tool) starts to\\nsuffer. The solution to this problem is to reduce the number of tools the LLM can\\nchoose from. But what if you do have many tools you want to see used for different\\nuser queries?\\nOne elegant solution is to use a RAG step to preselect the most relevant tools for the\\ncurrent query and then feed the LLM only that subset of tools instead of the entire\\narsenal. This can also help to reduce the cost of calling the LLM (commercial LLMs\\nusually charge based on the length of the prompt and outputs). On the other hand,\\nthis RAG step introduces additional latency to your application, so should only be\\ntaken when you see performance decreasing after adding more tools.\\nLet‚Äôs see how to do this:\\nPython\\nimport ast\\nfrom typing import Annotated, TypedDict\\nfrom langchain_community.tools import DuckDuckGoSearchRun\\nfrom langchain_core.documents import Document\\nfrom langchain_core.messages import HumanMessage\\nfrom langchain_core.tools import tool\\nfrom langchain_core.vectorstores.in_memory import InMemoryVectorStore\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langgraph.graph import START, StateGraph\\nfrom langgraph.graph.message import add_messages\\nfrom langgraph.prebuilt import ToolNode, tools_condition\\n@tool\\ndef calculator(query: str) -> str:\\n    \"\"\"A simple calculator tool. Input should be a mathematical expression.\"\"\"\\n    return ast.literal_eval(query)\\nsearch = DuckDuckGoSearchRun()\\n148 | Chapter 6: Agent Architecture'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 176, 'page_label': '149'}, page_content='tools = [search, calculator]\\nembeddings = OpenAIEmbeddings()\\nmodel = ChatOpenAI(temperature=0.1)\\ntools_retriever = InMemoryVectorStore.from_documents(\\n    [Document(tool.description, metadata={\"name\": tool.name}) for tool in tools],\\n    embeddings,\\n).as_retriever()\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n    selected_tools: list[str]\\ndef model_node(state: State) -> State:\\n    selected_tools = [\\n        tool for tool in tools if tool.name in state[\"selected_tools\"]\\n    ]\\n    res = model.bind_tools(selected_tools).invoke(state[\"messages\"])\\n    return {\"messages\": res}\\ndef select_tools(state: State) -> State:\\n    query = state[\"messages\"][-1].content\\n    tool_docs = tools_retriever.invoke(query)\\n    return {\"selected_tools\": [doc.metadata[\"name\"] for doc in tool_docs]}\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"select_tools\", select_tools)\\nbuilder.add_node(\"model\", model_node)\\nbuilder.add_node(\"tools\", ToolNode(tools))\\nbuilder.add_edge(START, \"select_tools\")\\nbuilder.add_edge(\"select_tools\", \"model\")\\nbuilder.add_conditional_edges(\"model\", tools_condition)\\nbuilder.add_edge(\"tools\", \"model\")\\ngraph = builder.compile()\\nJavaScript\\nimport { DuckDuckGoSearch } from \"@langchain/community/tools/duckduckgo_search\";\\nimport { Calculator } from \"@langchain/community/tools/calculator\";\\nimport { ChatOpenAI } from \"@langchain/openai\";\\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\\nimport { Document } from \"@langchain/core/documents\";\\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\\nimport {\\n  StateGraph,\\n  Annotation,\\n  messagesStateReducer,\\n  START,\\n} from \"@langchain/langgraph\";\\nimport { ToolNode, toolsCondition } from \"@langchain/langgraph/prebuilt\";\\nimport { HumanMessage } from \"@langchain/core/messages\";\\nDealing with Many Tools | 149'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 177, 'page_label': '150'}, page_content='const search = new DuckDuckGoSearch();\\nconst calculator = new Calculator();\\nconst tools = [search, calculator];\\nconst embeddings = new OpenAIEmbeddings();\\nconst model = new ChatOpenAI({ temperature: 0.1 });\\nconst toolsStore = await MemoryVectorStore.fromDocuments(\\n  tools.map(\\n    (tool) =>\\n      new Document({\\n        pageContent: tool.description,\\n        metadata: { name: tool.constructor.name },\\n      })\\n  ),\\n  embeddings\\n);\\nconst toolsRetriever = toolsStore.asRetriever();\\nconst annotation = Annotation.Root({\\n  messages: Annotation({ reducer: messagesStateReducer, default: () => [] }),\\n  selected_tools: Annotation(),\\n});\\nasync function modelNode(state) {\\n  const selectedTools = tools.filter((tool) =>\\n    state.selected_tools.includes(tool.constructor.name)\\n  );\\n  const res = await model.bindTools(selectedTools).invoke(state.messages);\\n  return { messages: res };\\n}\\nasync function selectTools(state) {\\n  const query = state.messages[state.messages.length - 1].content;\\n  const toolDocs = await toolsRetriever.invoke(query as string);\\n  return {\\n    selected_tools: toolDocs.map((doc) => doc.metadata.name),\\n  };\\n}\\nconst builder = new StateGraph(annotation)\\n  .addNode(\"select_tools\", selectTools)\\n  .addNode(\"model\", modelNode)\\n  .addNode(\"tools\", new ToolNode(tools))\\n  .addEdge(START, \"select_tools\")\\n  .addEdge(\"select_tools\", \"model\")\\n  .addConditionalEdges(\"model\", toolsCondition)\\n  .addEdge(\"tools\", \"model\");\\nY ou can see the visual representation in Figure 6-3.\\n150 | Chapter 6: Agent Architecture'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 178, 'page_label': '151'}, page_content='Figure 6-3. Modifying the agent architecture to deal with many tools\\nThis is very similar to the regular agent architecture. The only dif‚Äê\\nference is that we stop by the select_tools node before entering\\nthe actual agent loop. After that, it works just as the regular agent\\narchitecture we‚Äôve seen before.\\nNow let‚Äôs see some example output for the same query as before:\\nPython\\ninput = {\\n  \"messages\": [\\n    HumanMessage(\"\"\"How old was the 30th president of the United States when \\n        he died?\"\"\")\\n  ]\\n}\\nfor c in graph.stream(input):\\nprint(c)\\nJavaScript\\nconst input = {\\n  messages: [\\n    HumanMessage(`How old was the 30th president of the United States when he \\n      died?`)\\n  ]\\n}\\nfor await (const c of await graph.stream(input)) {\\n  console.log(c)\\n}\\nDealing with Many Tools | 151'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 179, 'page_label': '152'}, page_content='The output:\\n{\\n    \"select_tools\": {\\n        \"selected_tools\\': [\\'duckduckgo_search\\', \\'calculator\\']\\n    }\\n}\\n{\\n    \"model\": {\\n        \"messages\": AIMessage(\\n            content=\"\",\\n            tool_calls=[\\n                {\\n                    \"name\": \"duckduckgo_search\",\\n                    \"args\": {\\n                        \"query\": \"30th president of the United States\"\\n                    },\\n                    \"id\": \"9ed4328dcdea4904b1b54487e343a373\",\\n                    \"type\": \"tool_call\",\\n                }\\n            ],\\n        )\\n    }\\n}\\n{\\n    \"tools\": {\\n        \"messages\": [\\n            ToolMessage(\\n                content=\"Calvin Coolidge (born July 4, 1872, Plymouth, Vermont, \\n                    U.S.‚Äîdied January 5, 1933, Northampton, Massachusetts) was \\n                    the 30th president of the United States (1923-29). Coolidge \\n                    acceded to the presidency after the death in office of \\n                    Warren G. Harding, just as the Harding scandals were coming \\n                    to light....\",\\n                name=\"duckduckgo_search\",\\n                tool_call_id=\"9ed4328dcdea4904b1b54487e343a373\",\\n            )\\n        ]\\n    }\\n}\\n{\\n    \"model\": {\\n        \"messages\": AIMessage(\\n            content=\"Calvin Coolidge, the 30th president of the United States, \\n                was born on July 4, 1872, and died on January 5, 1933. To \\n                calculate his age at the time of his death, we can subtract his \\n                birth year from his death year. \\\\n\\\\nAge at death = Death year - \\n                Birth year\\\\nAge at death = 1933 - 1872\\\\nAge at death = 61 \\n                years\\\\n\\\\nCalvin Coolidge was 61 years old when he died.\",\\n        )\\n    }\\n}\\n152 | Chapter 6: Agent Architecture'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 180, 'page_label': '153'}, page_content='Notice how the first thing we did was query the retriever to get the most relevant\\ntools for the current user query. Then, we proceeded to the regular agent architecture.\\nSummary\\nThis chapter introduced the concept of agency and discussed what it takes to make\\nan LLM application agentic: giving the LLM the ability to decide between multiple\\noptions by using external information.\\nWe walked through the standard agent architecture built with LangGraph and looked\\nat two useful extensions: how to always call a specific tool first and how to deal with\\nmany tools.\\nChapter 7 looks at additional extensions to the agent architecture.\\nSummary | 153'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 182, 'page_label': '155'}, page_content='CHAPTER 7\\nAgents II\\nChapter 6  introduced the agent architecture, the most powerful of the LLM archi‚Äê\\ntectures we have seen up until now. It is hard to overstate the potential of this\\ncombination of chain-of-thought prompting, tool use, and looping.\\nThis chapter discusses two extensions to the agent architecture that improve perfor‚Äê\\nmance for some use cases:\\nReflection\\nTaking another page out of the repertoire of human thought patterns, this is\\nabout giving your LLM app the opportunity to analyze its past output and\\nchoices, together with the ability to remember reflections from past iterations.\\nMulti-agent\\nMuch the same way as a team can accomplish more than a single person, there\\nare problems that can be best tackled by teams of LLM agents.\\nLet‚Äôs start with reflection.\\nReflection\\nOne prompting technique we haven‚Äôt covered yet is reflection (also known as self-\\ncritique). Reflection is the creation of a loop between a creator prompt and a reviser\\nprompt. This mirrors the creation process for many human-created artifacts, such as\\nthis chapter you‚Äôre reading now, which is the result of a back and forth between the\\nauthors, reviewers, and editor until all are happy with the final product.\\nAs with many of the prompting techniques we have seen so far, reflection can be\\ncombined with other techniques, such as chain-of-thought and tool calling. In this\\nsection, we‚Äôll look at reflection in isolation.\\n155'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 183, 'page_label': '156'}, page_content='A parallel can be drawn to the modes of human thinking known as System 1 (reactive\\nor instinctive) and System 2 (methodical and reflective), first introduced  by Daniel\\nKahneman in the book Thinking, Fast and Slow  (Farrar, Straus and Giroux, 2011).\\nWhen applied correctly, self-critique can help LLM applications get closer to some‚Äê\\nthing that resembles System 2 behavior (Figure 7-1).\\nFigure 7-1. System 1 and System 2 thinking\\nWe‚Äôll implement reflection as a graph with two nodes: generate and reflect. This\\ngraph will be tasked with writing three-paragraph essays, with the generate node\\nwriting or revising drafts of the essay, and reflect writing a critique to inform the next\\nrevision. We‚Äôll run the loop a fixed number of times, but a variation on this technique\\nwould be to have the reflect node decide when to finish. Let‚Äôs see what it looks like:\\nPython\\nfrom typing import Annotated, TypedDict\\nfrom langchain_core.messages import (\\n    AIMessage,\\n    BaseMessage,\\n    HumanMessage,\\n    SystemMessage,\\n)\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import END, START, StateGraph\\nfrom langgraph.graph.message import add_messages\\nmodel = ChatOpenAI()\\n156 | Chapter 7: Agents II'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 184, 'page_label': '157'}, page_content='class State(TypedDict):\\n    messages: Annotated[list[BaseMessage], add_messages]\\ngenerate_prompt = SystemMessage(\\n    \"\"\"You are an essay assistant tasked with writing excellent 3-paragraph \\n        essays.\"\"\"\\n    \"Generate the best essay possible for the user\\'s request.\"\\n    \"\"\"If the user provides critique, respond with a revised version of your \\n        previous attempts.\"\"\"\\n)\\ndef generate(state: State) -> State:\\n    answer = model.invoke([generate_prompt] + state[\"messages\"])\\n    return {\"messages\": [answer]}\\nreflection_prompt = SystemMessage(\\n    \"\"\"You are a teacher grading an essay submission. Generate critique and \\n        recommendations for the user\\'s submission.\"\"\"\\n    \"\"\"Provide detailed recommendations, including requests for length, depth, \\n        style, etc.\"\"\"\\n)\\ndef reflect(state: State) -> State:\\n    # Invert the messages to get the LLM to reflect on its own output\\n    cls_map = {AIMessage: HumanMessage, HumanMessage: AIMessage}\\n    # First message is the original user request. \\n    # We hold it the same for all nodes\\n    translated = [reflection_prompt, state[\"messages\"][0]] + [\\n        cls_map[msg.__class__](content=msg.content) \\n            for msg in state[\"messages\"][1:]\\n    ]\\n    answer = model.invoke(translated)\\n    # We treat the output of this as human feedback for the generator\\n    return {\"messages\": [HumanMessage(content=answer.content)]}\\ndef should_continue(state: State):\\n    if len(state[\"messages\"]) > 6:\\n        # End after 3 iterations, each with 2 messages\\n        return END\\n    else:\\n        return \"reflect\"\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"generate\", generate)\\nbuilder.add_node(\"reflect\", reflect)\\nbuilder.add_edge(START, \"generate\")\\nbuilder.add_conditional_edges(\"generate\", should_continue)\\nbuilder.add_edge(\"reflect\", \"generate\")\\ngraph = builder.compile()\\nReflection | 157'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 185, 'page_label': '158'}, page_content='JavaScript\\nimport {\\n  AIMessage,\\n  BaseMessage,\\n  SystemMessage,\\n  HumanMessage,\\n} from \"@langchain/core/messages\";\\nimport { ChatOpenAI } from \"@langchain/openai\";\\nimport {\\n  StateGraph,\\n  Annotation,\\n  messagesStateReducer,\\n  START,\\n  END,\\n} from \"@langchain/langgraph\";\\nconst model = new ChatOpenAI();\\nconst annotation = Annotation.Root({\\n  messages: Annotation({ reducer: messagesStateReducer, default: () => [] }),\\n});\\n// fix multiline string\\nconst generatePrompt = new SystemMessage(\\n  `You are an essay assistant tasked with writing excellent 3-paragraph essays.\\n  Generate the best essay possible for the user\\'s request.\\n  If the user provides critique, respond with a revised version of your \\n    previous attempts.`\\n);\\nasync function generate(state) {\\n  const answer = await model.invoke([generatePrompt, ...state.messages]);\\n  return { messages: [answer] };\\n}\\nconst reflectionPrompt = new SystemMessage(\\n  `You are a teacher grading an essay submission. Generate critique and \\n    recommendations for the user\\'s submission.\\n  Provide detailed recommendations, including requests for length, depth, \\n    style, etc.`\\n);\\nasync function reflect(state) {\\n  // Invert the messages to get the LLM to reflect on its own output\\n  const clsMap: { [key: string]: new (content: string) => BaseMessage } = {\\n    ai: HumanMessage,\\n    human: AIMessage,\\n  };\\n  // First message is the original user request. \\n  // We hold it the same for all nodes\\n  const translated = [\\n    reflectionPrompt,\\n158 | Chapter 7: Agents II'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 186, 'page_label': '159'}, page_content='state.messages[0],\\n    ...state.messages\\n      .slice(1)\\n      .map((msg) => new clsMap[msg._getType()](msg.content as string)),\\n  ];\\n  const answer = await model.invoke(translated);\\n  // We treat the output of this as human feedback for the generator\\n  return { messages: [new HumanMessage({ content: answer.content })] };\\n}\\nfunction shouldContinue(state) {\\n  if (state.messages.length > 6) {\\n    // End after 3 iterations, each with 2 messages\\n    return END;\\n  } else {\\n    return \"reflect\";\\n  }\\n}\\nconst builder = new StateGraph(annotation)\\n  .addNode(\"generate\", generate)\\n  .addNode(\"reflect\", reflect)\\n  .addEdge(START, \"generate\")\\n  .addConditionalEdges(\"generate\", shouldContinue)\\n  .addEdge(\"reflect\", \"generate\");\\nconst graph = builder.compile();\\nThe visual representation of the graph is shown in Figure 7-2.\\nFigure 7-2. The reflection architecture\\nNotice how the reflect node tricks the LLM into thinking it is critiquing essays\\nwritten by the user. And in tandem, the generate node is made to think that the\\ncritique comes from the user. This subterfuge is required because dialogue-tuned\\nLLMs are trained on pairs of human-AI messages, so a sequence of many messages\\nfrom the same participant would result in poor performance.\\nReflection | 159'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 187, 'page_label': '160'}, page_content='One more thing to note: you might, at first glance, expect the end to come after\\na revise step, but in this architecture we have a fixed number of iterations of the\\ngenerate-reflect loop; therefore we terminate after generate (so that the last set\\nof revisions requested are dealt with). A variation on this architecture would instead\\nhave the reflect step make the decision to end the process (once it had no more\\ncomments).\\nLet‚Äôs see what one of the critiques looks like:\\n{\\n    \\'messages\\': [\\n        HumanMessage(content=\\'Your essay on the topicality of \"The Little Prince\" \\n            and its message in modern life is well-written and insightful. You \\n            have effectively highlighted the enduring relevance of the book\\\\\\'s \\n            themes and its importance in today\\\\\\'s society. However, there are a \\n            few areas where you could enhance your essay:\\\\n\\\\n1. **Depth**: \\n            While you touch upon the themes of cherishing simple joys, \\n            nurturing connections, and understanding human relationships, \\n            consider delving deeper into each of these themes. Provide specific \\n            examples from the book to support your points and explore how these \\n            themes manifest in contemporary life.\\\\n\\\\n2. **Analysis**: Consider \\n            analyzing how the book\\\\\\'s messages can be applied to current \\n            societal issues or personal experiences. For instance, you could \\n            discuss how the Little Prince\\\\\\'s perspective on materialism relates \\n            to consumer culture or explore how his approach to relationships \\n            can inform interpersonal dynamics in the digital age.\\\\n\\\\n3. \\n            **Length**: Expand on your ideas by adding more examples, \\n            discussing counterarguments, or exploring the cultural impact of \\n            \"The Little Prince\" in different parts of the world. This will \\n            enrich the depth of your analysis and provide a more comprehensive \\n            understanding of the book\\\\\\'s relevance.\\\\n\\\\n4. **Style**: Your essay \\n            is clear and well-structured. To enhance the engagement of your \\n            readers, consider incorporating quotes from the book to illustrate \\n            key points or including anecdotes to personalize your analysis.\\n            \\\\n\\\\n5. **Conclusion**: Conclude your essay by summarizing the \\n            enduring significance of \"The Little Prince\" and how its messages \\n            can inspire positive change in modern society. Reflect on the \\n            broader implications of the book\\\\\\'s themes and leave the reader \\n            with a lasting impression.\\\\n\\\\nBy expanding on your analysis, \\n            incorporating more examples, and deepening your exploration of the \\n            book\\\\\\'s messages, you can create a more comprehensive and \\n            compelling essay on the topicality of \"The Little Prince\" in modern \\n            life. Well done on your thoughtful analysis, and keep up the good \\n            work!\\', id=\\'70c22b1d-ec96-4dc3-9fd0-d2c6463f9e2c\\'),\\n    ],\\n}\\n160 | Chapter 7: Agents II'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 188, 'page_label': '161'}, page_content='And the final output:\\n{\\n    \\'messages\\': [\\n        AIMessage(content=\\'\"The Little Prince\" by Antoine de Saint-Exup√©ry \\n            stands as a timeless masterpiece that continues to offer profound \\n            insights into human relationships and values, resonating with \\n            readers across generations. The narrative of the Little Prince\\\\\\'s \\n            travels and encounters with a myriad of characters serves as a rich \\n            tapestry of allegorical representations, ....\\', response_metadata=\\n            {\\'token_usage\\': {\\'completion_tokens\\': 420, \\'prompt_tokens\\': 2501, \\n            \\'total_tokens\\': 2921}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\n            \\'system_fingerprint\\': None, \\'finish_reason\\': \\'stop\\', \\'logprobs\\': \\n            None}, id=\\'run-2e8f9f13-f625-4820-9c8b-b64e1c23daa2-0\\', \\n            usage_metadata={\\'input_tokens\\': 2501, \\'output_tokens\\': 420, \\n            \\'total_tokens\\': 2921}),\\n    ],\\n}\\nThis simple type of reflection can sometimes improve performance by giving the\\nLLM multiple attempts at refining its output and by letting the reflection node adopt\\na different persona while critiquing the output.\\nThere are several possible variations of this architecture. For one, we could combine\\nthe reflection step with the agent architecture of Chapter 6 , adding it as the last\\nnode right before sending output to the user. This would make the critique appear\\nto come from the user, and give the application a chance to improve its final output\\nwithout direct user intervention. Obviously this approach would come at the expense\\nof higher latency.\\nIn certain use cases, it could be helpful to ground the critique with external informa‚Äê\\ntion. For instance, if you were writing a code-generation agent, you could have a step\\nbefore reflect that would run the code through a linter or compiler and report any\\nerrors as input to reflect.\\nWhenever this approach is possible, we strongly recommend giv‚Äê\\ning it a try, as it‚Äôs likely to increase the quality of the final output.\\nSubgraphs in LangGraph\\nBefore we dive into multi-agent architectures, let‚Äôs look at an important technical\\nconcept in LangGraph that enables it. Subgraphs are graphs that are used as part of\\nanother graph. Here are some use cases for subgraphs:\\nSubgraphs in LangGraph | 161'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 189, 'page_label': '162'}, page_content='‚Ä¢ Building multi-agent systems (discussed in the next section).‚Ä¢\\n‚Ä¢ When you want to reuse a set of nodes in multiple graphs, you can define them‚Ä¢\\nonce in a subgraph and then use them in multiple parent graphs.\\n‚Ä¢ When you want different teams to work on different parts of the graph inde‚Äê‚Ä¢\\npendently, you can define each part as a subgraph, and as long as the subgraph\\ninterface (the input and output schemas) is respected, the parent graph can be\\nbuilt without knowing any details of the subgraph.\\nThere are two ways to add subgraph nodes to a parent graph:\\nAdd a node that calls the subgraph directly\\nThis is useful when the parent graph and the subgraph share state keys, and you\\ndon‚Äôt need to transform state on the way in or out.\\nAdd a node with a function that invokes the subgraph\\nThis is useful when the parent graph and the subgraph have different state\\nschemas, and you need to transform state before or after calling the subgraph.\\nLet‚Äôs look at each in turn.\\nCalling a Subgraph Directly\\nThe simplest way to create subgraph nodes is to attach a subgraph directly as a node.\\nWhen doing so, it is important that the parent graph and the subgraph share state\\nkeys, because those shared keys will be used to communicate. (If your graph and\\nsubgraph do not share any keys, see the next section.)\\nIf you pass extra keys to the subgraph node (that is, in addition\\nto the shared keys), they will be ignored by the subgraph node.\\nSimilarly, if you return extra keys from the subgraph, they will be\\nignored by the parent graph.\\nLet‚Äôs see what it looks like in action:\\nPython\\nfrom langgraph.graph import START, StateGraph\\nfrom typing import TypedDict\\nclass State(TypedDict):\\n    foo: str # this key is shared with the subgraph\\nclass SubgraphState(TypedDict):\\n    foo: str # this key is shared with the parent graph\\n    bar: str\\n162 | Chapter 7: Agents II'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 190, 'page_label': '163'}, page_content='# Define subgraph\\ndef subgraph_node(state: SubgraphState):\\n    # note that this subgraph node can communicate with the parent graph \\n    # via the shared \"foo\" key\\n    return {\"foo\": state[\"foo\"] + \"bar\"}\\nsubgraph_builder = StateGraph(SubgraphState)\\nsubgraph_builder.add_node(subgraph_node)\\n...\\nsubgraph = subgraph_builder.compile()\\n# Define parent graph\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"subgraph\", subgraph)\\n...\\ngraph = builder.compile()\\nJavaScript\\nimport { StateGraph, Annotation, START } from \"@langchain/langgraph\";\\nconst StateAnnotation = Annotation.Root({\\n  foo: Annotation(),\\n});\\nconst SubgraphStateAnnotation = Annotation.Root({\\n  // note that this key is shared with the parent graph state\\n  foo: Annotation(), \\n  bar: Annotation(),\\n});\\n// Define subgraph\\nconst subgraphNode = async (state) => {\\n  // note that this subgraph node can communicate with\\n  // the parent graph via the shared \"foo\" key\\n  return { foo: state.foo + \"bar\" };\\n};\\nconst subgraph = new StateGraph(SubgraphStateAnnotation)\\n  .addNode(\"subgraph\", subgraphNode);\\n  ...\\n  .compile();\\n// Define parent graph\\nconst parentGraph = new StateGraph(StateAnnotation)\\n  .addNode(\"subgraph\", subgraph)\\n  .addEdge(START, \"subgraph\")\\n  // Additional parent graph setup would go here\\n  .compile();\\nSubgraphs in LangGraph | 163'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 191, 'page_label': '164'}, page_content='Calling a Subgraph with a Function\\nY ou might want to define a subgraph with a completely different schema. In that case,\\nyou can create a node with a function that invokes the subgraph. This function will\\nneed to transform the input (parent) state to the subgraph state before invoking the\\nsubgraph and transform the results back to the parent state before returning the state\\nupdate from the node.\\nLet‚Äôs see what it looks like:\\nPython\\nclass State(TypedDict):\\n    foo: str\\nclass SubgraphState(TypedDict):\\n    # none of these keys are shared with the parent graph state\\n    bar: str\\n    baz: str\\n# Define subgraph\\ndef subgraph_node(state: SubgraphState):\\n    return {\"bar\": state[\"bar\"] + \"baz\"}\\nsubgraph_builder = StateGraph(SubgraphState)\\nsubgraph_builder.add_node(subgraph_node)\\n...\\nsubgraph = subgraph_builder.compile()\\n# Define parent graph\\ndef node(state: State):\\n    # transform the state to the subgraph state\\n    response = subgraph.invoke({\"bar\": state[\"foo\"]})\\n    # transform response back to the parent state\\n    return {\"foo\": response[\"bar\"]}\\nbuilder = StateGraph(State)\\n# note that we are using `node` function instead of a compiled subgraph\\nbuilder.add_node(node)\\n...\\ngraph = builder.compile()\\nJavaScript\\nimport { StateGraph, START, Annotation } from \"@langchain/langgraph\";\\nconst StateAnnotation = Annotation.Root({\\n  foo: Annotation(),\\n});\\nconst SubgraphStateAnnotation = Annotation.Root({\\n  // note that none of these keys are shared with the parent graph state\\n164 | Chapter 7: Agents II'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 192, 'page_label': '165'}, page_content='bar: Annotation(),\\n  baz: Annotation(),\\n});\\n// Define subgraph\\nconst subgraphNode = async (state) => {\\n  return { bar: state.bar + \"baz\" };\\n};\\nconst subgraph = new StateGraph(SubgraphStateAnnotation)\\n  .addNode(\"subgraph\", subgraphNode);\\n  ...\\n  .compile();\\n// Define parent graph\\nconst subgraphWrapperNode = async (state) => {\\n  // transform the state to the subgraph state\\n  const response = await subgraph.invoke({\\n    bar: state.foo,\\n  });\\n  // transform response back to the parent state\\n  return {\\n    foo: response.bar,\\n  };\\n}\\nconst parentGraph = new StateGraph(StateAnnotation)\\n  .addNode(\"subgraph\", subgraphWrapperNode)\\n  .addEdge(START, \"subgraph\")\\n  // Additional parent graph setup would go here\\n  .compile();\\nNow that we know how to use subgraphs, let‚Äôs take a look at one of the big use cases\\nfor them: multi-agent architectures.\\nMulti-Agent Architectures\\nAs LLM agents grow in size, scope, or complexity, several issues can show up and\\nimpact their performance, such as the following:\\n‚Ä¢ The agent is given too many tools to choose from and makes poor decisions‚Ä¢\\nabout which tool to call next ( Chapter 6  discussed some approaches to this\\nproblem).\\n‚Ä¢ The context grows too complex for a single agent to keep track of; that is, the‚Ä¢\\nsize of the prompts and the number of things they mention grows beyond the\\ncapability of the model you‚Äôre using.\\n‚Ä¢ Y ou want to use a specialized subsystem for a particular area, for instance,‚Ä¢\\nplanning, research, solving math problems, and so on.\\nMulti-Agent Architectures | 165'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 193, 'page_label': '166'}, page_content='To tackle these problems, you might consider breaking your application into multiple\\nsmaller, independent agents and composing them into a multi-agent system. These\\nindependent agents can be as simple as a prompt and an LLM call or as complex as a\\nReAct agent (introduced in Chapter 6). Figure 7-3 illustrates several ways to connect\\nagents in a multi-agent system.\\nFigure 7-3. Multiple strategies for coordinating multiple agents\\nLet‚Äôs look at Figure 7-3 in more detail:\\nNetwork\\nEach agent can communicate with every other agent. Any agent can decide which\\nother agent is to be executed next.\\nSupervisor\\nEach agent communicates with a single agent, called the supervisor. The supervi‚Äê\\nsor agent makes decisions on which agent (or agents) should be called next. A\\nspecial case of this architecture implements the supervisor agent as an LLM call\\nwith tools, as covered in Chapter 6.\\nHierarchical\\nYou can define a multi-agent system with a supervisor of supervisors. This is a\\ngeneralization of the supervisor architecture and allows for more complex control\\nflows.\\nCustom multi-agent workflow\\nEach agent communicates with only a subset of agents. Parts of the flow are\\ndeterministic, and only select agents can decide which other agents to call next.\\n166 | Chapter 7: Agents II'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 194, 'page_label': '167'}, page_content='The next section dives deeper into the supervisor architecture, which we think has a\\ngood balance of capability and ease of use.\\nSupervisor Architecture\\nIn this architecture, we add each agent to the graph as a node and also add a super‚Äê\\nvisor node, which decides which agents should be called next. We use conditional\\nedges to route execution to the appropriate agent node based on the supervisor‚Äôs\\ndecision. Refer back to Chapter 5 for an introduction to LangGraph, which goes over\\nthe concepts of nodes, edges, and more.\\nLet‚Äôs first see what the supervisor node looks like:\\nPython\\nfrom typing import Literal\\nfrom langchain_openai import ChatOpenAI\\nfrom pydantic import BaseModel\\nclass SupervisorDecision(BaseModel):\\n    next: Literal[\"researcher\", \"coder\", \"FINISH\"]\\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\\nmodel = model.with_structured_output(SupervisorDecision)\\nagents = [\"researcher\", \"coder\"]\\nsystem_prompt_part_1 = f\"\"\"You are a supervisor tasked with managing a \\nconversation between the following workers: {agents}. Given the following user \\nrequest, respond with the worker to act next. Each worker will perform a\\ntask and respond with their results and status. When finished,\\nrespond with FINISH.\"\"\"\\nsystem_prompt_part_2 = f\"\"\"Given the conversation above, who should act next? Or \\n    should we FINISH? Select one of: {\\', \\'.join(agents)}, FINISH\"\"\"\\ndef supervisor(state):\\n    messages = [\\n        (\"system\", system_prompt_part_1),\\n        *state[\"messages\"],\\n        (\"system\", system_prompt_part_2)\\n    ]\\n    return model.invoke(messages)\\nJavaScript\\nimport { ChatOpenAI } from \\'langchain-openai\\';\\nimport { z } from \\'zod\\';\\nconst SupervisorDecision = z.object({\\n  next: z.enum([\\'researcher\\', \\'coder\\', \\'FINISH\\']),\\n});\\nMulti-Agent Architectures | 167'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 195, 'page_label': '168'}, page_content='const model = new ChatOpenAI({ model: \\'gpt-4o\\', temperature: 0 });\\nconst modelWithStructuredOutput = model.withStructuredOutput(SupervisorDecision);\\nconst agents = [\\'researcher\\', \\'coder\\'];\\nconst systemPromptPart1 = `You are a supervisor tasked with managing a \\n  conversation between the following workers: ${agents.join(\\', \\')}. Given the \\n  following user request, respond with the worker to act next. Each worker \\n  will perform a task and respond with their results and status. When \\n  finished, respond with FINISH.`;\\nconst systemPromptPart2 = `Given the conversation above, who should act next? Or \\n  should we FINISH? Select one of: ${agents.join(\\', \\')}, FINISH`;\\nconst supervisor = async (state) => {\\n  const messages = [\\n    { role: \\'system\\', content: systemPromptPart1 },\\n    ...state.messages,\\n    { role: \\'system\\', content: systemPromptPart2 }\\n  ];\\n  return await modelWithStructuredOutput.invoke({ messages });\\n};\\nThe code in the prompt requires the names of your subagents to\\nbe self-explanatory and distinct. For instance, if they were simply\\ncalled agent_1 and agent_2, the LLM would have no information\\nto decide which one is appropriate for each task. If needed, you\\ncould modify the prompt to add a description of each agent, which\\ncould help the LLM in picking an agent for each query.\\nNow let‚Äôs see how to integrate this supervisor node into a larger graph that includes\\ntwo other subagents, which we will call researcher and coder. Our overall goal with\\nthis graph is to handle queries that can be answered either by the researcher by\\nitself or the coder by itself, or even both of them in succession. This example doesn‚Äôt\\ninclude implementations for either the researcher or coder‚Äîthe key idea is they\\ncould be any other LangGraph graph or node:\\nPython\\nfrom typing import Literal\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import StateGraph, MessagesState, START\\nmodel = ChatOpenAI()\\nclass AgentState(BaseModel):\\n    next: Literal[\"researcher\", \"coder\", \"FINISH\"]\\n168 | Chapter 7: Agents II'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 196, 'page_label': '169'}, page_content='def researcher(state: AgentState):\\n    response = model.invoke(...)\\n    return {\"messages\": [response]}\\ndef coder(state: AgentState):\\n    response = model.invoke(...)\\n    return {\"messages\": [response]}\\nbuilder = StateGraph(AgentState)\\nbuilder.add_node(supervisor)\\nbuilder.add_node(researcher)\\nbuilder.add_node(coder)\\nbuilder.add_edge(START, \"supervisor\")\\n# route to one of the agents or exit based on the supervisor\\'s decision\\nbuilder.add_conditional_edges(\"supervisor\", lambda state: state[\"next\"])\\nbuilder.add_edge(\"researcher\", \"supervisor\")\\nbuilder.add_edge(\"coder\", \"supervisor\")\\nsupervisor = builder.compile()\\nJavaScript\\nimport {\\n  StateGraph,\\n  Annotation,\\n  MessagesAnnotation,\\n  START,\\n  END,\\n} from \"@langchain/langgraph\";\\nimport { ChatOpenAI } from \"@langchain/openai\";\\nconst model = new ChatOpenAI({\\n  model: \"gpt-4o\",\\n});\\nconst StateAnnotation = Annotation.Root({\\n  ...MessagesAnnotation.spec,\\n  next: Annotation(),\\n});\\nconst researcher = async (state) => {\\n  const response = await model.invoke(...);\\n  return { messages: [response] };\\n};\\nconst coder = async (state) => {\\n  const response = await model.invoke(...);\\n  return { messages: [response] };\\n};\\nconst graph = new StateGraph(StateAnnotation)\\n  .addNode(\"supervisor\", supervisor)\\nMulti-Agent Architectures | 169'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 197, 'page_label': '170'}, page_content='.addNode(\"researcher\", researcher)\\n  .addNode(\"coder\", coder)\\n  .addEdge(START, \"supervisor\")\\n  // route to one of the agents or exit based on the supervisor\\'s decision\\n  .addConditionalEdges(\"supervisor\", async (state) => \\n    state.next === \\'FINISH\\' ? END : state.next)\\n  .addEdge(\"researcher\", \"supervisor\")\\n  .addEdge(\"coder\", \"supervisor\")\\n  .compile();\\nA few things to notice: In this example, both subagents (researcher and coder) can see\\neach other‚Äôs work, as all progress is recorded in the messages list. This isn‚Äôt the only way\\nto organize this. Each of the subagents could be more complex. For instance, a subagent\\ncould be its own graph that maintains internal state and only outputs a summary of the\\nwork it did.\\nAfter each agent executes, we route back to the supervisor node, which decides if\\nthere is more work to be done and which agent to delegate that to if so. This routing\\nisn‚Äôt a hard requirement for this architecture; we could have each subagent make a\\ndecision as to whether its output should be returned directly to the user. To do that,\\nwe‚Äô d replace the hard edge between, say, researcher and supervisor, with a conditional\\nedge (which would read some state key updated by researcher).\\nSummary\\nThis chapter covered two important extensions to the agent architecture: reflection\\nand multi-agent architectures. The chapter also looked at how to work with sub‚Äê\\ngraphs in LangGraph, which are a key building block for multi-agent systems.\\nThese extensions add more power to the LLM agent architecture, but they shouldn‚Äôt\\nbe the first thing you reach for when creating a new agent. The best place to start is\\nusually the straightforward architecture we discussed in Chapter 6.\\nChapter 8  returns to the trade-off between reliability and agency, which is the key\\ndesign decision when building LLM apps today. This is especially important when\\nusing the agent or multi-agent architectures, as their power comes at the expense\\nof reliability if left unchecked. After diving deeper into why this trade-off exists,\\nChapter 8 will cover the most important techniques at your disposal to navigate that\\ndecision, and ultimately improve your LLM applications and agents.\\n170 | Chapter 7: Agents II'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 198, 'page_label': '171'}, page_content='CHAPTER 8\\nPatterns to Make the Most of LLMs\\nLLMs today have some major limitations, but that doesn‚Äôt mean your dream LLM app\\nis impossible to build. The experience that you design for users of your application\\nneeds to work around, and ideally with, the limitations.\\nChapter 5 touched on the key trade-off we face when building LLM apps: the trade-\\noff between agency (the LLM‚Äôs capacity to act autonomously) and reliability (the\\ndegree to which we can trust its outputs). Intuitively, any LLM application will be\\nmore useful to us if it takes more actions without our involvement, but if we let\\nagency go too far, the application will inevitably do things we wish it hadn‚Äôt.\\nFigure 8-1 illustrates this trade-off.\\nFigure 8-1. The agency-reliability trade-off\\n171'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 199, 'page_label': '172'}, page_content='1 In finance, the efficient frontier in portfolio optimization; in economics, a production-possibility frontier; in\\nengineering, the Pareto front.\\nTo borrow a concept from other fields, 1 we can visualize the trade-off as a frontier‚Äî\\nall points on the frontier‚Äôs curved line are optimal LLM architectures for some appli‚Äê\\ncation, marking different choices between agency and reliability. (Refer to Chapter 5\\nfor an overview of different LLM application architectures.) As an example, notice\\nhow the chain architecture has relatively low agency but higher reliability, whereas\\nthe Agent architecture has higher agency at the expense of lower reliability.\\nLet‚Äôs briefly touch on a number of additional (but still important) objectives that you\\nmight want your LLM application to have. Each LLM app will be designed for a\\ndifferent mix of one or more of these objectives:\\nLatency\\nMinimize time to get final answer\\nAutonomy\\nMinimize interruptions for human input\\nVariance\\nMinimize variation between invocations\\nThis is not meant as an exhaustive list of all possible objectives, but rather as illus‚Äê\\ntrative of the trade-offs you face when building your application. Each objective\\nis somewhat at odds with all the others (for instance, the easiest path to higher\\nreliability requires either higher latency or lower autonomy). Each objective would\\nnullify the others if given full weight (for instance, the minimal latency app is the one\\nthat does nothing at all). Figure 8-2 illustrates this concept.\\nFigure 8-2. Shifting the frontier, or more agency, for the same reliability\\n172 | Chapter 8: Patterns to Make the Most of LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 200, 'page_label': '173'}, page_content='What we really want as application developers is to shift the frontier outward. For\\nthe same level of reliability, we‚Äô d like to achieve higher agency; and for the same level\\nof agency, we‚Äô d like to achieve higher reliability. This chapter covers a number of\\ntechniques you can use to achieve this:\\nStreaming/intermediate output\\nHigher latency is easier to accept if there is some communication of pro‚Äê\\ngress/intermediate output throughout.\\nStructured output\\nRequiring an LLM to produce output in a predefined format makes it more likely\\nthat it will conform to expectations.\\nHuman in the loop\\nHigher-agency architectures benefit from human intervention while they‚Äôre run‚Äê\\nning: interrupting, approving, forking, or undoing.\\nDouble texting modes\\nThe longer an LLM app takes to answer, the more likely it is that the user might\\nsend it new input before the previous one has finished being processed.\\nStructured Output\\nIt is often crucial to have LLMs return structured output, either because a down‚Äê\\nstream use of that output expects a things in a specific schema (a definition of the\\nnames and types of the various fields in a piece of structured output) or purely to\\nreduce variance to what would otherwise be completely free-form text output.\\nThere are a few different strategies you can use for this with different LLMs:\\nPrompting\\nThis is when you ask the LLM (very nicely) to return output in the desired format\\n(for instance, JSON, XML, or CSV). Prompting‚Äôs big advantage is that it works to\\nsome extent with any LLM; the downside is that it acts more as a suggestion to the\\nLLM and not as a guarantee that the output will come out in this format.\\nTool calling\\nThis is available for LLMs that have been fine-tuned to pick from a list of possible\\noutput schemas, and to produce something that conforms to the chosen one.\\nThis usually involves writing, for each of the possible output schemas: a name to\\nidentify it, a description to help the LLM decide when it is the appropriate choice,\\nand a schema for the desired output format (usually in JSONSchema notation).\\nJSON mode\\nThis is a mode available in some LLMs (such as recent OpenAI models) that\\nenforces the LLM to output a valid JSON document.\\nStructured Output | 173'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 201, 'page_label': '174'}, page_content='Different models may support different variants of these, with slightly different\\nparameters. To make it easy to get LLMs to return structured output, LangChain\\nmodels implement a common interface, a method called .with_structured_output.\\nBy invoking this method‚Äîand passing in a JSON schema or a Pydantic (in Python)\\nor Zod (in JS) model‚Äîthe model will add whatever model parameters and output\\nparsers are necessary to produce and return the structured output. When a particular\\nmodel implements more than one of the preceding strategies, you can configure\\nwhich method to use.\\nLet‚Äôs create a schema to use:\\nPython\\nfrom pydantic import BaseModel, Field\\nclass Joke(BaseModel):\\n    setup: str = Field(description=\"The setup of the joke\")\\n    punchline: str = Field(description=\"The punchline to the joke\")\\nJavaScript\\nimport { z } from \"zod\";\\nconst joke = z.object({\\n  setup: z.string().describe(\"The setup of the joke\"),\\n  punchline: z.string().describe(\"The punchline to the joke\"),\\n});\\nNotice how we take care to add a description to each field. This is key because‚Äî\\ntogether with the name of the field‚Äîthis is the information the LLM will use to\\ndecide what part of the output should go in each field. We could also have defined a\\nschema in raw JSONSchema notation, which would look like this:\\n{\\'properties\\': {\\'setup\\': {\\'description\\': \\'The setup of the joke\\',\\n    \\'title\\': \\'Setup\\',\\n    \\'type\\': \\'string\\'},\\n \\'punchline\\': {\\'description\\': \\'The punchline to the joke\\',\\n    \\'title\\': \\'Punchline\\',\\n    \\'type\\': \\'string\\'}},\\n \\'required\\': [\\'setup\\', \\'punchline\\'],\\n \\'title\\': \\'Joke\\',\\n \\'type\\': \\'object\\'}\\n174 | Chapter 8: Patterns to Make the Most of LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 202, 'page_label': '175'}, page_content='And now let‚Äôs get an LLM to generate output that conforms to this schema:\\nPython\\nfrom langchain_openai import ChatOpenAI\\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\nmodel = model.with_structured_output(Joke)\\nmodel.invoke(\"Tell me a joke about cats\")\\nJavaScript\\nimport { ChatOpenAI } from \"@langchain/openai\";\\nlet model = new ChatOpenAI({\\n  model: \"gpt-3.5-turbo\",\\n  temperature: 0\\n});\\nmodel = model.withStructuredOutput(joke);\\nawait structuredLlm.invoke(\"Tell me a joke about cats\");\\nAn example of output\\n{\\n    setup: \"Why don\\'t cats play poker in the wild?\",\\n    punchline: \"Too many cheetahs.\"\\n}\\nA couple of things to notice:\\n‚Ä¢ We create the instance of the model as usual, specifying the model name to use‚Ä¢\\nand other parameters.\\n‚Ä¢ Low temperature is usually a good fit for structured output, as it reduces the‚Ä¢\\nchance the LLM will produce invalid output that doesn‚Äôt conform to the schema.\\n‚Ä¢ Afterward, we attach the schema to the model, which returns a new object, which‚Ä¢\\nwill produce output that matches the schema provided. When you pass in a\\nPydantic or Zod object for schema, this will be used for validation as well; that\\nis, if the LLM produces output that doesn‚Äôt conform, a validation error will be\\nreturned to you instead of the failed output.\\n‚Ä¢ Finally, we invoke the model with our (free-form) input, and receive back output‚Ä¢\\nthat matches the structure we desired.\\nThis pattern of using structured output can be very useful both as a standalone tool\\nand as a part of a larger application; for instance, refer back to Chapter 5, where we\\nmake use of this capability to implement the routing step of the router architecture.\\nStructured Output | 175'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 203, 'page_label': '176'}, page_content='Intermediate Output\\nThe more complex your LLM architecture becomes, the more likely it will take longer\\nto execute. If you think back to the architecture diagrams in Chapters 5 and 6, every\\ntime you see multiple steps (or nodes) connected in sequence or in a loop, that is an\\nindication that the time it takes for a full invocation is increasing.\\nThis increase in latency‚Äîif not addressed‚Äîcan be a blocker to user adoption of\\nLLM applications, with most users expecting computer applications to produce some\\noutput within seconds. There are several strategies to make the higher latency more\\npalatable, but they all fall under the umbrella of streaming output, that is, receiving\\noutput from the application while it is still running.\\nFor this section, we‚Äôll use the last architecture described in ‚ÄúDealing with Many Tools‚Äù\\non page 148. Refer back to Chapter 6 for the full code snippet.\\nTo generate intermediate output with LangGraph, all you have to do is to invoke the\\ngraph with the stream method, which will yield the output of each node as soon as\\neach finishes. Let‚Äôs see what that looks like:\\nPython\\ninput = {\\n    \"messages\": [\\n        HumanMessage(\"\"\"How old was the 30th president of the United States \\n            when he died?\"\"\")\\n    ]\\n}\\nfor c in graph.stream(input, stream_mode=\\'updates\\'):\\n    print(c)\\nJavaScript\\nconst input = {\\n  messages: [\\n    new HumanMessage(`How old was the 30th president of the United States when \\n      he died?`)\\n  ]\\n}\\nconst output = await graph.stream(input, streamMode: \\'updates\\')\\nfor await (const c of output) {\\n  console.log(c)\\n}\\nThe output:\\n{\\n    \"select_tools\": {\\n        \"selected_tools\": [\\'duckduckgo_search\\', \\'calculator\\']\\n    }\\n}\\n{\\n176 | Chapter 8: Patterns to Make the Most of LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 204, 'page_label': '177'}, page_content='\"model\": {\\n        \"messages\": AIMessage(\\n            content=\"\",\\n            tool_calls=[\\n                {\\n                    \"name\": \"duckduckgo_search\",\\n                    \"args\": {\\n                        \"query\": \"30th president of the United States\"\\n                    },\\n                    \"id\": \"9ed4328dcdea4904b1b54487e343a373\",\\n                    \"type\": \"tool_call\",\\n                }\\n            ],\\n        )\\n    }\\n}\\n{\\n    \"tools\": {\\n        \"messages\": [\\n            ToolMessage(\\n                content=\"Calvin Coolidge (born July 4, 1872, Plymouth, Vermont, \\n                    U.S.‚Äîdied January 5, 1933, Northampton, Massachusetts) was \\n                    the 30th president of the United States (1923-29). Coolidge \\n                    acceded to the presidency after the death in office of \\n                    Warren G. Harding, just as the Harding scandals were coming \\n                    to light....\",\\n                name=\"duckduckgo_search\",\\n                tool_call_id=\"9ed4328dcdea4904b1b54487e343a373\",\\n            )\\n        ]\\n    }\\n}\\n{\\n    \"model\": {\\n        \"messages\": AIMessage(\\n            content=\"Calvin Coolidge, the 30th president of the United States, \\n                was born on July 4, 1872, and died on January 5, 1933. To \\n                calculate his age at the time of his death, we can subtract his \\n                birth year from his death year. \\\\n\\\\nAge at death = Death year - \\n                Birth year\\\\nAge at death = 1933 - 1872\\\\nAge at death = 61 \\n                years\\\\n\\\\nCalvin Coolidge was 61 years old when he died.\",\\n        )\\n    }\\n}\\nNotice how each output entry is a dictionary with the name of the node that emitted\\nas the key and the output of that node as the value. This gives you two key pieces of\\ninformation:\\n‚Ä¢ Where the application currently is; that is, if you think back to the architecture‚Ä¢\\ndiagrams shown in previous chapters, where in that diagram are we currently?\\nStructured Output | 177'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 205, 'page_label': '178'}, page_content='‚Ä¢ Each update to the shared state of the application, which together build up to the‚Ä¢\\nfinal output of the graph.\\nIn addition, LangGraph supports more stream modes:\\n‚Ä¢ updates. This is the default mode, described above.‚Ä¢\\n‚Ä¢ values. This mode yields the current state of the graph every time it changes,‚Ä¢\\nthat is after each set of nodes finishes executing. This can be useful when the way\\nyou display output to your users closely tracks the shape of the graph state.\\n‚Ä¢ debug. This mode yields detailed events every time something happens in your‚Ä¢\\ngraph, including:\\n‚Äî checkpoint events, whenever a new checkpoint of the current state is saved to‚Äî\\nthe database\\n‚Äî task events, emitted whenever a node is about to start running‚Äî\\n‚Äî task_result events, emitted whenever a node finishes running‚Äî\\n‚Ä¢ Finally, you can combine these modes; for instance, requesting both updates and‚Ä¢\\nvalues by passing a list.\\nY ou control the stream mode with the stream_mode argument to stream().\\nStreaming LLM Output Token-by-Token\\nSometimes you may also want to get streaming output from each LLM call inside\\nyour larger LLM application. This can be useful for various projects, such as when\\nbuilding an interactive chatbot, where you want each word to be displayed as soon as\\nit is produced by the LLM.\\nY ou can achieve this with LangGraph as well:\\nPython\\ninput = {\\n    \"messages\": [\\n        HumanMessage(\"\"\"How old was the 30th president of the United States \\n            when he died?\"\"\")\\n    ]\\n}\\noutput = app.astream_events(input, version=\"v2\")\\nasync for event in output:\\n    if event[\"event\"] == \"on_chat_model_stream\":\\n        content = event[\"data\"][\"chunk\"].content\\n        if content:\\n            print(content)\\n178 | Chapter 8: Patterns to Make the Most of LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 206, 'page_label': '179'}, page_content='JavaScript\\nconst input = {\\n  messages: [\\n    new HumanMessage(`How old was the 30th president of the United States when \\n      he died?`)\\n  ]\\n}\\nconst output = await agent.streamEvents(input, {version: \"v2\"});\\nfor await (const { event, data } of output) {\\n  if (event === \"on_chat_model_stream\") {\\n    const msg = data.chunk as AIMessageChunk;\\n    if (msg.content) {\\n      console.log(msg.content);\\n    }\\n  }\\n}\\nThis will emit each word (technically each token) as soon as it is received from the\\nLLM. Y ou can find more details on this pattern from LangChain.\\nHuman-in-the-Loop Modalities\\nAs we walk the autonomy (or agency) ladder, we find ourselves increasingly giving\\nup control (or oversight) in exchange for capability (or autonomy). The shared state\\npattern used in LangGraph (see Chapter 5  for an introduction) makes it easier to\\nobserve, interrupt, and modify the application. This makes it possible to use many\\ndifferent human-in-the-loop modes, or ways for the developer/end user of an applica‚Äê\\ntion to influence what the LLM is up to.\\nFor this section, we‚Äôll again use the last architecture described in ‚ÄúDealing with\\nMany Tools‚Äù on page 148. Refer back to Chapter 6 for the full code snippet. For all\\nhuman-in-the-loop modes, we first need to attach a checkpointer to the graph; refer\\nto ‚Äú Adding Memory to StateGraph‚Äù on page 105 for more details on this:\\nPython\\nfrom langgraph.checkpoint.memory import MemorySaver\\ngraph = builder.compile(checkpointer=MemorySaver())\\nJavaScript\\nimport {MemorySaver} from \\'@langchain/langgraph\\'\\ngraph = builder.compile({ checkpointer: new MemorySaver() })\\nStructured Output | 179'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 207, 'page_label': '180'}, page_content='This returns an instance of the graph that stores the state at the end of each step, so\\nevery invocation after the first doesn‚Äôt start from a blank slate. Any time the graph\\nis called, it starts by using the checkpointer to fetch the most recent saved state‚Äîif\\nany‚Äîand combines the new input with the previous state. And only then does it\\nexecute the first nodes. This is key to enabling human-in-the-loop modalities, which\\nall rely on the graph remembering the previous state.\\nThe first mode, interrupt, is the simplest form of control‚Äîthe user is looking at\\nstreaming output of the application as it is produced, and manually interrupts it when\\nhe sees fit (see Figure 8-3). The state is saved as of the last complete step prior to the\\nuser hitting the interrupt button. From there the user can choose to:\\n‚Ä¢ Resume from that point onward, and the computation will proceed as if it hadn‚Äôt‚Ä¢\\nbeen interrupted (see ‚ÄúResume‚Äù on page 183).\\n‚Ä¢ Send new input into the application (e.g., a new message in a chatbot), which will‚Ä¢\\ncancel any future steps that were pending and start dealing with the new input\\n(see ‚ÄúRestart‚Äù on page 184).\\n‚Ä¢ Do nothing and nothing else will run.‚Ä¢\\nFigure 8-3. The interrupt pattern\\n180 | Chapter 8: Patterns to Make the Most of LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 208, 'page_label': '181'}, page_content='Let‚Äôs see how to do this in LangGraph:\\nPython\\nimport asyncio\\nevent = asyncio.Event()\\ninput = {\\n    \"messages\": [\\n        HumanMessage(\"\"\"How old was the 30th president of the United States \\n            when he died?\"\"\")\\n    ]\\n}\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\nasync with aclosing(graph.astream(input, config)) as stream:\\n    async for chunk in stream:\\n        if event.is_set():\\n            break\\n        else:\\n            ... # do something with the output\\n# Somewhere else in your application\\nevent.set()\\nJavaScript\\nconst controller = new AbortController()\\nconst input = {\\n  \"messages\": [\\n    new HumanMessage(`How old was the 30th president of the United States when \\n      he died?`)\\n  ]\\n}\\nconst config = {\"configurable\": {\"thread_id\": \"1\"}}\\ntry {\\n  const output = await graph.stream(input, {\\n    ...config,\\n    signal: controller.signal\\n  });\\n  for await (const chunk of output) {\\n    console.log(chunk); // do something with the output\\n  }\\n} catch (e) {\\n  console.log(e);\\n}\\nStructured Output | 181'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 209, 'page_label': '182'}, page_content='// Somewhere else in your application\\ncontroller.abort()\\nThis makes use of an event or signal, so that you can control interruption from\\noutside of the running application. Notice in the Python code block the use of\\naclosing; this ensures the stream is properly closed when interrupted. Notice in JS\\nthe use of the try-catch statement, as interrupting the run will result in an abort\\nexception being raised. Finally notice that usage of the checkpointer requires passing\\nin an identifier for this thread, to distinguish this interaction with the graph from all\\nothers.\\nFigure 8-4. The authorize pattern\\nA second control mode is authorize, where the user defines ahead of time that they\\nwant the application to hand off control to them every time a particular node is about\\nto be called (see Figure 8-4 ). This is usually implemented for tool confirmation‚Äî\\nbefore any tool (or particular tools) is called, the application will pause and ask for\\nconfirmation, at which point the user can, again:\\n‚Ä¢ Resume computation, accepting the tool call.‚Ä¢\\n‚Ä¢ Send a new message to guide the bot in a different direction, in which case the‚Ä¢\\ntool will not be called.\\n‚Ä¢ Do nothing.‚Ä¢\\n182 | Chapter 8: Patterns to Make the Most of LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 210, 'page_label': '183'}, page_content='Here‚Äôs the code:\\nPython\\ninput = {\\n    \"messages\": [\\n        HumanMessage(\"\"\"How old was the 30th president of the United States \\n            when he died?\"\"\")\\n    ]\\n}\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\noutput = graph.astream(input, config, interrupt_before=[\\'tools\\'])\\nasync for c in output:\\n    ... # do something with the output\\nJavaScript\\nconst input = {\\n  \"messages\": [\\n    new HumanMessage(`How old was the 30th president of the United States when \\n      he died?`)\\n  ]\\n}\\nconst config = {\"configurable\": {\"thread_id\": \"1\"}}\\nconst output = await graph.stream(input, {\\n  ...config,\\n  interruptBefore: [\\'tools\\']\\n});\\nfor await (const chunk of output) {\\n  console.log(chunk); // do something with the output\\n}\\nThis will run the graph up until it is about to enter the node called tools, thus\\ngiving you the chance to inspect the current state, and decide whether to proceed or\\nnot. Notice that interrupt_before is a list where order is not important; if you pass\\nmultiple node names, it will interrupt before entering each of them.\\nResume\\nTo proceed from an interrupted graph‚Äîsuch as when using one of the previous two\\npatterns‚Äîyou just need to re-invoke the graph with null input (or None in Python).\\nThis is taken as a signal to continue processing the previous non-null input:\\nPython\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\noutput = graph.astream(None, config, interrupt_before=[\\'tools\\'])\\nStructured Output | 183'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 211, 'page_label': '184'}, page_content='async for c in output:\\n    ... # do something with the output\\nJavaScript\\nconst config = {\"configurable\": {\"thread_id\": \"1\"}}\\nconst output = await graph.stream(null, {\\n  ...config,\\n  interruptBefore: [\\'tools\\']\\n});\\nfor await (const chunk of output) {\\n  console.log(chunk); // do something with the output\\n}\\nRestart\\nIf instead you want an interrupted graph to start over from the first node, with\\nadditional new input, you just need to invoke it with new input:\\nPython\\ninput = {\\n    \"messages\": [\\n        HumanMessage(\"\"\"How old was the 30th president of the United States \\n            when he died?\"\"\")\\n    ]\\n}\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\noutput = graph.astream(input, config)\\nasync for c in output:\\n    ... # do something with the output\\nJavaScript\\nconst input = {\\n  \"messages\": [\\n    new HumanMessage(`How old was the 30th president of the United States when \\n      he died?`)\\n  ]\\n}\\nconst config = {\"configurable\": {\"thread_id\": \"1\"}}\\nconst output = await graph.stream(input, config);\\nfor await (const chunk of output) {\\n  console.log(chunk); // do something with the output\\n}\\n184 | Chapter 8: Patterns to Make the Most of LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 212, 'page_label': '185'}, page_content='This will keep the current state of the graph, merge it with the new input, and start\\nagain from the first node.\\nIf you want to lose the current state, just change the thread_id, which will start a new\\ninteraction from a blank slate. Any string value is a valid thread_id; we‚Äô d recommend\\nusing UUIDs (or other unique identifiers) as thread IDs.\\nEdit state\\nSometimes you might want to update the state of the graph before resuming; this\\nis possible with the update_state method. Y ou‚Äôll usually want to first inspect the\\ncurrent state with get_state.\\nHere‚Äôs what it looks like:\\nPython\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\nstate = graph.get_state(config)\\n# something you want to add or replace\\nupdate = { }\\ngraph.update_state(config, update)\\nJavaScript\\nconst config = \"configurable\": {\"thread_id\": \"1\"}\\nconst state = await graph.getState(config)\\n// something you want to add or replace\\nconst update = { }\\nawait graph.updateState(config, update)\\nThis will create a new checkpoint containing your update. After this, you‚Äôre ready to\\nresume the graph from this new point. See ‚ÄúResume‚Äù on page 183 to find out how.\\nFork\\nY ou can also browse the history of all past states the graph has passed through, and\\nany of them can be visited again, for instance, to get an alternative answer. This can\\nbe very useful in more creative applications, where each run through the graph is\\nexpected to produce different output.\\nStructured Output | 185'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 213, 'page_label': '186'}, page_content='Let‚Äôs see what it looks like:\\nPython\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\nhistory = [\\n    state for state in\\n    graph.get_state_history(config)\\n]\\n# replay a past state\\ngraph.invoke(None, history[2].config)\\nJavaScript\\nconst config = \"configurable\": {\"thread_id\": \"1\"}\\nconst history = await Array.fromAsync(graph.getStateHistory(config))\\n// replay a past state\\nawait graph.invoke(null, history[2].config)\\nNotice how we collect the history into a list/array in both languages;\\nget_state_history returns an iterator of states (to allow consuming lazily). The\\nstates returned from the history method are sorted with the most recent first and the\\noldest last.\\nThe true power of the human-in-the-loop controls comes from mixing them in\\nwhatever way suits your application.\\nMultitasking LLMs\\nThis section covers the problem of handling concurrent input for LLM applications.\\nThis is a particularly relevant problem given that LLMs are quite slow, much more\\nso when producing long outputs or when chained in multistep architectures (like\\nyou can do with LangGraph). Even as LLMs become faster, dealing with concurrent\\ninputs will continue to be a challenge, as latency improvements will also unlock the\\ndoor for more and more complex use cases, in much the same way as even the most\\nproductive person still faces the need to prioritize competing demands on their time.\\nLet‚Äôs walk through the options.\\nRefuse concurrent inputs\\nAny input received while processing a previous one is rejected. This is the simplest\\nstrategy, but unlikely to cover all needs, as it effectively means handing off concur‚Äê\\nrency management to the caller.\\n186 | Chapter 8: Patterns to Make the Most of LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 214, 'page_label': '187'}, page_content='Handle independently\\nAnother simple option is to treat any new input as an independent invocation,\\ncreating a new thread (a container for remembering state) and producing output in\\nthat context. This has the obvious downside of needing to be shown to the user as two\\nseparate and unreconcilable invocations, which isn‚Äôt always possible or desirable. On\\nthe other hand, it has the upside of scaling to arbitrarily large sizes, and is something\\nyou‚Äôll use to some extent in your application almost certainly. For instance, this is\\nhow you would think about the problem of getting a chatbot to ‚Äúchat‚Äù with two\\ndifferent users concurrently.\\nQueue concurrent inputs\\nAny input received while processing a previous one is queued up and handled when\\nthe current one is finished. This strategy has some pros:\\n‚Ä¢ It supports receiving an arbitrary number of concurrent requests.‚Ä¢\\n‚Ä¢ Because we wait for current input to finish processing, it doesn‚Äôt matter if the‚Ä¢\\nnew input arrives almost immediately after we start handling the current input\\nor immediately before we finish; the end result will be the same, as we will finish\\nprocessing the current input before moving on to the next.\\nThe strategy suffers from a few drawbacks as well:\\n‚Ä¢ It may take a while to process all queued inputs; in fact, the queue may grow‚Ä¢\\nunbounded if inputs are produced at a rate faster than processed.\\n‚Ä¢ The inputs may be stale by the time they get processed, given that they are‚Ä¢\\nqueued before seeing the response to the previous one, and not altered after‚Äê\\nwards. This strategy is not appropriate when new inputs depend on previous\\nanswers.\\nInterrupt\\nWhen a new input is received while another is being processed, abandon processing\\nof the current one and restart the chain with the new input. This strategy can vary by\\nwhat is kept of the interrupted run. Here are a few options:\\n‚Ä¢ Keep nothing. The previous input is completely forgotten, as if it had never been‚Ä¢\\nsent or processed.\\n‚Ä¢ Keep the last completed step. In a checkpointing app (which stores progress as it‚Ä¢\\nmoves through the computation), keep the state produced by the last completed\\nstep, discard any pending state updates from the currently executing step, and\\nstart handling the new input in that context.\\nStructured Output | 187'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 215, 'page_label': '188'}, page_content='‚Ä¢ Keep the last completed step, as well as the current in-progress step. Attempt to‚Ä¢\\ninterrupt the current step while taking care to save any incomplete updates to\\nstate that were being produced at the time. This is likely to not generalize beyond\\nthe simplest architectures.\\n‚Ä¢ Wait for the current node (but not any subsequent nodes) to finish, then save and‚Ä¢\\ninterrupt.\\nThis option has some pros compared to queuing concurrent inputs:\\n‚Ä¢ New input is handled as soon as possible, reducing latency and the chance of‚Ä¢\\nproducing stale outputs.\\n‚Ä¢ For the ‚Äúkeep nothing‚Äù variant, the final output doesn‚Äôt depend on when the new‚Ä¢\\ninput was received.\\nBut it also has drawbacks:\\n‚Ä¢ Effectively, this strategy is still limited to processing one input at a time; any old‚Ä¢\\ninput is abandoned when new input is received.\\n‚Ä¢ Keeping partial state updates for the next run requires the state to be designed‚Ä¢\\nwith that in mind; if not, then your application is likely to end up in an invalid\\nstate. For instance, OpenAI chat models require an AI message requesting tool\\ncalls to be immediately followed by tool messages with the tool outputs. If your\\nrun is interrupted in between, you either defensively clean up the intermediate\\nstate or risk being unable to progress further.\\n‚Ä¢ The final outputs produced are very sensitive to when the new input is received;‚Ä¢\\nnew input will be handled in the context of the (incomplete) progress previously\\nmade toward handling the previous input. This can result in brittle or unpredict‚Äê\\nable outcomes if you don‚Äôt design accordingly.\\nFork and merge\\nAnother option is to handle new input in parallel, forking the state of the thread as it\\nis when the new input is received and merging the final states as inputs finish being\\nhandled. This option requires designing your state to either be mergeable without\\nconflicts (e.g., using conflict-free replicated data types [CRDTs] or other conflict\\nresolution algorithms) or having the user manually resolve conflicts before you‚Äôre\\nable to make sense of the output or send new input in this thread. If either of those\\ntwo requirements is met, this is likely to be the best option overall. This way, new\\ninput is handled in a timely manner, output is independent of time received, and it\\nsupports an arbitrary number of concurrent runs.\\nSome of these strategies are implemented in LangGraph Platform, which will be\\ncovered in Chapter 9.\\n188 | Chapter 8: Patterns to Make the Most of LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 216, 'page_label': '189'}, page_content='Summary\\nIn this chapter, we returned to the main trade-off you face when building LLM\\napplications: agency versus reliability. We learned that there are strategies to partially\\nbeat the odds and get more reliability without sacrificing agency, and vice versa.\\nWe started by covering structured outputs, which can improve the predictability\\nof LLM-generated text. Next, we discussed emitting streaming/intermediate output\\nfrom your application, which can make high latency (an inevitable side effect of\\nagency currently) applications pleasant to use.\\nWe also walked through a variety of human-in-the-loop controls‚Äîthat is, techniques\\nto give back some oversight to the end user of your LLM application‚Äîwhich can\\noften make the difference in making high-agency architectures reliable. Finally, we\\ntalked about the problem of handling concurrent input to your application, a particu‚Äê\\nlarly salient problem for LLM apps given their high latency.\\nIn the next chapter, you‚Äôll learn how to deploy your AI application into production.\\nSummary | 189'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 218, 'page_label': '191'}, page_content='CHAPTER 9\\nDeployment: Launching Your\\nAI Application into Production\\nSo far, we‚Äôve explored the key concepts, ideas, and tools to help you build the core\\nfunctionality of your AI application. Y ou‚Äôve learned how to utilize LangChain and\\nLangGraph to generate LLM outputs, index and retrieve data, and enable memory\\nand agency.\\nBut your application is limited to your local environment, so external users can‚Äôt\\naccess its features yet.\\nIn this chapter, you‚Äôll learn the best practices for deploying your AI application into\\nproduction. We‚Äôll also explore various tools to debug, collaborate, test, and monitor\\nyour LLM applications.\\nLet‚Äôs get started.\\nPrerequisites\\nIn order to effectively deploy your AI application, you need to utilize various services\\nto host your application, store and retrieve data, and monitor your application. In the\\ndeployment example in this chapter, we will incorporate the following services:\\nVector store\\nSupabase\\nMonitoring and debugging\\nLangSmith\\nBackend API\\nLangGraph Platform\\n191'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 219, 'page_label': '192'}, page_content='We will dive deeper into each of these components and services and see how to adapt\\nthem for your use case. But first, let‚Äôs install necessary dependencies and set up the\\nenvironment variables.\\nIf you‚Äô d like to follow the example, fork this LangChain template  to your Git‚Äê\\nHub account. This repository contains the full logic of a retrieval agent-based AI\\napplication.\\nInstall Dependencies\\nFirst, follow the instructions in the README.md file to install the project\\ndependencies.\\nIf you‚Äôre not using the template, you can install the dependencies individually from\\nthe respective pyproject.toml or package.json files.\\nSecond, create a .env file and store the following variables:\\nOPENAI_API_KEY=\\nSUPABASE_URL=\\nSUPABASE_SERVICE_ROLE_KEY=\\n# for tracing\\nLANGCHAIN_TRACING_V2=true\\nLANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\\nLANGCHAIN_API_KEY=\\nNext, we‚Äôll walk through the process of retrieving the values for each of these\\nvariables.\\nLarge Language Model\\nThe LLM is responsible for generating the output based on a given query. LangChain\\nprovides access to popular LLM providers, including OpenAI, Anthropic, Google,\\nand Cohere.\\nIn this deployment example, we‚Äôll utilize OpenAI by retrieving the API keys , as\\nshown in Figure 9-1 . Once you‚Äôve retrieved your API keys, input the value as\\nOPENAI_API_KEY in your .env file.\\n192 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 220, 'page_label': '193'}, page_content='Figure 9-1. OpenAI API keys dashboard\\nVector Store\\nAs discussed in previous chapters, a vector store is a special database responsible for\\nstoring and managing vector representations of your data‚Äîin other words, embed‚Äê\\ndings. A vector store enables similarity search and context retrieval to help the LLM\\ngenerate accurate answers based on the user‚Äôs query.\\nFor our deployment, we‚Äôll use Supabase‚Äîa PostgreSQL database‚Äîas the vector store.\\nSupabase utilizes the pgvector extension to store embeddings and query vectors for\\nsimilarity search.\\nIf you haven‚Äôt yet done it, create a Supabase account. Once you‚Äôve created an account,\\nclick ‚ÄúNew project‚Äù on the dashboard page. Follow the steps and save the database\\npassword after creating it, as shown in Figure 9-2.\\nPrerequisites | 193'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 221, 'page_label': '194'}, page_content='Figure 9-2. Supabase project creation dashboard\\nOnce your Supabase project is created, navigate to the Project Settings tab and select\\nAPI under Configuration. Under this new tab, you will see Project URL and Project\\nAPI keys.\\nIn your .env file, copy and paste the Project URL as the value to SUPABASE_URL and\\nthe service_role secret API key as the value to SUPABASE_SERVICE_ROLE_KEY.\\nNavigate to the SQL editor in the Supabase menu and run the following SQL scripts.\\nFirst, let‚Äôs enable pgvector:\\n## Enable the pgvector extension to work with embedding vectors\\ncreate extension vector;\\nNow create a table called documents to store vectors of your data:\\n## Create a table to store your documents\\ncreate table documents (\\n  id bigserial primary key,\\n  content text, -- corresponds to Document.pageContent\\n  metadata jsonb, -- corresponds to Document.metadata\\n194 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 222, 'page_label': '195'}, page_content=\"embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\\n);\\nY ou should now see the documents table in the Supabase database.\\nNow you can create a script to generate the embeddings of your data, store them, and\\nquery from the database. Open the Supabase SQL editor again and run the following\\nscript:\\n## Create a function to search for documents\\ncreate function match_documents (\\n  query_embedding vector(1536),\\n  match_count int DEFAULT null,\\n  filter jsonb DEFAULT '{}'\\n) returns table (\\n  id bigint,\\n  content text,\\n  metadata jsonb,\\n  embedding jsonb,\\n  similarity float\\n)\\nlanguage plpgsql\\nas $$\\n#variable_conflict use_column\\nbegin\\n  return query\\n  select\\n    id,\\n    content,\\n    metadata,\\n    (embedding::text)::jsonb as embedding,\\n    1 - (documents.embedding <=> query_embedding) as similarity\\n  from documents\\n  where metadata @> filter\\n  order by documents.embedding <=> query_embedding\\n  limit match_count;\\nend;\\n$$;\\nThe match_documents database function takes a query_embedding vector and\\ncompares it to embeddings in the documents table using cosine similarity. It\\ncalculates a similarity score for each document (1 - ( documents.embedding <=>\\nquery_embedding)), then returns the most similar matches. The results are:\\n1. Filtered first by the metadata criteria specified in the filter argument (using JSON\\ncontainment @>).\\n2. Ordered by similarity score (highest first).\\n3. Limited to the number of matches specified in match_count.\\nPrerequisites | 195\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 223, 'page_label': '196'}, page_content='Once the vector similarity function is generated, you can use Supabase as a vector\\nstore by importing the class and providing the necessary parameters. Here‚Äôs an\\nexample of how it works:\\nPython\\nimport os\\nfrom langchain_community.vectorstores import SupabaseVectorStore\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom supabase.client import Client, create_client\\nsupabase_url = os.environ.get(\"SUPABASE_URL\")\\nsupabase_key = os.environ.get(\"SUPABASE_SERVICE_ROLE_KEY\")\\nsupabase: Client = create_client(supabase_url, supabase_key)\\nembeddings = OpenAIEmbeddings()\\n## Assuming you\\'ve already generated embeddings of your data\\nvector_store = SupabaseVectorStore(\\n    embedding=embeddings,\\n    client=supabase,\\n    table_name=\"documents\",\\n    query_name=\"match_documents\",\\n)\\n## Test that similarity search is working\\nquery = \"What is this document about?\"\\nmatched_docs = vector_store.similarity_search(query)\\nprint(matched_docs[0].page_content)\\nJavaScript\\nimport {\\n  SupabaseVectorStore\\n} from \"@langchain/community/vectorstores/supabase\";\\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\\nimport { createClient } from \"@supabase/supabase-js\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst supabaseClient = createClient(\\n  process.env.SUPABASE_URL,\\n  process.env.SUPABASE_SERVICE_ROLE_KEY\\n);\\nconst vectorStore = new SupabaseVectorStore(embeddings, {\\n  client: supabaseClient,\\n196 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 224, 'page_label': '197'}, page_content='tableName: \"documents\",\\n  queryName: \"match_documents\",\\n});\\n// Example documents structure of your data\\nconst document1: Document = {\\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\\n  metadata: { source: \"https://example.com\" },\\n};\\nconst document2: Document = {\\n  pageContent: \"Buildings are made out of brick\",\\n  metadata: { source: \"https://example.com\" },\\n};\\nconst documents = [document1, document2]\\n//Embed and store the data in the database\\nawait vectorStore.addDocuments(documents, { ids: [\"1\", \"2\"] });\\n// Query the Vector Store\\nconst filter = { source: \"https://example.com\" };\\nconst similaritySearchResults = await vectorStore.similaritySearch(\\n  \"biology\",\\n  2,\\n  filter\\n);\\nfor (const doc of similaritySearchResults) {\\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\\n}\\nThe output:\\nThe powerhouse of the cell is the mitochondria [{\"source\":\"https://example.com\"}]\\nY ou can review the full logic of the Supabase vector store implementation in the\\nGithub LangChain template mentioned previously.\\nBackend API\\nAs discussed in previous chapters, LangGraph is a low-level open source frame‚Äê\\nwork used to build complex agentic systems powered by LLMs. LangGraph enables\\nfine-grained control over the flow and state of your application, built-in persis‚Äê\\ntence, and advanced human-in-the-loop and memory features. Figure 9-3 illustrates\\nLangGraph‚Äôs control flow.\\nPrerequisites | 197'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 225, 'page_label': '198'}, page_content='Figure 9-3. Example of LangGraph API control flow\\nTo deploy an AI application that utilizes LangGraph, we will use LangGraph Plat‚Äê\\nform. LangGraph Platform is a managed service for deploying and hosting Lang‚Äê\\nGraph agents at scale.\\nAs your agentic use case gains traction, uneven task distribution among agents can\\noverload the system, leading to downtime. LangGraph Platform manages horizontally\\nscaling task queues, servers, and a robust Postgres checkpointer to handle many con‚Äê\\ncurrent users and efficiently store large states and threads. This ensures fault-tolerant\\nscalability.\\nLangGraph Platform is designed to support real-world interaction patterns. In addi‚Äê\\ntion to streaming and human-in-the-loop features, LangGraph Platform enables the\\nfollowing:\\n‚Ä¢ Double texting to handle new user inputs on ongoing graph threads‚Ä¢\\n‚Ä¢ Asynchronous background jobs for long-running tasks‚Ä¢\\n‚Ä¢ Cron jobs for running common tasks on a schedule‚Ä¢\\nLangGraph Platform also provides an integrated solution for collaborating on,\\ndeploying, and monitoring agentic AI applications. It includes LangGraph Studio‚Äîa\\nvisual playground for debugging, editing, and testing agents. LangGraph Studio also\\nenables you to share your LangGraph agent with team members for collaborative\\nfeedback and rapid iteration, as Figure 9-4 shows.\\n198 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 226, 'page_label': '199'}, page_content='Figure 9-4. Snapshot of LangGraph Studio UI\\nAdditionally, LangGraph Platform simplifies agentic deployment by enabling one-\\nclick submissions.\\nCreate a LangSmith Account\\nLangSmith is an all-in-one developer platform that enables you to debug, collaborate,\\ntest, and monitor your LLM applications. LangGraph Platform is seamlessly integra‚Äê\\nted with LangSmith and is accessible from within the LangSmith UI.\\nTo deploy your application on LangGraph Platform, you need to create a LangSmith\\naccount. Once you‚Äôre logged in to the dashboard, navigate to the Settings page, then\\nscroll to the API Keys section and click Create API Key. Y ou should see a UI similar\\nto Figure 9-5.\\nPrerequisites | 199'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 227, 'page_label': '200'}, page_content='Figure 9-5. Create LangSmith API Key UI\\nCopy the API Key value as your LANGCHAIN_API_KEY in your .env file.\\nNavigate to ‚ÄúUsage and billing‚Äù and set up your billing details. Then click the ‚ÄúPlans\\nand Billings‚Äù tab and the ‚ÄúUpgrade to Plus‚Äù button to get instructions on transition‚Äê\\ning to a LangSmith Plus plan, which will enable LangGraph Platform usage. If you‚Äô d\\nprefer to use a free self-hosted deployment, you can follow the instructions here .\\nPlease note that this option requires management of the infrastructure, including\\nsetting up and maintaining required databases and Redis instances.\\nUnderstanding the LangGraph Platform API\\nBefore deploying your AI application on LangGraph Platform, it‚Äôs important to\\nunderstand how each component of the LangGraph API works. These components\\ncan generally be split into data models and features.\\n200 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 228, 'page_label': '201'}, page_content='Data Models\\nThe LangGraph Platform API consists of a few core data models:\\n‚Ä¢ Assistants‚Ä¢\\n‚Ä¢ Threads‚Ä¢\\n‚Ä¢ Runs‚Ä¢\\n‚Ä¢ Cron jobs‚Ä¢\\nAssistants\\nAn assistant is a configured instance of a CompiledGraph. It abstracts the cognitive\\narchitecture of the graph and contains instance-specific configuration and metadata.\\nMultiple assistants can reference the same graph but can contain different configu‚Äê\\nration and metadata‚Äîwhich may differentiate the behavior of the assistants. An\\nassistant (that is, the graph) is invoked as part of a run.\\nThe LangGraph Platform API provides several endpoints for creating and managing\\nassistants.\\nThreads\\nA thread contains the accumulated state of a group of runs. If a run is executed on\\na thread, then the state of the underlying graph of the assistant will be persisted to\\nthe thread. A thread‚Äôs current and historical state can be retrieved. To persist state, a\\nthread must be created prior to executing a run. The state of a thread at a particular\\npoint in time is called a checkpoint.\\nThe LangGraph Platform API provides several endpoints for creating and managing\\nthreads and thread state.\\nRuns\\nA run is an invocation of an assistant. Each run may have its own input, configura‚Äê\\ntion, and metadata‚Äîwhich may affect the execution and output of the underlying\\ngraph. A run can optionally be executed on a thread.\\nThe LangGraph Platform API provides several endpoints for creating and managing\\nruns.\\nUnderstanding the LangGraph Platform API | 201'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 229, 'page_label': '202'}, page_content='Cron jobs\\nLangGraph Platform supports cron jobs, which enable graphs to be run on a user-\\ndefined schedule. The user specifies a schedule, an assistant, and an input. Then\\nLangGraph Platform creates a new thread with the specified assistant and sends the\\nspecified input to that thread.\\nFeatures\\nThe LangGraph Platform API also offers several features to support complex agent\\narchitectures, including the following:\\n‚Ä¢ Streaming‚Ä¢\\n‚Ä¢ Human-in-the-loop‚Ä¢\\n‚Ä¢ Double texting‚Ä¢\\n‚Ä¢ Stateless runs‚Ä¢\\n‚Ä¢ Webhooks‚Ä¢\\nStreaming\\nStreaming is critical for ensuring that LLM applications feel responsive to end\\nusers. When creating a streaming run, the streaming mode determines what data\\nis streamed back to the API client. The LangGraph Platform API supports five\\nstreaming modes:\\nValues\\nStream the full state of the graph after each super-step is executed.\\nMessages\\nStream complete messages (at the end of node execution) as well as tokens for\\nany messages generated inside a node. This mode is primarily meant for power‚Äê\\ning chat applications. This is only an option if your graph contains a messages\\nkey.\\nUpdates\\nStream updates to the state of the graph after each node is executed.\\nEvents\\nStream all events (including the state of the graph) that occur during graph\\nexecution. This can be used to do token-by-token streaming for LLMs.\\nDebug\\nStream debug events throughout graph execution.\\n202 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 230, 'page_label': '203'}, page_content='Human-in-the-loop\\nIf left to run autonomously, a complex agent can take unintended actions, leading\\nto catastrophic application outcomes. To prevent this, human intervention is recom‚Äê\\nmended, especially at checkpoints where application logic involves invoking certain\\ntools or accessing specific documents. LangGraph Platform enables you to insert this\\nhuman-in-the-loop behavior to ensure your graph doesn‚Äôt have undesired outcomes.\\nDouble texting\\nGraph execution may take longer than expected, and often users may send one message\\nand then, before the graph has finished running, send a second message. This is known\\nas double texting. For example, a user might notice a typo in their original request and\\nedit the prompt and resend it. In such scenarios, it‚Äôs important to prevent your graphs\\nfrom behaving in unexpected ways and ensure a smooth user experience. LangGraph\\nPlatform provides four different solutions to handle double texting:\\nReject\\nThis rejects any follow-up runs and does not allow double texting.\\nEnqueue\\nThis option continues the first run until it completes the whole run, then sends\\nthe new input as a separate run.\\nInterrupt\\nThis option interrupts the current execution but saves all the work done up until\\nthat point. It then inserts the user input and continues from there. If you enable\\nthis option, your graph should be able to handle weird edge cases that may arise.\\nRollback\\nThis option rolls back all work done up until that point. It then sends the user\\ninput in‚Äîas if it just followed the original run input.\\nStateless runs\\nAll runs use the built-in checkpointer to store checkpoints for runs. However, it can\\noften be useful to just kick off a run without worrying about explicitly creating a\\nthread and keeping those checkpointers around. Stateless runs allow you to do this by\\nexposing an endpoint that does these things:\\n‚Ä¢ Takes in user input‚Ä¢\\n‚Ä¢ Creates a thread‚Ä¢\\n‚Ä¢ Runs the agent, but skips all checkpointing steps‚Ä¢\\n‚Ä¢ Cleans up the thread afterwards‚Ä¢\\nUnderstanding the LangGraph Platform API | 203'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 231, 'page_label': '204'}, page_content='Stateless runs are retried while keeping memory intact. However, in the case of\\nstateless background runs, if the task worker dies halfway, the entire run will be\\nretried from scratch.\\nWebhooks\\nLangGraph Platform also supports completion webhooks. A webhook URL is pro‚Äê\\nvided, which notifies your application whenever a run completes.\\nDeploying Your AI Application on LangGraph Platform\\nAt this point, you have created accounts for the recommended services, filled in\\nyour .env file with values of all necessary environment variables, and completed the\\ncore logic for your AI application. Next, we will take the necessary steps to effectively\\ndeploy your application.\\nCreate a LangGraph API Config\\nPrior to deployment, you need to configure your application with a LangGraph API\\nconfiguration file called langgraph.json. Here‚Äôs an example of what the file looks like\\nin a Python repository:\\nPython\\n{\\n    \"dependencies\": [\"./my_agent\"],\\n    \"graphs\": {\\n        \"agent\": \"./my_agent/agent.py:graph\"\\n    },\\n    \"env\": \".env\"\\n}\\nAnd here‚Äôs an example repository structure:\\nmy-app/\\n‚îú‚îÄ‚îÄ my_agent # all project code lies within here\\n‚îÇ   ‚îú‚îÄ‚îÄ utils # utilities for your graph\\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tools.py # tools for your graph\\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nodes.py # node functions for you graph\\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py # state definition of your graph\\n‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt # package dependencies\\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\\n‚îÇ   ‚îî‚îÄ‚îÄ agent.py # code for constructing your graph\\n‚îú‚îÄ‚îÄ .env # environment variables\\n‚îî‚îÄ‚îÄ langgraph.json # configuration file for LangGraph\\nNote that the langgraph.json file is placed on the same level or higher than the files\\nthat contain compiled graphs and associated dependencies.\\n204 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 232, 'page_label': '205'}, page_content='In addition, the dependencies are specified in a requirements.txt file. But they can also\\nbe specified in pyproject.toml, setup.py, or package.json files.\\nHere‚Äôs what each of the properties mean:\\nDependencies\\nArray of dependencies for LangGraph Platform API server\\nGraphs\\nMapping from graph ID to path where the compiled graph or a function that\\nmakes a graph is defined\\nEnv\\nPath to your .env file or a mapping from environment variable to its value (you\\ncan learn more about configurations for the langgraph.json file here)\\nTest Your LangGraph App Locally\\nTesting your application locally ensures that there are no errors or dependency\\nconflicts prior to deployment. To do this, we will utilize the LangGraph CLI, which\\nincludes commands to run a local development server with hot reloading and debug‚Äê\\nging capabilities.\\nFor Python, install the Python langgraph-cli package (note: this requires Python\\n3.11 or higher):\\npip install -U \"langgraph-cli[inmem]\"\\nOr for JavaScript, install the package as follows:\\nnpm i @langchain/langgraph-cli\\nOnce the CLI is installed, run the following command to start the API:\\nlanggraph dev\\nThis will start up the LangGraph API server locally. If this runs successfully, you\\nshould see something like this:\\nReady!\\nAPI: http://localhost:2024\\nDocs: http://localhost:2024/docs\\nThe LangGraph Platform API reference is available with each deployment at the /docs\\nURL path (http://localhost:2024/docs).\\nDeploying Your AI Application on LangGraph Platform | 205'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 233, 'page_label': '206'}, page_content='The easiest way to interact with your local API server is to use the auto-launched\\nLangGraph Studio UI. Alternatively, you can interact with the local API server using\\ncURL, as seen in this example:\\ncurl --request POST \\\\\\n    --url http://localhost:8123/runs/stream \\\\\\n    --header \\'Content-Type: application/json\\' \\\\\\n    --data \\'{\\n    \"assistant_id\": \"agent\",\\n    \"input\": {\\n        \"messages\": [\\n            {\\n                \"role\": \"user\",\\n                \"content\": \"How are you?\"\\n            }\\n        ]\\n    },\\n    \"metadata\": {},\\n    \"config\": {\\n        \"configurable\": {}\\n    },\\n    \"multitask_strategy\": \"reject\",\\n    \"stream_mode\": [\\n        \"values\"\\n    ]\\n}\\'\\nIf you receive a valid response, your application is functioning well. Next, we can\\ninteract with the server using the LangGraph SDK.\\nHere‚Äôs an example both initializing the SDK client and invoking the graph:\\nPython\\nfrom langgraph_sdk import get_client\\n# only pass the url argument to get_client() if you changed the default port \\n# when calling langgraph up\\nclient = get_client()\\n# Using the graph deployed with the name \"agent\"\\nassistant_id = \"agent\"\\nthread = await client.threads.create()\\ninput = {\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s the weather in sf\"}]}\\nasync for chunk in client.runs.stream(\\n    thread[\"thread_id\"],\\n    assistant_id,\\n    input=input,\\n    stream_mode=\"updates\",\\n):\\n    print(f\"Receiving new event of type: {chunk.event}...\")\\n206 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 234, 'page_label': '207'}, page_content='print(chunk.data)\\n    print(\"\\\\n\\\\n\")\\nJavaScript\\nimport { Client } from \"@langchain/langgraph-sdk\";\\n// only set the apiUrl if you changed the default port when calling langgraph up\\nconst client = new Client();\\n// Using the graph deployed with the name \"agent\"\\nconst assistantId = \"agent\";\\nconst thread = await client.threads.create();\\nconst input = {\\n  messages: [{ \"role\": \"user\", \"content\": \"what\\'s the weather in sf\"}]\\n}\\nconst streamResponse = client.runs.stream(\\n  thread[\"thread_id\"],\\n  assistantId,\\n  {\\n    input: input,\\n    streamMode: \"updates\",\\n  }\\n);\\nfor await (const chunk of streamResponse) {\\n  console.log(`Receiving new event of type: ${chunk.event}...`);\\n  console.log(chunk.data);\\n  console.log(\"\\\\n\\\\n\");\\n}\\nIf your LangGraph application is working correctly, you should see your graph output\\ndisplayed in the console.\\nDeploy from the LangSmith UI\\nAt this point, you should have completed all prerequisite steps and your LangGraph\\nAPI should be working locally. Y our next step is to navigate to your LangSmith\\ndashboard panel and click the Deployments tab. Y ou should see a UI similar to\\nFigure 9-6.\\nDeploying Your AI Application on LangGraph Platform | 207'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 235, 'page_label': '208'}, page_content='Figure 9-6. LangGraph Platform deployment UI page\\nNext, click the New Deployment button in the top right corner of the page.\\nIf you don‚Äôt see a page with the New Deployment button, it‚Äôs likely\\nthat you haven‚Äôt yet upgraded to a LangSmith Plus plan according\\nto the instructions in the ‚ÄúUsage and billing‚Äù setting.\\nY ou should now see a page of three form fields to complete.\\nDeployment details\\n1. Select ‚ÄúImport with GitHub‚Äù and follow the GitHub OAuth workflow to install1.\\nand authorize LangChain‚Äôs hosted-langserve GitHub app to access the selected\\nrepositories. After installation is complete, return to the Create New Deployment\\npanel and select the GitHub repository to deploy from the drop-down menu.\\n2. Specify a name for the deployment and the full path to the LangGraph API2.\\nconfig file, including the filename. For example, if the file langgraph.json is in the\\nroot of the repository, simply specify langgraph.json.\\n3. Specify the desired git reference (branch name) of your repository to deploy.3.\\n208 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 236, 'page_label': '209'}, page_content='Development type\\nSelect Production from the dropdown. This will enable a production deployment that\\ncan serve up to 500 requests/second and is provisioned with highly available storage\\nand automatic backups.\\nEnvironment variables\\nProvide the properties and values in your .env here. For sensitive values, like your\\nOPENAI_API_KEY, make sure to tick the Secret box before inputting the value.\\nOnce you‚Äôve completed the fields, click the button to submit the deployment and wait\\nfor a few seconds for the build to complete. Y ou should see a new revision associated\\nwith the deployment.\\nSince LangGraph Platform is integrated within LangSmith, you can gain deeper\\nvisibility into your app and track and monitor usage, errors, performance, and costs\\nin production too. Figure 9-7  shows a visual Trace Count summary chart showing\\nsuccessful, pending, and error traces over a given time period. Y ou can also view all\\nmonitoring info for your server by clicking the ‚Äú All charts‚Äù button.\\nFigure 9-7. Deployment revisions and trace count on dashboard\\nTo view the build and deployment logs, select the desired revision from the Revisions\\ntab, then choose the Deploy tab to view the full deployment logs history. Y ou can also\\nadjust the date and time range.\\nTo create a new deployment, click the New Revision button in the navigation bar. Fill\\nout the necessary fields, including the LangGraph API config file path, git reference,\\nand environment variables, as done previously.\\nFinally, you can access the API documentation by clicking the API docs link, which\\nshould display a similar page to the UI shown in Figure 9-8.\\nDeploying Your AI Application on LangGraph Platform | 209'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 237, 'page_label': '210'}, page_content='Figure 9-8. LangGraph API documentation\\nLaunch LangGraph Studio\\nLangGraph Studio provides a specialized agent IDE for visualizing, interacting with,\\nand debugging complex agentic applications. It enables developers to modify an agent\\nresult (or the logic underlying a specific node) halfway through the agent‚Äôs trajectory.\\nThis creates an iterative process by letting you interact with and manipulate the state\\nat that point in time.\\nOnce you‚Äôve deployed your AI application, click the LangGraph Studio button at the\\ntop righthand corner of the deployment dashboard, as you can see in Figure 9-9.\\nFigure 9-9. LangGraph deployment UI\\n210 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 238, 'page_label': '211'}, page_content='After clicking the button, you should see the LangGraph Studio UI (for example, see\\nFigure 9-10).\\nFigure 9-10. LangGraph Studio UI\\nTo invoke a graph and start a new run, follow these steps:\\n1. Select a graph from the drop-down menu in the top left corner of the lefthand1.\\npane. The graph in Figure 9-10 is called agent.\\n2. In the Input section, click the ‚Äú+ Message‚Äù icon and input a human message, but2.\\nthe input will vary depending on your application state definitions.\\n3. Click Submit to invoke the selected graph.3.\\n4. View the output of the invocation in the right-hand pane.4.\\nThe output of your invoked graph should look like Figure 9-11.\\nDeploying Your AI Application on LangGraph Platform | 211'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 239, 'page_label': '212'}, page_content='Figure 9-11. LangGraph Studio invocation output\\nIn addition to invocation, LangGraph Studio enables you to change run configura‚Äê\\ntions, create and edit threads, interrupt your graphs, edit graph code, and enable\\nhuman-in-the-loop intervention. Y ou can read the full guide to learn more.\\nLangGraph Studio is also available as a desktop application (for\\nApple silicon), which enables you to test your AI application\\nlocally.\\nIf you‚Äôve followed the installation guide in the GitHub template and successfully\\ndeployed your AI application, it‚Äôs now live for production use. But before you share\\nto external users or use the backend API in existing applications, it‚Äôs important to be\\naware of key security considerations.\\n212 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 240, 'page_label': '213'}, page_content='Security\\nAlthough AI applications are powerful, they are vulnerable to several security risks\\nthat may lead to data corruption or loss, unauthorized access to confidential infor‚Äê\\nmation, and compromised performance. These risks may carry adverse legal, reputa‚Äê\\ntional, and financial consequences.\\nTo mitigate these risks, it‚Äôs recommended to follow general application security best\\npractices, including the following:\\nLimit permissions\\nScope permissions specific to the application‚Äôs need. Granting broad or excessive\\npermissions can introduce significant security vulnerabilities. To avoid such vul‚Äê\\nnerabilities, consider using read-only credentials, disallowing access to sensitive\\nresources, and using sandboxing techniques (such as running inside a container).\\nAnticipate potential misuse\\nAlways assume that any system access or credentials may be used in any way\\nallowed by the permissions they are assigned. For example, if a pair of database\\ncredentials allows deleting data, it‚Äôs safest to assume that any LLM able to use\\nthose credentials may in fact delete data.\\nDefense in depth\\nIt‚Äôs often best to combine multiple layered security approaches rather than rely\\non any single layer of defense to ensure security. For example, use both read-only\\npermissions and sandboxing to ensure that LLMs are only able to access data that\\nis explicitly meant for them to use.\\nHere are three example scenarios implementing these mitigation strategies:\\nFile access\\nA user may ask an agent with access to the file system to delete files that should\\nnot be deleted or read the content of files that contain sensitive information. To\\nmitigate this risk, limit the agent to only use a specific directory and only allow it\\nto read or write files that are safe to read or write. Consider further sandboxing\\nthe agent by running it in a container.\\nAPI access\\nA user may ask an agent with write access to an external API to write malicious\\ndata to the API or delete data from that API. To mitigate, give the agent read-only\\nAPI keys, or limit it to only use endpoints that are already resistant to such\\nmisuse.\\nSecurity | 213'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 241, 'page_label': '214'}, page_content='Database access\\nA user may ask an agent with access to a database to drop a table or mutate the\\nschema. To mitigate, scope the credentials to only the tables that the agent needs\\nto access and consider issuing read-only credentials.\\nIn addition to the preceding security measures, you can take further steps to mitigate\\nabuse of your AI application. Due to the dependency of external LLM API providers\\n(such as OpenAI), there is a direct cost associated with running your application. To\\nprevent abuse of your API and exponential costs, you can implement the following:\\nAccount creation verification\\nThis typically includes a form of authentication login, such as email or phone\\nnumber verification.\\nRate limiting\\nImplement a rate-limiting mechanism in the middleware of the application to\\nprevent users from making too many requests in a short period of time. This\\nshould check the number of requests a user has made in the last X minutes and\\n‚Äútimeout‚Äù or ‚Äúban‚Äù the user if the abuse is severe.\\nImplement prompt injection guardrails\\nPrompt injection occurs when a malicious user injects a prompt in an attempt to\\ntrick the LLM to act in unintended ways. This usually includes extracting confi‚Äê\\ndential data or generating unrelated outputs. To mitigate this, you should ensure\\nthe LLM has proper permission scoping and that the application‚Äôs prompts are\\nspecific and strict to the desired outcomes.\\nSummary\\nThroughout this chapter, you‚Äôve learned the best practices for deploying your AI\\napplication and enabling users to interact with it. We explored recommended services\\nto handle various key components of the application in production, including the\\nLLM, vector store, and backend API.\\nWe also discussed using LangGraph Platform as a managed service for deploying\\nand hosting LangGraph agents at scale‚Äîin conjunction with LangGraph Studio‚Äîto\\nvisualize, interact with, and debug your application.\\nFinally, we briefly explored various security best practices to mitigate data breach\\nrisks often associated with AI applications.\\nIn Chapter 10 , you‚Äôll learn how to effectively evaluate, monitor, benchmark, and\\nimprove the performance of your AI application.\\n214 | Chapter 9: Deployment: Launching Your AI Application into Production'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 242, 'page_label': '215'}, page_content='CHAPTER 10\\nTesting: Evaluation, Monitoring, and\\nContinuous Improvement\\nIn Chapter 9 , you learned how to deploy your AI application into production and\\nutilize LangGraph Platform to host and debug your app.\\nAlthough your app can respond to user inputs and execute complex tasks, its under‚Äê\\nlying LLM is nondeterministic and prone to hallucination. As discussed in previous\\nchapters, LLMs can generate inaccurate and outdated outputs due to a variety of rea‚Äê\\nsons including the prompt, format of user‚Äôs input, and retrieved context. In addition,\\nharmful or misleading LLM outputs can significantly damage a company‚Äôs brand and\\ncustomer loyalty.\\nTo combat this tendency toward hallucination, you need to build an efficient sys‚Äê\\ntem to test, evaluate, monitor, and continuously improve your LLM applications‚Äô\\nperformance. This robust testing process will enable you to quickly debug and fix\\nAI-related issues before and after your app is in production.\\nIn this chapter, you‚Äôll learn how to build an iterative testing system across the key\\nstages of the LLM app development life-cycle and maintain high performance of your\\napplication.\\n215'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 243, 'page_label': '216'}, page_content='Testing Techniques Across the LLM App\\nDevelopment Cycle\\nBefore we construct the testing system, let‚Äôs briefly review how testing can be applied\\nacross the three key stages of LLM app development:\\nDesign\\nIn this stage, LLM tests are applied directly to your application. These tests\\ncan be assertions executed at runtime that feed failures back to the LLM for\\nself-correction. The purpose of testing at this stage is error handling within your\\napp before it affects users.\\nPreproduction\\nIn this stage, tests are run right before deployment into production. The purpose\\nof testing at this stage is to catch and fix any regressions before the app is released\\nto real users.\\nProduction\\nIn this stage, tests are run while your application is in production to help monitor\\nand catch errors affecting real users. The purpose is to identify issues and feed\\nthem back into the design or preproduction phases.\\nThe combination of testing across these stages creates a continuous improvement\\ncycle where these steps are repeated: design, test, deploy, monitor, fix, and redesign.\\nSee Figure 10-1.\\nFigure 10-1. The three key stages of the LLM app development cycle\\nIn essence, this cycle helps you to identify and fix production issues in an efficient\\nand quick manner.\\nLet‚Äôs dive deeper into testing techniques across each of these stages.\\n216 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 244, 'page_label': '217'}, page_content='The Design Stage: Self-Corrective RAG\\nAs discussed previously, your application can incorporate error handling at runtime\\nthat feeds errors to the LLM for self-correction. Let‚Äôs explore a RAG use case using\\nLangGraph as the framework to orchestrate error handling.\\nBasic RAG-driven AI applications are prone to hallucination due to inaccurate or\\nincomplete retrieval of relevant context to generate outputs. But you can utilize an\\nLLM to grade retrieval relevance and fix hallucination issues.\\nLangGraph enables you to effectively implement the control flow of this process, as\\nshown in Figure 10-2.\\nFigure 10-2. Self-corrective RAG control flow\\nThe control flow steps are as follows:\\n1. In the routing step, each question is routed to the relevant retrieval method, that1.\\nis, vector store and web search.\\n2. If, for example, the question is routed to a vector store for retrieval, the LLM in2.\\nthe control flow will retrieve and grade the documents for relevancy.\\n3. If the document is relevant, the LLM proceeds to generate an answer.3.\\n4. The LLM will check the answer for hallucinations and only proceed to display4.\\nthe answer to the user if the output is accurate and relevant.\\nThe Design Stage: Self-Corrective RAG | 217'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 245, 'page_label': '218'}, page_content='5. As a fallback, if the retrieved document is irrelevant or the generated answer5.\\ndoesn‚Äôt answer the user‚Äôs question, the flow utilizes web search to retrieve rele‚Äê\\nvant information as context.\\nThis process enables your app to iteratively generate answers, self-correct errors  and\\nhallucinations, and improve the quality of outputs.\\nLet‚Äôs run through an example code implementation of this control flow. First, down‚Äê\\nload the required packages and initialize relevant API keys. For these examples, you‚Äôll\\nneed to set your OpenAI and LangSmith API keys as environment variables.\\nFirst, we‚Äôll create an index of three blog posts:\\nPython\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import InMemoryVectorStore\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom pydantic import BaseModel, Field\\nfrom langchain_openai import ChatOpenAI\\n    \\n    \\n# --- Create an index of documents ---\\n    \\nurls = [\\n    \"https://blog.langchain.dev/top-5-langgraph-agents-in-production-2024/\",\\n    \"https://blog.langchain.dev/langchain-state-of-ai-2024/\",\\n    \"https://blog.langchain.dev/introducing-ambient-agents/\",\\n]\\n    \\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n    \\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=250, chunk_overlap=0\\n)\\ndoc_splits = text_splitter.split_documents(docs_list)\\n    \\n# Add to vectorDB\\nvectorstore = InMemoryVectorStore.from_documents(\\n    documents=doc_splits,\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever()\\n    \\n# Retrieve the relevant documents\\nresults = retriever.invoke(\\n    \"What are 2 LangGraph agents used in production in 2024?\")\\n    \\nprint(\"Results: \\\\n\", results)\\n218 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 246, 'page_label': '219'}, page_content='JavaScript\\nimport { RecursiveCharacterTextSplitter } from \\'@langchain/textsplitters\\';\\nimport {\\n  CheerioWebBaseLoader\\n} from \"@langchain/community/document_loaders/web/cheerio\";\\nimport { \\n  InMemoryVectorStore \\n} from \\'@langchain/community/vectorstores/in_memory\\';\\nimport { OpenAIEmbeddings } from \\'@langchain/openai\\';\\nimport { ChatPromptTemplate } from \\'@langchain/core/prompts\\';\\nimport { z } from \\'zod\\';\\nimport { ChatOpenAI } from \\'@langchain/openai\\';\\n    \\nconst urls = [\\n  \\'https://blog.langchain.dev/top-5-langgraph-agents-in-production-2024/\\',\\n  \\'https://blog.langchain.dev/langchain-state-of-ai-2024/\\',\\n  \\'https://blog.langchain.dev/introducing-ambient-agents/\\',\\n];\\n    \\n// Load documents from URLs\\nconst loadDocs = async (urls) => {\\n  const docs = [];\\n  for (const url of urls) {\\n    const loader = new CheerioWebBaseLoader(url);\\n    const loadedDocs = await loader.load();\\n    docs.push(...loadedDocs);\\n  }\\n  return docs;\\n};\\n    \\nconst docsList = await loadDocs(urls);\\n    \\n// Initialize the text splitter\\nconst textSplitter = new RecursiveCharacterTextSplitter({\\n  chunkSize: 250,\\n  chunkOverlap: 0,\\n});\\n    \\n// Split the documents into smaller chunks\\nconst docSplits = textSplitter.splitDocuments(docsList);\\n    \\n// Add to vector database\\nconst vectorstore = await InMemoryVectorStore.fromDocuments(\\n  docSplits,\\n  new OpenAIEmbeddings()\\n);\\n    \\n// The `retriever` object can now be used for querying\\nconst retriever = vectorstore.asRetriever(); \\n    \\nconst question = \\'What are 2 LangGraph agents used in production in 2024?\\';\\n    \\nThe Design Stage: Self-Corrective RAG | 219'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 247, 'page_label': '220'}, page_content='const docs = retriever.invoke(question);\\n    \\nconsole.log(\\'Retrieved documents: \\\\n\\', docs[0].page_content);\\nAs discussed previously, the LLM will grade the relevancy of the retrieved documents\\nfrom the index. We can construct this instruction in a system prompt:\\nPython\\n### Retrieval Grader\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_openai import ChatOpenAI\\n# Data model\\nclass GradeDocuments(BaseModel):\\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\\n    binary_score: str = Field(\\n        description=\"Documents are relevant to the question, \\'yes\\' or \\'no\\'\"\\n    )\\n# LLM with function call\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\\n# Prompt\\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a\\n    user question.  \\n    If the document contains keyword(s) or semantic meaning related to the \\n    question, grade it as relevant. \\n    Give a binary score \\'yes\\' or \\'no\\' to indicate whether the document is \\n    relevant to the question.\"\"\"\\ngrade_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"\"\"Retrieved document: \\\\n\\\\n {document} \\\\n\\\\n User question: \\n            {question}\"\"\"),\\n    ]\\n)\\nretrieval_grader = grade_prompt | structured_llm_grader\\nquestion = \"agent memory\"\\ndocs = retriever.get_relevant_documents(question)\\ndoc_txt = docs[0].page_content # as an example\\nretrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\\n220 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 248, 'page_label': '221'}, page_content='JavaScript\\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\\nimport { z } from \"zod\";\\nimport { ChatOpenAI } from \"@langchain/openai\";\\n// Define the schema using Zod\\nconst GradeDocumentsSchema = z.object({\\n  binary_score: z.string().describe(`Documents are relevant to the question, \\n      \\'yes\\' or \\'no\\'`),\\n});\\n// Initialize LLM with structured output using Zod schema\\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 });\\nconst structuredLLMGrader = llm.withStructuredOutput(GradeDocumentsSchema);\\n// System and prompt template\\nconst systemMessage = `You are a grader assessing relevance of a retrieved \\n  document to a user question. \\nIf the document contains keyword(s) or semantic meaning related to the \\n  question, grade it as relevant.\\nGive a binary score \\'yes\\' or \\'no\\' to indicate whether the document is relevant \\n  to the question.`;\\nconst gradePrompt = ChatPromptTemplate.fromMessages([\\n  { role: \"system\", content: systemMessage },\\n  {\\n    role: \"human\",\\n    content: \"Retrieved document: \\\\n\\\\n {document} \\\\n\\\\n \\n      User question: {question}\",\\n  },\\n]);\\n// Combine prompt with the structured output\\nconst retrievalGrader = gradePrompt.pipe(structuredLLMGrader);\\nconst question = \"agent memory\";\\nconst docs = await retriever.getRelevantDocuments(question);\\nawait retrievalGrader.invoke({\\n  question,\\n  document: docs[1].pageContent,\\n});\\nThe output:\\nbinary_score=\\'yes\\'\\nNotice the use of Pydantic/Zod to help model the binary decision output in a format\\nthat can be used to programmatically decide which node in the control flow to move\\ntoward.\\nThe Design Stage: Self-Corrective RAG | 221'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 249, 'page_label': '222'}, page_content='In LangSmith, you can see a trace of the logic flow across the nodes discussed\\npreviously (see Figure 10-3).\\nFigure 10-3. LangSmith trace results\\nLet‚Äôs test to see what happens when the input question cannot be answered by the\\nretrieved documents in the index.\\nFirst, utilize LangGraph to make it easier to construct, execute, and debug the full\\ncontrol flow. See the full graph definition in the book‚Äôs GitHub repository. Notice that\\nwe‚Äôve added a transform_query node to help rewrite the input query in a format that\\nweb search can use to retrieve higher-quality results.\\n222 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 250, 'page_label': '223'}, page_content='As a final step, we set up our web search tool and execute the graph using the\\nout-of-context question. The LangSmith trace shows that the web search tool was\\nused as a fallback to retrieve relevant information prior to the final LLM generated\\nanswer (see Figure 10-4).\\nFigure 10-4. LangSmith trace of self-corrective RAG utilizing web search as a fallback\\nLet‚Äôs move on to the next stage in LLM app testing: preproduction.\\nThe Design Stage: Self-Corrective RAG | 223'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 251, 'page_label': '224'}, page_content='The Preproduction Stage\\nThe purpose of the preproduction stage of testing is to measure and evaluate the per‚Äê\\nformance of your application prior to production. This will enable you to efficiently\\nassess the accuracy, latency, and cost of utilizing the LLM.\\nCreating Datasets\\nPrior to testing, you need to define a set of scenarios you‚Äô d like to test and evaluate.\\nA dataset is a collection of examples that provide inputs and expected outputs used to\\nevaluate your LLM app.\\nThese are three common methods to build datasets for valuation:\\nManually curated examples\\nThese are handwritten examples based on expected user inputs and ideal gener‚Äê\\nated outputs. A small dataset consists of between 10 and 50 quality examples.\\nOver time, more examples can be added to the dataset based on edge cases that\\nemerge in production.\\nApplication logs\\nOnce the application is in production, you can store real-time user inputs and\\nlater add them to the dataset. This will help ensure the dataset is realistic and\\ncovers the most common user questions.\\nSynthetic data\\nThese are artificially generated examples that simulate various scenarios and edge\\ncases. This enables you to generate new inputs by sampling existing inputs, which\\nis useful when you don‚Äôt have enough real data to test on.\\nIn LangSmith, you can create a new dataset by selecting Datasets and Testing in the\\nsidebar and clicking the ‚Äú+ New Dataset‚Äù button on the top right of the app, as shown\\nin Figure 10-5.\\nIn the opened window, enter the relevant dataset details, including a name, descrip‚Äê\\ntion, and dataset type. If you‚Äô d like to use your own dataset, click the ‚ÄúUpload a CSV\\ndataset‚Äù button.\\n224 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 252, 'page_label': '225'}, page_content='Figure 10-5. Creating a new dataset in the LangSmith UI\\nLangSmith offers three different dataset types:\\nkv (key-value) dataset\\n‚Ä¢ Inputs and outputs are represented as arbitrary key-value pairs.‚Ä¢\\n‚Ä¢ The kv dataset is the most versatile, and it is the default type. The kv dataset is‚Ä¢\\nsuitable for a wide range of evaluation scenarios.\\n‚Ä¢ This dataset type is ideal for evaluating chains and agents that require multiple‚Ä¢\\ninputs or generate multiple outputs.\\nllm (large language model) dataset\\n‚Ä¢ The llm dataset is designed for evaluating completion style language models.‚Ä¢\\n‚Ä¢ The inputs dictionary contains a single input key mapped to the prompt string.‚Ä¢\\n‚Ä¢ The outputs dictionary contains a single output key mapped to the correspond‚Äê‚Ä¢\\ning response string.\\n‚Ä¢ This dataset type simplifies evaluation for LLMs by providing a standardized‚Ä¢\\nformat for inputs and outputs.\\nThe Preproduction Stage | 225'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 253, 'page_label': '226'}, page_content='chat dataset\\n‚Ä¢ The chat dataset is designed for evaluating LLM structured chat messages as‚Ä¢\\ninputs and outputs.\\n‚Ä¢ The inputs dictionary contains a single input key mapped to a list of serialized‚Ä¢\\nchat messages.\\n‚Ä¢ The outputs dictionary contains a single output key mapped to a list of serialized‚Ä¢\\nchat messages.\\n‚Ä¢ This dataset type is useful for evaluating conversational AI systems or chatbots.‚Ä¢\\nThe most flexible option is the key-value data type (see Figure 10-6).\\nFigure 10-6. Selecting a dataset type in the LangSmith UI\\nNext, add examples to the dataset by clicking Add Example. Provide the input and\\noutput examples as JSON objects, as shown in Figure 10-7.\\n226 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 254, 'page_label': '227'}, page_content='Figure 10-7. Add key-value dataset examples in the LangSmith UI\\nY ou can also define a schema for your dataset in the ‚ÄúDataset schema‚Äù section, as\\nshown in Figure 10-8.\\nFigure 10-8. Adding a dataset schema in the LangSmith UI\\nThe Preproduction Stage | 227'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 255, 'page_label': '228'}, page_content='Defining Your Evaluation Criteria\\nAfter creating your dataset, you need to define evaluation metrics to assess your\\napplication‚Äôs outputs before deploying into production. This batch evaluation on a\\npredetermined test suite is often referred to as offline evaluation.\\nFor offline evaluation, you can optionally label expected outputs (that is, ground\\ntruth references) for the data points you are testing on. This enables you to compare\\nyour application‚Äôs response with the ground truth references, as shown in Figure 10-9.\\nFigure 10-9. AI evaluation diagram\\nThere are three main evaluators to score your LLM app performance:\\nHuman evaluators\\nIf you can‚Äôt express your testing requirements as code, you can use human feed‚Äê\\nback to express qualitative characteristics and label app responses with scores.\\nLangSmith speeds up the process of collecting and incorporating human feed‚Äê\\nback with annotation queues.\\nHeuristic evaluators\\nThese are hardcoded functions and assertions that perform computations to\\ndetermine a score. Y ou can use reference-free heuristics (for example, checking\\nwhether output is valid JSON) or reference-based heuristics such as accuracy.\\nReference-based evaluation compares an output to a predefined ground truth,\\nwhereas reference-free evaluation assesses qualitative characteristics without a\\nground truth. Custom heuristic evaluators are useful for code-generation tasks\\nsuch as schema checking and unit testing with hardcoded evaluation logic.\\nLLM-as-a-judge evaluators\\nThis evaluator integrates human grading rules into an LLM prompt to evaluate\\nwhether the output is correct relative to the reference answer supplied from the\\ndataset output. As you iterate in preproduction, you‚Äôll need to audit the scores\\nand tune the LLM-as-a-judge to produce reliable scores.\\n228 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 256, 'page_label': '229'}, page_content='To get started with evaluation, start simple with heuristic evaluators. Then implement\\nhuman evaluators before moving on to LLM-as-a-judge to automate your human\\nreview. This enables you to add depth and scale once your criteria are well-defined.\\nWhen using LLM-as-a-judge evaluators, use straightforward\\nprompts that can easily be replicated and understood by a human.\\nFor example, avoid asking an LLM to produce scores on a range of\\n0 to 10 with vague distinctions between scores.\\nFigure 10-10 illustrates LLM-as-a-judge evaluator in the context of a RAG use case.\\nNote that the reference answer is the ground truth.\\nFigure 10-10. LLM-as-a-judge evaluator used in a RAG use case\\nImproving LLM-as-a-judge evaluators performance\\nUsing an LLM-as-a-judge is an effective method to grade natural language outputs\\nfrom LLM applications. This involves passing the generated output to a separate LLM\\nfor judgment and evaluation. But how can you trust the results of LLM-as-a-judge\\nevaluation?\\nOften, rounds of prompt engineering are required to improve accuracy, which\\nis cumbersome and time-consuming. Fortunately, LangSmith  provides a few-shot\\nprompt solution whereby human corrections to LLM-as-a-judge outputs are stored\\nas few-shot examples, which are then fed back into the prompt in future iterations.\\nThe Preproduction Stage | 229'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 257, 'page_label': '230'}, page_content='By utilizing few-shot learning, the LLM can improve accuracy and align outputs\\nwith human preferences by providing examples of correct behavior. This is especially\\nuseful when it‚Äôs difficult to construct instructions on how the LLM should behave or\\nbe formatted.\\nThe few-shot evaluator follows these steps:\\n1. The LLM evaluator provides feedback on generated outputs, assessing factors1.\\nsuch as correctness, relevance, or other criteria.\\n2. It adds human corrections to modify or correct the LLM evaluator‚Äôs feedback in2.\\nLangSmith. This is where human preferences and judgment are captured.\\n3. These corrections are stored as few-shot examples in LangSmith, with an option3.\\nto leave explanations for corrections.\\n4. The few-shot examples are incorporated into future prompts as subsequent eval‚Äê4.\\nuation runs.\\nOver time, the few-shot evaluator will become increasingly aligned with human\\npreferences. This self-improving mechanism reduces the need for time-consuming\\nprompt engineering, while improving the accuracy and relevance of LLM-as-a-judge\\nevaluations.\\nHere‚Äôs how to easily set up the LLM-as-a-judge evaluator in LangSmith for offline\\nevaluation. First, navigate to the ‚ÄúDatasets and Testing‚Äù section in the sidebar and\\nselect the dataset you want to configure the evaluator for. Click the Add Auto-\\nEvaluator button at the top right of the dashboard to add an evaluator to the dataset.\\nThis will open a modal you can use to configure the evaluator.\\nSelect the LLM-as-a-judge option and give your evaluator a name. Y ou will now have\\nthe option to set an inline prompt or load a prompt from the prompt hub that will\\nbe used to evaluate the results of the runs in the experiment. For  the sake of this\\nexample, choose the Create Few-Shot Evaluator option, as shown in Figure 10-11.\\n230 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 258, 'page_label': '231'}, page_content='Figure 10-11. LangSmith UI options for the LLM-as-a-judge evaluator\\nThis option will create a dataset that holds few-shot examples that will autopopulate\\nwhen you make corrections on the evaluator feedback. The examples in this dataset\\nwill be inserted in the system prompt message.\\nThe Preproduction Stage | 231'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 259, 'page_label': '232'}, page_content='Y ou can also specify the scoring criteria in the Schema field and toggle between\\nprimitive types‚Äîfor example, integer and Boolean (see Figure 10-12).\\nFigure 10-12. LLM-as-a-judge evaluator scoring criteria\\nSave the evaluator and navigate back to the dataset details page. Moving forward,\\neach subsequent experiment run from the dataset will be evaluated by the evaluator\\nyou configured.\\n232 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 260, 'page_label': '233'}, page_content='Pairwise evaluation\\nRanking LLM outputs by preference can be less cognitively demanding for human or\\nLLM-as-a-judge evaluators. For example, assessing which output is more informative,\\nspecific, or safe. Pairwise evaluation compares two outputs simultaneously from dif‚Äê\\nferent versions of an application to determine which version better meets evaluation\\ncriteria.\\nLangSmith natively supports running and visualizing pairwise LLM app generations,\\nhighlighting preference for one generation over another based on guidelines set\\nby the pairwise evaluator. LangSmith‚Äôs pairwise evaluation enables you to do the\\nfollowing:\\n‚Ä¢ Define a custom pairwise LLM-as-a-judge evaluator using any desired criteria‚Ä¢\\n‚Ä¢ Compare two LLM generations using this evaluator‚Ä¢\\nAs per the LangSmith docs, you can use custom pairwise evaluators in the LangSmith\\nSDK and visualize the results of pairwise evaluations in the LangSmith UI.\\nAfter creating an evaluation experiment, you can navigate to the Pairwise Experi‚Äê\\nments tab in the Datasets & Experiments section. The UI enables you to dive into\\neach pairwise experiment, showing which LLM generation is preferred based upon\\nour criteria. If you click the RANKED_PREFERENCE score under each answer, you\\ncan dive deeper into each evaluation trace (see Figure 10-13).\\nFigure 10-13. Pairwise experiment UI evaluation trace\\nThe Preproduction Stage | 233'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 261, 'page_label': '234'}, page_content='Regression Testing\\nIn traditional software development, tests are expected to pass 100% based on func‚Äê\\ntional requirements. This ensures stable behavior once the test is validated. In con‚Äê\\ntrast, however, AI models‚Äô output performances can vary significantly due to model\\ndrift (degradation due to changes in data distribution or updates to the model). As a\\nresult, testing AI applications may not always lead to a perfect score on the evaluation\\ndataset.\\nThis has several implications. First, it‚Äôs important to track results and performance\\nof your tests over time to prevent regression of your app‚Äôs performance. Regression\\ntesting ensures that the latest updates or changes of the LLM model of your app do\\nnot regress (perform worse) relative to the baseline.\\nSecond, it‚Äôs crucial to compare the individual data points between two or more\\nexperimental runs to see where the model got it right or wrong.\\nLangSmith‚Äôs comparison view has native support for regression testing, allowing\\nyou to quickly see examples that have changed relative to the baseline. Runs that\\nregressed or improved are highlighted differently in the LangSmith dashboard (see\\nFigure 10-14).\\nFigure 10-14. LangSmith‚Äôs experiments comparison view\\n234 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 262, 'page_label': '235'}, page_content='In LangSmith‚Äôs Comparing Experiments dashboard, you can do the following:\\n‚Ä¢ Compare multiple experiments and runs associated with a dataset. Aggregate‚Ä¢\\nstats of runs is useful for migrating models or prompts, which may result in\\nperformance improvements or regression on specific examples.\\n‚Ä¢ Set a baseline run and compare it against prior app versions to detect unexpected‚Ä¢\\nregressions. If a regression occurs, you can isolate both the app version and the\\nspecific examples that contain performance changes.\\n‚Ä¢ Drill into data points that behaved differently between compared experiments‚Ä¢\\nand runs.\\nThis regression testing is crucial to ensure that your application maintains high\\nperformance over time regardless of updates and LLM changes.\\nNow that we‚Äôve covered various preproduction testing strategies, let‚Äôs explore a spe‚Äê\\ncific use case.\\nEvaluating an Agent‚Äôs End-to-End Performance\\nAlthough agents show a lot of promise in executing autonomous tasks and work‚Äê\\nflows, testing an agent‚Äôs performance can be challenging. In previous chapters, you\\nlearned how agents use tool calling with planning and memory to generate responses.\\nIn particular, tool calling enables the model to respond to a given prompt by generat‚Äê\\ning a tool to invoke and the input arguments required to execute the tool.\\nSince agents use an LLM to decide the control flow of the application, each agent\\nrun can have significantly different outcomes. For example, different tools might be\\ncalled, agents might get stuck in a loop, or the number of steps from start to finish\\ncan vary significantly.\\nIdeally, agents should be tested at three different levels of granularity:\\nResponse\\nThe agent‚Äôs final response to focus on the end-to-end performance. The inputs\\nare a prompt and an optional list of tools, whereas the output is the final agent\\nresponse.\\nSingle step\\nAny single, important step of the agent to drill into specific tool calls or decisions.\\nIn this case, the output is a tool call.\\nTrajectory\\nThe full trajectory of the agent. In this case, the output is the list of tool calls.\\nThe Preproduction Stage | 235'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 263, 'page_label': '236'}, page_content='Figure 10-15 illustrates these levels:\\nFigure 10-15. An example of an agentic app‚Äôs flow\\nLet‚Äôs dive deeper into each of these three agent-testing granularities.\\nTesting an agent‚Äôs final response\\nIn order to assess the overall performance of an agent on a task, you can treat the\\nagent as a black box and define success based on whether or not it completes the task.\\nTesting for the agent‚Äôs final response typically involves the following:\\nInputs\\nUser input and (optionally) predefined tools\\nOutput\\nAgent‚Äôs final response\\nEvaluator\\nLLM-as-a-judge\\nTo implement this in a programmatic manner, first create a dataset that includes\\nquestions and expected answers from the agent:\\nPython\\nfrom langsmith import Client\\nclient = Client()\\n# Create a dataset\\nexamples = [\\n236 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 264, 'page_label': '237'}, page_content='(\"Which country\\'s customers spent the most? And how much did they spend?\",\\n        \"\"\"The country whose customers spent the most is the USA, with a total \\n        expenditure of $523.06\"\"\"),\\n    (\"What was the most purchased track of 2013?\", \\n        \"The most purchased track of 2013 was Hot Girl.\"),\\n    (\"How many albums does the artist Led Zeppelin have?\",\\n        \"Led Zeppelin has 14 albums\"),\\n    (\"What is the total price for the album ‚ÄúBig Ones‚Äù?\",\\n        \"The total price for the album \\'Big Ones\\' is 14.85\"),\\n    (\"Which sales agent made the most in sales in 2009?\", \\n        \"Steve Johnson made the most sales in 2009\"),\\n]\\ndataset_name = \"SQL Agent Response\"\\nif not client.has_dataset(dataset_name=dataset_name):\\n    dataset = client.create_dataset(dataset_name=dataset_name)\\n    inputs, outputs = zip(\\n        *[({\"input\": text}, {\"output\": label}) for text, label in examples]\\n    )\\n    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\\n## chain\\ndef predict_sql_agent_answer(example: dict):\\n    \"\"\"Use this for answer evaluation\"\"\"\\n    msg = {\"messages\": (\"user\", example[\"input\"])}\\n    messages = graph.invoke(msg, config)\\n    return {\"response\": messages[\\'messages\\'][-1].content}\\nJavaScript\\nimport { Client } from \\'langsmith\\';\\nconst client = new Client();\\n// Create a dataset\\nconst examples = [\\n  [\"Which country\\'s customers spent the most? And how much did they spend?\", \\n    `The country whose customers spent the most is the USA, with a total \\n    expenditure of $523.06`],\\n  [\"What was the most purchased track of 2013?\", \\n    \"The most purchased track of 2013 was Hot Girl.\"],\\n  [\"How many albums does the artist Led Zeppelin have?\", \\n    \"Led Zeppelin has 14 albums\"],\\n  [\"What is the total price for the album \\'Big Ones\\'?\", \\n    \"The total price for the album \\'Big Ones\\' is 14.85\"],\\n  [\"Which sales agent made the most in sales in 2009?\", \\n    \"Steve Johnson made the most sales in 2009\"],\\n];\\nconst datasetName = \"SQL Agent Response\";\\nasync function createDataset() {\\n  const hasDataset = await client.hasDataset({ datasetName });\\nThe Preproduction Stage | 237'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 265, 'page_label': '238'}, page_content='if (!hasDataset) {\\n    const dataset = await client.createDataset(datasetName);\\n    const inputs = examples.map(([text]) => ({ input: text }));\\n    const outputs = examples.map(([, label]) => ({ output: label }));\\n    await client.createExamples({ inputs, outputs, datasetId: dataset.id });\\n  }\\n}\\ncreateDataset();\\n// Chain function\\nasync function predictSqlAgentAnswer(example) {\\n  // Use this for answer evaluation\\n  const msg = { messages: [{ role: \"user\", content: example.input }] };\\n  const output = await graph.invoke(msg, config);\\n  return { response: output.messages[output.messages.length - 1].content };\\n}\\nNext, as discussed earlier, we can utilize the LLM to compare the generated answer\\nwith the reference answer:\\nPython\\nfrom langchain import hub\\nfrom langchain_openai import ChatOpenAI\\nfrom langsmith.evaluation import evaluate\\n# Grade prompt\\ngrade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\\ndef answer_evaluator(run, example) -> dict:\\n    \"\"\"\\n    A simple evaluator for RAG answer accuracy\\n    \"\"\"\\n    # Get question, ground truth answer, RAG chain answer\\n    input_question = example.inputs[\"input\"]\\n    reference = example.outputs[\"output\"]\\n    prediction = run.outputs[\"response\"]\\n    # LLM grader\\n    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\\n    # Structured prompt\\n    answer_grader = grade_prompt_answer_accuracy | llm\\n    # Run evaluator\\n    score = answer_grader.invoke({\"question\": input_question,\\n                                  \"correct_answer\": reference,\\n                                  \"student_answer\": prediction})\\n    score = score[\"Score\"]\\n238 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 266, 'page_label': '239'}, page_content='return {\"key\": \"answer_v_reference_score\", \"score\": score}\\n## Run evaluation\\nexperiment_results = evaluate(\\n    predict_sql_agent_answer,\\n    data=dataset_name,\\n    evaluators=[answer_evaluator],\\n    num_repetitions=3,\\n)\\nJavaScript\\nimport { pull } from \"langchain/hub\";\\nimport { ChatOpenAI } from \"langchain_openai\";\\nimport { evaluate } from \"langsmith/evaluation\";\\nasync function answerEvaluator(run, example) {\\n  /**\\n   * A simple evaluator for RAG answer accuracy\\n   */\\n  // Get question, ground truth answer, RAG chain answer\\n  const inputQuestion = example.inputs[\"input\"];\\n  const reference = example.outputs[\"output\"];\\n  const prediction = run.outputs[\"response\"];\\n  // LLM grader\\n  const llm = new ChatOpenAI({ model: \"gpt-4o\", temperature: 0 });\\n  // Grade prompt \\n  const gradePromptAnswerAccuracy = pull(\\n    \"langchain-ai/rag-answer-vs-reference\"\\n  );\\n  // Structured prompt\\n  const answerGrader = gradePromptAnswerAccuracy.pipe(llm);\\n  // Run evaluator\\n  const scoreResult = await answerGrader.invoke({\\n    question: inputQuestion,\\n    correct_answer: reference,\\n    student_answer: prediction\\n  });\\n  const score = scoreResult[\"Score\"];\\n  return { key: \"answer_v_reference_score\", score: score };\\n}\\n// Run evaluation\\nconst experimentResults = evaluate(predictSqlAgentAnswer, {\\nThe Preproduction Stage | 239'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 267, 'page_label': '240'}, page_content='data: datasetName,\\n  evaluators: [answerEvaluator],\\n  numRepetitions: 3,\\n});\\nTesting a single step of an agent\\nTesting an agent‚Äôs individual action or decision enables you to identify and analyze\\nspecifically where your application is underperforming. Testing for a single step of an\\nagent involves the following:\\nInputs\\nUser input to a single step (for example, user prompt, set of tools). This can also\\ninclude previously completed steps.\\nOutput\\nLLM response from the inputs step, which often contains tool calls indicating\\nwhat action the agent should take next.\\nEvaluator\\nBinary score for correct tool selection and heuristic assessment of the tool input‚Äôs\\naccuracy.\\nThe following example checks a specific tool call using a custom evaluator:\\nPython\\nfrom langsmith.schemas import Example, Run\\ndef predict_assistant(example: dict):\\n    \"\"\"Invoke assistant for single tool call evaluation\"\"\"\\n    msg = [ (\"user\", example[\"input\"]) ]\\n    result = assistant_runnable.invoke({\"messages\":msg})\\n    return {\"response\": result}\\ndef check_specific_tool_call(root_run: Run, example: Example) -> dict:\\n    \"\"\"\\n    Check if the first tool call in the response matches the expected tool call.\\n    \"\"\"\\n    # Expected tool call\\n    expected_tool_call = \\'sql_db_list_tables\\'\\n    # Run\\n    response = root_run.outputs[\"response\"]\\n    # Get tool call\\n    try:\\n        tool_call = getattr(response, \\'tool_calls\\', [])[0][\\'name\\']\\n    except (IndexError, KeyError):\\n        tool_call = None\\n240 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 268, 'page_label': '241'}, page_content='score = 1 if tool_call == expected_tool_call else 0\\n    return {\"score\": score, \"key\": \"single_tool_call\"}\\nexperiment_results = evaluate(\\n    predict_assistant,\\n    data=dataset_name,\\n    evaluators=[check_specific_tool_call],\\n    num_repetitions=3,\\n    metadata={\"version\": metadata},\\n)\\nJavaScript\\nimport {evaluate} from \\'langsmith/evaluation\\';\\n// Predict Assistant\\nfunction predictAssistant(example) {\\n    /**\\n     * Invoke assistant for single tool call evaluation\\n     */\\n    const msg = [{ role: \"user\", content: example.input }];\\n    const result = assistantRunnable.invoke({ messages: msg });\\n    return { response: result };\\n}\\n// Check Specific Tool Call\\nfunction checkSpecificToolCall(rootRun, example) {\\n    /**\\n     * Check if the first tool call in the response matches the expected \\n     * tool call.\\n     */\\n    // Expected tool call\\n    const expectedToolCall = \"sql_db_list_tables\";\\n    // Run\\n    const response = rootRun.outputs.response;\\n    // Get tool call\\n    let toolCall;\\n    try {\\n        toolCall = response.tool_calls?.[0]?.name;\\n    } catch (error) {\\n        toolCall = null;\\n    }\\n    const score = toolCall === expectedToolCall ? 1 : 0;\\n    return { score, key: \"single_tool_call\" };\\n}\\n// Experiment Results\\nconst experimentResults = evaluate(predictAssistant, {\\nThe Preproduction Stage | 241'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 269, 'page_label': '242'}, page_content='data: datasetName,\\n    evaluators: [checkSpecificToolCall],\\n    numRepetitions: 3,\\n});\\nThe preceding code block implements these distinct components:\\n‚Ä¢ Invoke the assistant, assistant_runnable, with a prompt and check if the result‚Äê‚Ä¢\\ning tool call is as expected.\\n‚Ä¢ Utilize a specialized agent where the tools are hardcoded rather than passed with‚Ä¢\\nthe dataset input.\\n‚Ä¢ Specify the reference tool call for the step that we are evaluating for‚Ä¢\\nexpected_tool_call.\\nTesting an agent‚Äôs trajectory\\nIt‚Äôs important to look back on the steps an agent took in order to assess whether or\\nnot the trajectory lined up with expectations of the agent‚Äîthat is, the number of\\nsteps or sequence of steps taken.\\nTesting an agent‚Äôs trajectory involves the following:\\nInputs\\nUser input and (optionally) predefined tools.\\nOutput\\nExpected sequence of tool calls or a list of tool calls in any order.\\nEvaluator\\nFunction over the steps taken. To test the outputs, you can look at an exact match\\nbinary score or metrics that focus on the number of incorrect steps. Y ou‚Äô d need to\\nevaluate the full agent‚Äôs trajectory against a reference trajectory and then compile\\nas a set of messages to pass into the LLM-as-a-judge.\\nThe following example assesses the trajectory of tool calls using custom evaluators:\\nPython\\ndef predict_sql_agent_messages(example: dict):\\n    \"\"\"Use this for answer evaluation\"\"\"\\n    msg = {\"messages\": (\"user\", example[\"input\"])}\\n    messages = graph.invoke(msg, config)\\n    return {\"response\": messages}\\ndef find_tool_calls(messages):\\n    \"\"\"\\n    Find all tool calls in the messages returned\\n    \"\"\"\\n    tool_calls = [\\n242 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 270, 'page_label': '243'}, page_content='tc[\\'name\\']\\n        for m in messages[\\'messages\\'] for tc in getattr(m, \\'tool_calls\\', [])\\n    ]\\n    return tool_calls\\ndef contains_all_tool_calls_any_order(\\n    root_run: Run, example: Example\\n) -> dict:\\n    \"\"\"\\n    Check if all expected tools are called in any order.\\n    \"\"\"\\n    expected = [\\n        \\'sql_db_list_tables\\',\\n        \\'sql_db_schema\\',\\n        \\'sql_db_query_checker\\',\\n        \\'sql_db_query\\',\\n        \\'check_result\\'\\n    ]\\n    messages = root_run.outputs[\"response\"]\\n    tool_calls = find_tool_calls(messages)\\n    # Optionally, log the tool calls -\\n    #print(\"Here are my tool calls:\")\\n    #print(tool_calls)\\n    if set(expected) <= set(tool_calls):\\n        score = 1\\n    else:\\n        score = 0\\n    return {\"score\": int(score), \"key\": \"multi_tool_call_any_order\"}\\ndef contains_all_tool_calls_in_order(root_run: Run, example: Example) -> dict:\\n    \"\"\"\\n    Check if all expected tools are called in exact order.\\n    \"\"\"\\n    messages = root_run.outputs[\"response\"]\\n    tool_calls = find_tool_calls(messages)\\n    # Optionally, log the tool calls -\\n    #print(\"Here are my tool calls:\")\\n    #print(tool_calls)\\n    it = iter(tool_calls)\\n    expected = [\\n        \\'sql_db_list_tables\\', \\n        \\'sql_db_schema\\', \\n        \\'sql_db_query_checker\\',\\n        \\'sql_db_query\\', \\n        \\'check_result\\'\\n    ]\\n    if all(elem in it for elem in expected):\\n        score = 1\\n    else:\\n        score = 0\\n    return {\"score\": int(score), \"key\": \"multi_tool_call_in_order\"}\\nThe Preproduction Stage | 243'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 271, 'page_label': '244'}, page_content='def contains_all_tool_calls_in_order_exact_match(\\n    root_run: Run, example: Example\\n) -> dict:\\n    \"\"\"\\n    Check if all expected tools are called in exact order and without any \\n        additional tool calls.\\n    \"\"\"\\n    expected = [\\n        \\'sql_db_list_tables\\',\\n        \\'sql_db_schema\\',\\n        \\'sql_db_query_checker\\',\\n        \\'sql_db_query\\',\\n        \\'check_result\\'\\n    ]\\n    messages = root_run.outputs[\"response\"]\\n    tool_calls = find_tool_calls(messages)\\n    # Optionally, log the tool calls -\\n    #print(\"Here are my tool calls:\")\\n    #print(tool_calls)\\n    if tool_calls == expected:\\n        score = 1\\n    else:\\n        score = 0\\n    return {\"score\": int(score), \"key\": \"multi_tool_call_in_exact_order\"}\\nexperiment_results = evaluate(\\n    predict_sql_agent_messages,\\n    data=dataset_name,\\n    evaluators=[\\n        contains_all_tool_calls_any_order,\\n        contains_all_tool_calls_in_order,\\n        contains_all_tool_calls_in_order_exact_match\\n    ],\\n    num_repetitions=3,\\n)\\nJavaScript\\nimport {evaluate} from \\'langsmith/evaluation\\';\\n// Predict SQL Agent Messages\\nfunction predictSqlAgentMessages(example) {\\n  /**\\n   * Use this for answer evaluation\\n   */\\n  const msg = { messages: [{ role: \"user\", content: example.input }] };\\n  // Replace with your graph and config\\n  const messages = graph.invoke(msg, config); \\n  return { response: messages };\\n}\\n// Find Tool Calls\\n244 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 272, 'page_label': '245'}, page_content='function findToolCalls({messages}) {\\n  /**\\n   * Find all tool calls in the messages returned\\n   */\\n  return messages.flatMap(m => m.tool_calls?.map(tc => tc.name) || []);\\n}\\n// Contains All Tool Calls (Any Order)\\nfunction containsAllToolCallsAnyOrder(rootRun, example) {\\n  /**\\n   * Check if all expected tools are called in any order.\\n   */\\n  const expected = [\\n    \"sql_db_list_tables\",\\n    \"sql_db_schema\",\\n    \"sql_db_query_checker\",\\n    \"sql_db_query\",\\n    \"check_result\"\\n  ];\\n  const messages = rootRun.outputs.response;\\n  const toolCalls = findToolCalls(messages);\\n  const score = expected.every(tool => toolCalls.includes(tool)) ? 1 : 0;\\n  return { score, key: \"multi_tool_call_any_order\" };\\n}\\n// Contains All Tool Calls (In Order)\\nfunction containsAllToolCallsInOrder(rootRun, example) {\\n  /**\\n   * Check if all expected tools are called in exact order.\\n   */\\n  const messages = rootRun.outputs.response;\\n  const toolCalls = findToolCalls(messages);\\n  const expected = [\\n    \"sql_db_list_tables\",\\n    \"sql_db_schema\",\\n    \"sql_db_query_checker\",\\n    \"sql_db_query\",\\n    \"check_result\"\\n  ];\\n  const score = expected.every(tool => {\\n    let found = false;\\n    for (let call of toolCalls) {\\n      if (call === tool) {\\n          found = true;\\n          break;\\n      }\\n    }\\n    return found;\\n  }) ? 1 : 0;\\nThe Preproduction Stage | 245'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 273, 'page_label': '246'}, page_content='return { score, key: \"multi_tool_call_in_order\" };\\n}\\n// Contains All Tool Calls (Exact Order, Exact Match)\\nfunction containsAllToolCallsInOrderExactMatch(rootRun, example) {\\n  /**\\n   * Check if all expected tools are called in exact order and without any \\n   * additional tool calls.\\n   */\\n  const expected = [\\n    \"sql_db_list_tables\",\\n    \"sql_db_schema\",\\n    \"sql_db_query_checker\",\\n    \"sql_db_query\",\\n    \"check_result\"\\n  ];\\n  const messages = rootRun.outputs.response;\\n  const toolCalls = findToolCalls(messages);\\n  const score = JSON.stringify(toolCalls) === JSON.stringify(expected) \\n    ? 1 \\n    : 0;\\n  return { score, key: \"multi_tool_call_in_exact_order\" };\\n}\\n// Experiment Results\\nconst experimentResults = evaluate(predictSqlAgentMessages, {\\n  data: datasetName,\\n  evaluators: [\\n    containsAllToolCallsAnyOrder,\\n    containsAllToolCallsInOrder,\\n    containsAllToolCallsInOrderExactMatch\\n  ],\\n  numRepetitions: 3,\\n});\\nThis implementation example includes the following:\\n‚Ä¢ Invoking a precompiled LangGraph agent graph.invoke with a prompt‚Ä¢\\n‚Ä¢ Utilizing a specialized agent where the tools are hardcoded rather than passed‚Ä¢\\nwith the dataset input\\n‚Ä¢ Extracting of the list of tools called using the function find_tool_calls‚Ä¢\\n‚Ä¢ Checking if all expected tools are called in any order using the‚Ä¢\\nfunction contains_all_tool_calls_any_order or called in order using\\ncontains_all_tool_calls_in_order\\n‚Ä¢ Checking whether all expected tools are called in the exact order using‚Ä¢\\ncontains_all_tool_calls_in_order_exact_match\\n246 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 274, 'page_label': '247'}, page_content='All three of these agent evaluation methods can be observed and debugged in\\nLangSmith‚Äôs experimentation UI (see Figure 10-16).\\nFigure 10-16. Example of an agent evaluation test in the LangSmith UI\\nIn general, these tests are a solid starting point to help mitigate an agent‚Äôs cost and\\nunreliability due to LLM invocations and variability in tool calling.\\nProduction\\nAlthough testing in the preproduction phase is useful, certain bugs and edge cases\\nmay not emerge until your LLM application interacts with live users. These issues\\ncan affect latency, as well as the relevancy and accuracy of outputs. In addition,\\nobservability and the process of online evaluation can help ensure that there are\\nguardrails for LLM inputs or outputs. These guardrails can provide much-needed\\nprotection from prompt injection and toxicity.\\nThe first step in this process is to set up LangSmith‚Äôs tracing feature.\\nTracing\\nA trace is a series of steps that your application takes to go from input to output.\\nLangSmith makes it easy to visualize, debug, and test each trace generated from your\\napp.\\nOnce you‚Äôve installed the relevant LangChain and LLM dependencies, all you need to\\ndo is configure the tracing environment variables based on your LangSmith account\\ncredentials:\\nexport LANGCHAIN_TRACING_V2=true\\nexport LANGCHAIN_API_KEY=<your-api-key>\\nProduction | 247'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 275, 'page_label': '248'}, page_content='# The below examples use the OpenAI API, though you can use other LLM providers\\nexport OPENAI_API_KEY=<your-openai-api-key>\\nAfter the environment variables are set, no other code is required to enable tracing.\\nTraces will be automatically logged to their specific project in the ‚ÄúTracing projects‚Äù\\nsection of the LangSmith dashboard. The metrics provided include trace volume,\\nsuccess and failure rates, latency, token count and cost, and more‚Äîas shown in\\nFigure 10-17.\\nFigure 10-17. An example of LangSmith‚Äôs trace performance metrics\\nY ou can review a variety of strategies to implement tracing based on your needs.\\nCollect Feedback in Production\\nUnlike the preproduction phase, evaluators for production testing don‚Äôt have groun‚Äê\\nded reference responses for the LLM to compare against. Instead, evaluators need\\nto score performance in real time as your application processes user inputs. This\\nreference-free, real-time evaluation is often referred to as online evaluation.\\n248 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 276, 'page_label': '249'}, page_content='There are at least two types of feedback you can collect in production to improve app\\nperformance:\\nFeedback from users\\nY ou can directly collect user feedback explicitly or implicitly. For example, giving\\nusers the ability to click a like and dislike button or provide detailed feedback\\nbased on the application‚Äôs output is an effective way to track user satisfaction. In\\nLangSmith, you can attach user feedback to any trace or intermediate run (that is,\\nspan) of a trace, including annotating traces inline or reviewing runs together in\\nan annotation queue.\\nFeedback from LLM-as-a judge evaluators\\nAs discussed previously, these evaluators can be implemented directly on traces\\nto identify hallucination and toxic responses.\\nThe earlier preproduction section already discussed how to set up LangSmith‚Äôs auto\\nevaluation in the Datasets & Experiments section of the dashboard.\\nClassification and Tagging\\nIn order to implement effective guardrails against toxicity or gather insights on user\\nsentiment analysis, we need to build an effective system for labeling user inputs and\\ngenerated outputs.\\nThis system is largely dependent on whether or not you have a dataset that contains\\nreference labels. If you don‚Äôt have preset labels, you can use the LLM-as-a-judge\\nevaluator to assist in performing classification and tagging based upon specified\\ncriteria.\\nIf, however, ground truth classification labels are provided, then a custom heuristic\\nevaluator can be used to score the chain‚Äôs output relative to the ground truth class\\nlabels.\\nMonitoring and Fixing Errors\\nOnce your application is in production, LangSmith‚Äôs tracing will catch errors and\\nedge cases. Y ou can add these errors into your test dataset for offline evaluation in\\norder to prevent recurrences of the same issues.\\nAnother useful strategy is to release your app in phases to a small group of beta\\nusers before a larger audience can access its features. This will enable you to uncover\\ncrucial bugs, develop a solid evaluation dataset with ground truth references, and\\nassess the general performance of the app including cost, latency, and quality of\\noutputs.\\nProduction | 249'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 277, 'page_label': '250'}, page_content='Summary\\nAs discussed in this chapter, robust testing is crucial to ensure that your LLM appli‚Äê\\ncation is accurate, reliable, fast, toxic-free, and cost-efficient. The three key stages\\nof LLM app development create a data cycle that helps to ensure high performance\\nthroughout the lifetime of the application.\\nDuring the design phase, in-app error handling enables self-correction before the\\nerror reaches the user. Preproduction testing ensures each of your app‚Äôs updates\\navoids regression in performance metrics. Finally, production monitoring gathers\\nreal-time insights and application errors that inform the subsequent design process\\nand the cycle repeats.\\nUltimately, this process of testing, evaluation, monitoring, and continuous improve‚Äê\\nment, will help you fix issues and iterate faster, and most importantly, deliver a\\nproduct that users can trust to consistently deliver their desired results.\\n250 | Chapter 10: Testing: Evaluation, Monitoring, and Continuous Improvement'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 278, 'page_label': '251'}, page_content='CHAPTER 11\\nBuilding with LLMs\\nOne of the biggest open questions in the world of LLMs today is how to best put\\nthem in the hands of end users. In some ways, LLMs are actually a more intuitive\\ninterface for computing than what came before them. They are much more forgiving\\nof typos, slips of the tongue, and the general imprecision of humans, when compared\\nto traditional computer applications. On the other hand, the very ability to handle\\ninputs that are ‚Äúslightly off ‚Äù comes with a tendency to sometimes produce results\\nthat are also ‚Äúslightly off ‚Äù‚Äîwhich is also very much unlike any previous computing\\ntendencies.\\nIn fact, computers were designed to reliably repeat the same set of instructions with\\nthe same results every time. Over the past few decades, that principle of reliability\\nhas permeated the design of human-computer interfaces (variously called HCI, UX,\\nand UI) to the extent that a lot of the usual constructs end up being subpar for use in\\napplications that rely heavily on LLMs.\\nLet‚Äôs take an example: Figma is a software application used by designers to create\\nfaithful renderings of designs for websites, mobile applications, book or magazine\\ncovers‚Äîthe list goes on. As is the case with pretty much all productivity software\\n(software for the creation of some kind of long-form content), its interface is a\\ncombination of the following:\\n‚Ä¢ A palette of tools and prebuilt primitives (fundamental building blocks), in this‚Ä¢\\ncase lines, shapes, selection and paint tools, and many more\\n‚Ä¢ A canvas, where the user inserts these building blocks and organizes them into‚Ä¢\\ntheir creation: a website page, a mobile app screen, and so on\\n251'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 279, 'page_label': '252'}, page_content='This interface is built upon the premise that the capabilities of the software are\\nknown ahead of time, which is in fact true in the case of Figma. All building blocks\\nand tools were coded by a software engineer ahead of time. Therefore, they were\\nknown to exist at the time the interface was designed. It sounds almost silly to point\\nthat out, but the same is not strictly true of software that makes heavy use of LLMs.\\nLook at a word processor (e.g., Microsoft Word or Google Docs). This is a software\\napplication for the creation of long-form text content of some kind, such as a blog\\npost, article, book chapter, and the like. The interface at our disposal here is also\\nmade up of a familiar combination:\\n‚Ä¢ A palette of tools and prebuilt primitives: in the case of a word processor, the‚Ä¢\\nprimitives available are tables, lists, headings, image placeholders, and so forth,\\nand the tools are spellcheck, commenting, and so on.\\n‚Ä¢ A canvas: in this case, it‚Äôs literally a blank page, where the user types words and‚Ä¢\\nmay include some of the elements just mentioned.\\nHow would this situation change if we were to build an LLM-native word processor?\\nThis chapter explores three possible answers to this question, which are broadly\\napplicable to any LLM application. For each of the patterns we explore, we‚Äôll go over\\nwhat key concepts you‚Äô d need to implement it successfully. We don‚Äôt mean to imply\\nthat these are the only ones, it will be a while until the dust settles on this particular\\nquestion.\\nLet‚Äôs look at each of these patterns, starting with the easiest to add to an existing app.\\nInteractive Chatbots\\nThis is arguably the easiest lift to add to an existing software application. At its most\\nbasic conception, this idea just bolts on an AI sidekick‚Äîto bounce ideas off of‚Äîwhile\\nall work still happens in the existing user interface of the application. An example\\nhere is GitHub Copilot Chat, which can be used in a sidebar inside the VSCode code\\neditor.\\nAn upgrade to this pattern is to add some communication points between the AI\\nsidekick extension and the main application. For example, in VSCode, the assistant\\ncan ‚Äúsee‚Äù the content of the file currently being edited or whatever portion of that\\ncode the user has selected. And in the other direction, the assistant can insert or edit\\ntext in that open editor, arriving at some basic form of collaboration between the user\\nand the LLM.\\n252 | Chapter 11: Building with LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 280, 'page_label': '253'}, page_content='Streaming chat as we‚Äôre describing here is currently the prototyp‚Äê\\nical application of LLMs. It‚Äôs almost always the first thing app\\ndevelopers learn to build on their LLM journey, and it‚Äôs almost\\nalways the first thing companies reach for when adding LLMs to\\ntheir existing applications. Maybe this will remain the case for years\\nto come, but another possible outcome could be for streaming chat\\nto become the command line of the LLM era‚Äîthat is, the closest\\nto direct programming access, becoming a niche interface, just as it\\ndid for computers.\\nTo build the most basic chatbot you should use these components:\\nA chat model\\nTheir dialogue tuning lends itself well to multiturn interactions with a user. Refer\\nto the Preface for more on dialogue tuning.\\nConversation history\\nA useful chatbot needs to be able to ‚Äúget past hello. ‚Äù That is, if the chatbot can‚Äôt\\nremember the previous user inputs, it will be much harder to have meaningful\\nconversations with it, which implicitly refer to previous messages.\\nTo go beyond the basics, you‚Äô d probably add the following:\\nStreaming output\\nThe best chatbot experiences currently stream LLM output token by token (or in\\nlarger chunks, like sentences or paragraphs) directly to the user, which alleviates\\nthe latency inherent to LLMs today.\\nTool calling\\nTo give the chatbot the ability to interact with the main canvas and tools of the\\napplication, you can expose them as tools the model can decide to call on‚Äîfor\\ninstance, a ‚Äúget selected text‚Äù tool and an ‚Äúinsert text at end of doc‚Äù tool.\\nHuman-in-the-loop\\nAs soon as you give the chatbot tools that can change what‚Äôs in the application\\ncanvas, you create the need to give back some control to the user‚Äîfor example,\\nletting the user confirm, or even edit, before new text is inserted.\\nInteractive Chatbots | 253'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 281, 'page_label': '254'}, page_content='Collaborative Editing with LLMs\\nMost productivity software has some form of collaborative editing built in, which we\\ncan classify into one of these buckets (or somewhere in between):\\nSave and send\\nThis is the most basic version, which only supports one user editing the docu‚Äê\\nment at a time, before ‚Äúpassing the buck‚Äù to another user (for example, sending\\nthe file over email) and repeating the process until done. The most obvious\\nexample is the Microsoft Office suite of apps: Excel, Word, PowerPoint.\\nVersion control\\nThis is an evolution of save and send that supports multiple editors working\\nsimultaneously on their own (and unaware of each other‚Äôs changes) by providing\\ntools to combine their work afterward: merge strategies (how to combine unre‚Äê\\nlated changes) and conflict resolution (how to combine incompatible changes).\\nThe most popular example today is Git/GitHub, used by software engineers to\\ncollaborate on software projects.\\nReal-time collaboration\\nThis enables multiple editors to work on the same document at the same time,\\nwhile seeing each other‚Äôs changes. This is arguably the most natural form of\\nsoftware-enabled collaboration, evidenced by the popularity of Google Docs and\\nGoogle Sheets among technical and nontechnical computer users.\\nThis pattern of LLM user experience consists of employing an LLM agent as one\\nof those ‚Äúusers‚Äù contributing to this shared document. This can take many forms,\\nincluding the following:\\n‚Ä¢ An always-on ‚Äúcopilot‚Äù giving you suggestions on how to complete the next‚Ä¢\\nsentence\\n‚Ä¢ An asynchronous ‚Äúdrafter, ‚Äù which you task with, for example, going off and‚Ä¢\\nresearching the topic in question and returning later with a section you can\\nincorporate in your final document\\nTo build this, you‚Äô d likely need the following:\\nShared state\\nThe LLM agent and the human users should be on the same footing in terms\\nof access and understanding of the state of the document‚Äîthat is, they would\\nbe able to parse the state of the document and produce edits to that state in a\\ncompatible format.\\n254 | Chapter 11: Building with LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 282, 'page_label': '255'}, page_content='Task manager\\nProducing a useful edit to the document will invariably be a multistep process,\\nwhich can take time and fail halfway. This creates the need for reliable scheduling\\nand orchestration of long-running jobs, with queueing, error recovery, and con‚Äê\\ntrol over running tasks.\\nMerging forks\\nUsers will continue to edit the document after tasking the LLM agent, so LLM\\noutputs will need to be merged with the users‚Äô work, either manually by the user\\n(an experience like Git) or automatically (through conflict resolution algorithms\\nsuch as CRDT and operational transformation (OT), employed by applications\\nsuch as Google Docs).\\nConcurrency\\nThe fact that the human user and the LLM agent are working on the same\\nthing at the same time requires the ability to handle interruptions, cancellations,\\nreroutings (do this instead), and queueing (do this as well).\\nUndo/redo stack\\nThis is a ubiquitous pattern in productivity software, which inevitably is needed\\nhere too. Users change their minds and want to go back to an earlier state of the\\ndocument, and the LLM application needs to be capable of following them there.\\nIntermediate output\\nMerging user and LLM outputs is made a lot easier when those outputs are\\ngradual and arrive piecemeal as soon as they‚Äôre produced, in much the same way\\nthat a person writes a 10-paragraph page one sentence at a time.\\nAmbient Computing\\nA very useful UX pattern has been the always-on background software that  pipes up\\nwhen something ‚Äúinteresting‚Äù has happened that deserves your attention. Y ou can\\nfind this in many places today. A few examples are:\\n‚Ä¢ Y ou can set an alert in your brokerage app to notify you when some stock goes‚Ä¢\\nbelow a certain price.\\n‚Ä¢ Y ou can ask Google to notify you when new search results are found matching‚Ä¢\\nsome search query.\\n‚Ä¢ Y ou can define alerts for your computer infrastructure to notify you when some‚Äê‚Ä¢\\nthing is outside the regular pattern of behavior.\\nAmbient Computing | 255'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 283, 'page_label': '256'}, page_content='The main obstacle to deploying this pattern more widely may be coming up with a\\nreliable definition of interesting ahead of time that is both of the following:\\nUseful\\nIt will notify you when you think it should.\\nPractical\\nMost users won‚Äôt want to spend massive amounts of time ahead precreating\\nendless rules for alerts.\\nThe reasoning capabilities of LLMs can unlock new applications of this pattern of\\nambient computing that are simultaneously more useful (they identify more of what\\nyou‚Äô d find interesting) and less work to set up (their reasoning can replace a lot or all\\nof the manual setup of rules).\\nThe big difference between collaborative and ambient is concurrency:\\nCollaborative\\nY ou and the LLM are usually (or sometimes) doing work at the same time and\\nfeeding off each other‚Äôs work.\\nAmbient\\nThe LLM is continuously doing some kind of work in the background while you,\\nthe user, are presumably doing something else entirely.\\nTo build this, you need:\\nTriggers\\nThe LLM agent needs to receive (or poll periodically for) new information from\\nthe environment. This is in fact what motivates ambient computing: a preexisting\\nsource of periodic or continuous new information that needs to be processed.\\nLong-term memory\\nIt would not be possible to detect new interesting events without consulting a\\ndatabase of previously received information.\\nReflection (or learning)\\nUnderstanding what is interesting (what deserves human input) likely requires\\nlearning from each previous interesting event after it happens. This is usually\\ncalled a reflection step , in which the LLM produces an update to its long-term\\nmemory, possibly modifying its internal ‚Äúrules‚Äù for detecting future interesting\\nevents.\\n256 | Chapter 11: Building with LLMs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 284, 'page_label': '257'}, page_content='Summarize output\\nAn agent working in the background is likely to produce much more output than\\nthe human user would like to see. This requires that the agent architecture be\\nmodified to produce summaries of the work done and surface to the user only\\nwhat is new or noteworthy.\\nTask manager\\nHaving an LLM agent working continuously in the background requires employ‚Äê\\ning some system for managing the work, queuing new runs, and handling and\\nrecovering from error.\\nSummary\\nLLMs have the potential to change not only how we build software , but also the very\\nsoftware we build. This new capability that we developers have at our disposal to\\ngenerate new content will not only enhance many existing apps, but it can make new\\nthings possible that we haven‚Äôt dreamed of yet.\\nThere‚Äôs no shortcut here. Y ou really do need to build something (s)crappy, speak to\\nusers, and rinse and repeat until something new and unexpected comes out the other\\nside.\\nWith this last chapter, and the book as a whole, we have tried to give you the\\nknowledge we think can help you build something uniquely good with LLMs. We\\nwant to thank you for coming on this journey with us and wish you the best of luck in\\nyour career and future.\\nSummary | 257'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 286, 'page_label': '259'}, page_content='Index\\nSymbols\\n{} (curly braces), 9\\nA\\nabstractions, xx\\nagency, 116, 138, 171\\nagency-reliability trade-off, 116, 171-173\\nagent architecture\\nagent abstractions, xxi\\nalways calling tools first, 143-148\\ndealing with many tools, 148-153\\nLangGraph agent creation, 139-143\\nmulti-agent architectures, 165-170, 186\\nplan-do loop, 136-138\\nreflection prompting technique, 155-161\\nunique aspects of, 135\\nagents, evaluating performance of\\nfinal response, 236-240\\nlevels of testing, 235\\nsingle steps, 240-242\\ntrajectory, 242-247\\nAI chatbots (see chatbots)\\nAIMessage interface, 6\\nalerts, 255\\nalgorithms, ix\\nambient computing, 255\\nAnthropic, x, xxi, 2\\nAPI access, protecting, 213\\nAPI keys, retrieving, 192\\napplication logs, 224\\narchitecture (see also agent architecture; cogni‚Äê\\ntive architectures)\\nReAct architecture, xxi, 138\\nrouter architecture, 125-133\\ntransformer neural network architecture,\\nxiii\\nArcSearch, 98\\nargument k, 62\\nassistant role, xv, 6\\nassistants (see chatbots)\\nassistants (LangGraph Platform), 201\\nasync execution, 17\\nauthentication, 214\\nauthorize mode, 182\\nauto-evaluators, adding, 230\\nautonomy, 116, 172\\nB\\nbag-of-words model, 26\\nBard, x\\nbatch method, 16, 22\\nBLOOM, xi\\nbraces, curly ({}), 9\\nC\\ncanvases, 251\\nchain architecture, 121-125\\nchain-of-thought (CoT) prompting, xvii, xxi,\\n136\\nChase, Harrison, x\\nchat dataset, 226\\nchat models\\nalternative LLM providers for, 2\\ninteractive chatbots, 253\\nmessage interfaces in, 6\\nroles in, 6\\nchatbots\\nadding memory to chatbots, 105-107\\n259'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 287, 'page_label': '260'}, page_content=\"advanced components, 253\\nasynchronous execution of, 20\\nbasic components, 253\\nbuilding chatbot memory systems, 96-98\\nChatGPT, ix\\nconversation history, 107-114, 253\\ninteractive, 252\\nlimitation of simple, 23\\nmemory and, xxi\\nmodifying chat history, 107-114\\noverview of creating, xxii\\nprompt and chat model example of, 18\\nstreaming support with Python or Java‚Äê\\nScript, 19\\nChatGPT, ix\\nChatMessage interface, 7\\nChatPromptTemplate, 11-13\\ncheckpoint events, 178\\ncheckpointers, 105, 179\\nclassification problems, 26\\nClaude, x\\ncode examples, obtaining and using, xxiv\\ncognitive architectures\\nchain architecture, 121-125\\ndefinition of term, 116\\nLLM call architecture, 118-121\\nmajor LLM architectures, 117\\nrouter architecture, 125-133\\nCohere, 2, 36\\nColBERT model, 54\\ncollaborative editing, 254-255\\ncomments and questions, xxv\\ncomposition (see declarative composition;\\nimperative composition)\\nconcurrency, 255\\nconcurrent inputs, dealing with, 186-188\\ncontext\\naccurate output with, 24, 58\\ndealing with complex, 165\\ndefinition of term, xviii\\ncontext windows, 32, 107\\ncontinuous improvement cycle, 216 (see also\\ntesting)\\nconversation history, 253\\ncopilots, 254\\ncosine similarity, 29, 60\\nCoT prompting (see chain-of-thought prompt‚Äê\\ning)\\ncreate, read, update, delete (CRUD), 40\\ncron jobs (LangGraph Platform), 202\\nCSV format, 15\\ncurly braces ({}), 9\\ncurrent events, 23, 58\\nD\\ndata indexing\\nconverting documents into text, 30-32\\nembeddings, 25-30\\nend-to-end example, 38-39\\ngenerating text embeddings, 36-38\\nlimitations of LLM's knowledge corpus, 23\\noptimizing, 48-56\\nprocess of, 59\\nrelevant content for LLMs, 24, 59-68\\nsplitting text into chunks, 32-36\\nstoring embeddings in vector stores, 39-44\\ntracking document changes, 44-48\\ndata models (LangGraph Platform), 201\\ndata queries\\nquery construction, 87-92\\nquery routing, 81-87\\nquery transformation, 68-81\\ndatabase access, protecting, 214\\ndatabase description, 90\\ndatasets\\ncreating, 224-228\\ndialogue datasets, xiv\\ntask-specific datasets, xiv\\ndebug mode (LangGraph), 178\\ndeclarative composition, 17, 20-22\\ndefense in depth, 213\\ndense embeddings, 27\\ndependencies, installing, 192\\ndeployment\\nLangGraph API configuration file, 204\\nLangGraph Studio, 210-212\\nfrom LangSmith UI, 207-209\\nprerequisites, 191-200\\nsecurity considerations, 213\\ntesting LangGraph app locally, 205-207\\ndesign stage, testing during, 217-223\\ndialogue-tuning, xiv\\ndimensions, 27\\ndocument loaders, 30\\ndocuments\\nclassifying, 26\\ndecoupling, 49-53\\ndeleting, 44\\n260 | Index\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 288, 'page_label': '261'}, page_content='preprocessing, 24\\nreranking/reordering, 75\\nretrieving relevant documents, 59-63\\ntracking document changes, 44-48\\ndouble texting (LangGraph Platform), 203\\ndrift, 234\\nDuckDuckGo, 139\\ndynamic few-shot prompting, xx\\ndynamic inputs, 8\\nE\\nedges, 101\\nediting, collaborative, 254-255\\nembedding models, xxi, 2, 27\\nembeddings\\nbag-of-words model, 26\\nbenefits of, 25\\ndefinition of term, 25\\ngenerating text embeddings, 36-38\\nLLM-based embeddings, 27\\noptimizing, 54\\nretrieving relevant embeddings, 61\\nsemantic embeddings, 27-29\\nstoring embeddings in vector stores, 39-44\\nuses for, 25, 29\\nenvironment variables, 209\\nerror handling, 217\\nerrors, monitoring, 249\\nevaluation metrics (see also testing)\\ndefining, 228\\ngetting started, 229\\nimproving LLM-as-a-judge performance,\\n229-232\\nmain evaluators, 228\\npairwise evaluation, 233\\nexpected output, xiii\\nexternal data, retrieving, 24\\nF\\nfeedback, collecting in production, 248\\nfew-shot prompting, xx, 90, 229\\nfile access, protecting, 213\\nfinal response, evaluating, 236-240\\nfine-tuning\\ndialogue datasets, xiv\\nfine-tuned LLMs, xv\\nfloating-point values, 28\\nflow engineering, 121\\nfork and merge strategy, 188\\nformats\\nmachine-readable, 15\\nproviding format instructions, 15\\nspecifying from LLMs, 13-16\\nfrontiers, 172\\nfunction calls, 81\\nG\\nGemma, xv, xxi\\ngenerate-reflect loop, 160\\ngeneration stage, 63\\ngenerative AI\\nbeginnings of, ix\\nLLMs versus ML algorithms, x\\nGitHub Copilot Chat, 252\\nGoogle\\nGemini, x\\nLangChain integration with, xxi\\ngraphs, 98 (see also LangGraph)\\nground truth references, 228\\nH\\nhallucinations, 23, 54, 68, 107, 215\\nheuristic evaluators, 228\\nHierarchical Navigable Small World (HNSW),\\n61\\nHugging Face, 36\\nhuman evaluators, 228\\nhuman-computer interfaces (HCI), 251\\nhuman-in-the-loop modalities\\nauthorize mode, 182\\ncheckpointers, 179\\nedit state, 185\\nfork step, 185\\ninteractive chatbots and, 253\\ninterrupt mode, 180\\nLangGraph Platform API, 203\\nrestart step, 184\\nresume step, 183\\nHumanMessage interface, 6\\nHypothetical Document Embeddings (HyDE),\\n78-81\\nI\\nimperative composition, 17-20, 22\\ninaccuracies, 23\\nindependent agents, 166\\nindexing (see data indexing; vector indexes)\\nIndex | 261'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 289, 'page_label': '262'}, page_content='ingestions, 25\\ninput\\nchat format and, xv\\ndynamic inputs, 8\\nhandling concurrent, 186-188\\nin LLMs versus machine learning, xiii\\nrole in LLMs, xi\\ninstruction-tuning, xiv\\ninteractive chatbots, 178, 252 (see also chatbots)\\nintermediate output, 176-178, 255\\ninterrupt mode, 180\\ninterrupt strategy, 187\\ninvoke method, 5, 9, 16, 22\\nJ\\njailbreaking, xv\\nJavaScript\\nagent architecture\\nimplementing, 140\\nsubagents, 169\\nsupervisor, 167\\nasynchronous execution with, 20\\nchatbot conversation configuration, 6\\nchatbot edges, adding, 104\\nchatbot memory systems, 97\\nchatbot nodes, adding, 103\\nchatbot streaming support, 19, 22\\nChatPromptTemplate, 11-13\\ncognitive architectures\\nchain, 123\\nLLM call, 119\\nrouter, 129\\ncomposition\\ndeclarative, 21\\nimperative, 18\\ndata indexing\\nend-to-end example, 39\\nexample of, 60\\ndocument loaders and, 31\\ndocuments, retrieving full context, 51-53\\ndynamic inputs\\nconstructing, 9\\nfeeding into LLMs, 10\\nembeddings, retrieving relevant, 62\\nHypothetical Document Embeddings\\n(HyDE), 79-81\\nLangChain set up using, 4\\nLangGraph\\ninstallation, 101\\nvisual representation, 104\\nmessages\\nfiltering, 110\\nmerging, 113\\ntrimming, 108\\nmodel predictions, invoking, 5, 64-68\\nmulti-query retrieval in, 72-74\\noutput\\nintermediate, 176\\nstreaming token-by-token, 179\\nstructured, 175\\noutput parsers in, 16\\nRAG-Fusion strategy, 74-78\\nrecord management in, 46\\nreflection prompting technique, 158\\nRewrite-Retrieve-Read strategy, 70\\nrouting\\nlogical, 82-84\\nsemantic, 86-87\\nrunnable interface, 17\\nschema creation, 174\\nstate inspection and updating, 107\\nStateGraph\\ncreating, 102\\nmemory addition, 106\\nsubgraphs\\ncalling directly, 163\\ncalling with functions, 164\\nSystemMessage instruction, 7\\ntext embeddings with, 37\\ntext extraction with, 32\\ntext, splitting into chunks, 33-36\\ntext-to-metadata filter, 88-90\\ntext-to-SQL translations, 91\\ntools\\nalways calling first, 144\\ndealing with many, 149\\nvector stores and, 41\\nJSON output, 14, 173\\nJSONSchema notation, 174\\nK\\nk parameter, 77\\nk-nearest neighbors (k-NN), 53\\nKahneman, Daniel, 156\\nkeyword searches, 26\\nkv (key-value) dataset, 225\\n262 | Index'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 290, 'page_label': '263'}, page_content=\"L\\nLangChain (see also LangGraph; large lan‚Äê\\nguage models; LLM applications; retrieval-\\naugmented generation)\\nassembling LLM applications, 16-22,\\n115-118\\nbeginnings of, x\\nbenefits of, ix-x, xx, 1-3\\ndocumentation for, 2\\ngetting specific formats out of LLMs, 13-16\\ninterfaces provided, 4\\nmaking LLM prompts reusable, 7-13\\nsetting up, 3\\nusing LLMs in, 4-7\\nLangChain Expression Language (LCEL)\\nimperative versus declarative composition,\\n17\\noptimized execution plan using, 20\\nLangGraph (see also cognitive architectures;\\nLangChain; LangSmith)\\nadding memory to chatbots, 105-107\\nbasics of, 98-101\\nbuilding chatbot memory systems, 96-98\\nbuilding LangGraph agents, 139-143\\nconfiguration file, 204\\ncontrol flow illustration, 197\\ncreating a StateGraph, 101-105\\ninstalling, 101\\nintermediate output, 176\\nmodifying chat history, 107-114\\npurpose of, 98\\nself-corrective RAG, 217-223\\nstream modes supported in, 178\\nstreaming LLM output token-by-token, 178\\nsubgraphs, 161-165\\nLangGraph Platform API\\ndata models, 201\\ndeployment using, 198\\nfeatures, 202-204\\nLangGraph Studio, 210-212\\nLangSmith (see also LangChain; LangGraph)\\naccount creation, 199\\nagent evaluation example, 247\\ncomparison view/regression testing, 234\\ndataset creation, 224\\ndeployment from LangSmith UI, 207-209\\nfew-shot prompting, 229-232\\npairwise evaluation, 233\\ntrace results in, 222\\ntracing, 248\\nlanguage models, xi\\nlarge language model (llm) dataset, 225\\nlarge language models (LLMs) (see also struc‚Äê\\ntured output)\\nalternative providers for, 2\\nambient computing with, 255\\nbenefits and drawbacks of, 251\\nbrief primer on, xi-xiii\\nbuilding with, 251-257\\nClaude and Bard, x\\ncollaborative editing with, 254-255\\ndeployment example, 192\\ngetting specific formats out of LLMs, 13-16\\nlimitations of knowledge corpus, 23\\nLLM-based embeddings, 27\\nversus machine learning, x\\nfor multilingual output, xi\\nopen source LLMs, xv, xxi\\nOpenAI's GPT-3, xi\\nOpenAI's GPT-3.5, ix\\nrelevant content for LLMs, 24, 59-68\\ntypes of, xiii-xv\\nusing in LangChain, 4-7\\nlatency, 172, 176, 253\\nLCEL (see LangChain Expression Language)\\nLlama, xv, xxi\\nllm (large language model) dataset, 225\\nLLM applications (see also deployment; evalua‚Äê\\ntion metrics; testing)\\nagentic, 135\\nassembling, 16-22, 115-118\\nbenefits of LangChain for building, 1-3\\nchallenge in building good applications, 1\\ncommon features of, 115\\nkey stages of app development, 216\\nmajor LLM architectures, 117\\nmultiactor applications, 98\\nmultitasking LLMs, 186-188\\npatterns and key concepts for successful,\\n173-188\\nsecurity considerations, 213\\ntrade-off between agency and reliability,\\n116, 171-173\\nLLM call architecture, 118-121\\nLLM-as-a-judge evaluators, 228\\nLLM-driven loop, 136\\nLLMs (see large language models)\\nlogical routing, 81-84\\nIndex | 263\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 291, 'page_label': '264'}, page_content='long-term memory, 256\\nloops, 136\\nM\\nmachine learning (ML)\\nbasics of, ix\\ncurrent applications of, x\\nversus LLMs, x\\nmax_tokens parameter, 5\\nmemory systems\\nadding memory to StateGraph, 105-107\\nbuilding chatbot memory systems, 96-98\\nlong-term memory, 256\\npurpose of, 95\\nmerging forks, 255\\nmessages\\nappending, 103\\nfiltering, 110-112\\nmerging, 112-114\\ntrimming, 107-110\\nmisuse, anticipating potential, 213\\nmitigation strategies, 213\\nML (see machine learning)\\nmodel drift, 234\\nmodel parameter, 5\\nmonitoring errors, 249 (see also testing)\\nmulti-agent architectures, 165-170\\nmulti-query retrieval strategy, 71-74\\nmultiactor applications, 98\\nmultilingual output, xi\\nmultitasking, 186-188\\nMultiVectorRetriever, 49-53\\nN\\nneural networks, xi\\nnodes, 100\\nNorvig, Peter, 135\\nO\\nobservability, 247\\noffline evaluation, 228\\nOllama, 2\\nonline evaluation, 247\\nOpenAI API\\naccount creation, xvi, 3\\ncopying prompts to Playground, xvi\\nintegration with LangChain, xxi, 36\\nlaunch of ChatGPT and ChatGPT API, ix\\noverview of models offered, 5\\npricing, ix\\nOpenSearch, 2\\noptimized execution plan, 20\\norchestration capabilities, 3\\noutput (see also structured output)\\naccurate, 24, 58\\nexpected output, xiii\\nJSON output, 14, 173\\nmultilingual output, xi\\nrole in LLMs, xi\\nstreaming token-by-token, 253\\nsummarizing, 257\\noutput parsers, 15\\nP\\npairwise evaluation, 233\\nparallel execution, 17\\nparameters\\nbasics of, xi\\nk parameter, 77\\nmax_tokens, 5\\nmodel parameter, 5\\ntemperature, 5\\npermissions, limiting, 213\\nPerplexity, 98\\npersistence, 105\\nPGVector, 40\\nplan-do loop, 136-138\\nPlayground (OpenAI API), xvi\\nPostgres, 40\\npreprocessing, 24\\npreproduction stage testing\\ndataset creation, 224-228\\ndefining evaluation criteria, 228-233\\nend-to-end performance, 235-247\\npurpose of, 224\\nregression testing, 234\\npretrained LLMs, xiii\\nprimitives, 251\\nprivate data, 23\\nproduction stage testing\\nclassification and tagging, 249\\ncollecting feedback, 248\\nmonitoring and fixing errors, 249\\ntracing, 247\\nprompt engineering, xv-xvi\\nprompt injection guardrails, 214\\nprompt template abstractions, xxi\\n264 | Index'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 292, 'page_label': '265'}, page_content='prompts\\nbenefits of LangChain for prompting, xx\\nbest practices for, xiii\\nbrief primer on prompting, xv-xx\\ncombining prompting techniques, xix, xxi\\ndefinition of term, xiii\\nexample of detailed, 8\\npurpose of, 7\\nreflection prompting technique, 155-161\\nreusing, xxi, 7-13\\nstructured output with, 173\\nPydantic, 15\\nPython\\nagent architecture\\nimplementing, 139\\nsubagents, 168\\nsupervisor, 167\\nasynchronous execution with, 20\\nchatbot conversation configuration, 6\\nchatbot edges, adding, 104\\nchatbot memory systems, 96\\nchatbot nodes, adding, 103\\nchatbot streaming support, 19, 21\\nChatPromptTemplate, 11-13\\ncognitive architectures\\nchain, 121\\nLLM call, 118\\nrouter, 126\\nColBERT model, 55\\ncomposition\\ndeclarative, 20\\nimperative, 18\\ndata indexing\\nend-to-end example, 38\\nexample of, 60\\ndocument loaders and, 31\\ndocument summaries, generating, 49\\ndocuments, retrieving full context, 51\\ndynamic inputs\\nconstructing, 8\\nfeeding into LLMs, 10\\nembeddings, retrieving relevant, 62\\nf-string syntax, 9\\nHypothetical Document Embeddings\\n(HyDE), 78-81\\nLangChain set up using, 3\\nLangGraph\\ninstallation, 101\\nvisual representation, 104\\nmessages\\nfiltering, 110\\nmerging, 112\\ntrimming, 108\\nmodel predictions, invoking, 5, 64-68\\nmulti-query retrieval in, 71-74\\noutput\\nintermediate, 176\\nJSON, 14\\nstreaming token-by-token, 178\\nstructured, 175\\noutput parsers in, 15\\nRAG-Fusion strategy, 74-78\\nrecord management in, 45\\nreflection prompting technique, 156\\nRewrite-Retrieve-Read strategy, 69\\nrouting\\nlogical, 82-84\\nsemantic, 85-87\\nrunnable interface, 16\\nschema creation, 174\\nstate inspection and updating, 107\\nStateGraph\\ncreating, 101\\nmemory addition, 105\\nsubgraphs\\ncalling directly, 162\\ncalling with functions, 164\\nSystemMessage instruction, 7\\ntext embeddings with, 37\\ntext extraction with, 32\\ntext, splitting into chunks, 33-36\\ntext-to-metadata filter, 88-90\\ntext-to-SQL translations, 91\\ntools\\nalways calling first, 143\\ndealing with many, 148\\nvector stores\\nstoring summaries in, 50\\nworking with, 41\\nQ\\nqueries (see data queries)\\nquery construction\\nprocess of, 87\\npurpose of, 93\\ntext-to-metadata filter, 87-90\\ntext-to-SQL translations, 90-92\\nquery routing strategy\\nIndex | 265'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 293, 'page_label': '266'}, page_content='logical routing, 81-84\\npurpose of, 81\\nsemantic routing, 84-87\\nquery transformation\\ndefinition of term, 68\\nHypothetical Document Embeddings\\n(HyDE), 78-81\\nmulti-query retrieval strategy, 71-74\\npurpose of, 93\\nRAG-Fusion strategy, 74-78\\nRewrite-Retrieve-Read strategy, 68-70\\nquestions and comments, xxv\\nqueueing, 187\\nR\\nRAG (see retrieval-augmented generation)\\nRAG-Fusion strategy, 74-78\\nRAPTOR (recursive abstractive processing for\\ntree-organized retrieval), 53\\nrate limiting, 214\\nReAct architecture, xxi, 138\\nreal-time collaboration, 254\\nreciprocal rank fusion (RRF) algorithm, 74-78\\nrecord management, 44-48\\nrecursive abstractive processing for tree-\\norganized retrieval (RAPTOR), 53\\nreflection prompting technique, 155-161\\nreflection step, 256\\nregression testing, 234\\nreinforcement learning from human feedback\\n(RLHF), xiv\\nreliability, 116, 171, 251\\nreranking/reordering, 75\\nresponse, evaluating, 235\\nretrieval process, 61\\nretrieval-augmented generation (RAG)\\nbuilding robust RAG systems, 67\\ncore stages of, 59\\ndefinition of term, 24\\ngenerating LLM predictions, 63-68\\nintroduction to, 57\\nprocess of, 57\\nprompting with, xviii\\nquery handling\\nconstruction, 87-92\\nrouting, 81-87\\ntransformation, 68-81\\nretrieving relevant documents, 59-63\\nself-corrective RAG, 217-223\\nRewrite-Retrieve-Read strategy, 68-70\\nRLHF (reinforcement learning from human\\nfeedback), xiv\\nroles, xv, 6\\nrouter architecture, 125-133\\nrouting\\nlogical routing, 81-84\\nquery routing, 81-87\\nsemantic routing, 84-87\\nRRF (reciprocal rank fusion) algorithm, 74\\nrunnable interface, 16\\nruns (LangGraph Platform), 201\\nRussell, Stuart, 135\\nS\\nsandboxing, 213\\nsave and send, 254\\nsecurity best practices, 213\\nself-correction, 218\\nself-critique prompting technique, 155\\nsemantic embeddings, 27-29\\nsemantic relationships, 32\\nsemantic routing, 84-87\\nsensitive data, protecting, 213\\nsingle steps, evaluating, 235\\nsparse embeddings, 26\\nspecialized subsystems, 165\\nstate, 100, 102, 180, 254\\nstatefulness, 98\\nStateGraph\\nadding memory to, 105-107\\ncreation, 101-105\\nstateless runs (LangGraph Platform), 203\\nstatelessness, 95\\nstatic few-shot prompting, xx\\nstream method, 16, 22, 176\\nstreaming (LangGraph Platform), 202\\nstreaming chat, 253\\nstructured output\\nhuman-in-the-loop modalities, 179-186\\nintermediate output, 176-178\\nneed for, 173\\nstrategies to achieve, 173\\nstreaming LLM output token-by-token, 178\\nsubagents, 168\\nsubgraphs\\nadding to parent graphs, 162\\ncalling subgraphs directly, 162-164\\ncalling subgraphs with functions, 164\\n266 | Index'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 294, 'page_label': '267'}, page_content='use cases for, 161\\nsubsystems, specialized, 165\\nSupabase, 193\\nsupervisor architecture, 166-170\\nsynthetic data, 224\\nsystem role, xv, 6\\nSystemMessage interface, 7\\nT\\ntables, 49\\ntask managers, 255, 257\\ntask-specific datasets, xiv\\ntask_result events, 178\\ntemperature parameter, 5\\ntesting\\ndesign stage, 217-223\\nkey stages of app development, 216\\npredeployment, 205-207\\npreproduction stage, 224-247\\nproduction stage, 247-249\\ntext\\nconverting documents into text, 30-32\\nconverting text to numbers, 25-29\\ngenerating text embeddings, 36-38\\nmixed with tables, 49\\nsplitting text into chunks, 32-36\\ntext-to-metadata filter, 87-90\\ntext-to-SQL translations, 90-92\\nthird-party services, xxi\\nthreads, 106, 187\\nthreads (LangGraph Platform), 201\\ntoken-by-token streaming output, 178\\ntokens, xii\\ntool-calling technique\\nagentic LLM applications and, 135\\nalways calling tools first, 143-148\\ndealing with many tools, 148-153, 165\\ndefinition of term, xviii\\nintegrations provided for, xxi\\ninteractive chatbots and, 253\\nstructured output with, 173\\ntracing, 247\\ntrajectory, evaluating, 235, 242-247\\ntransformer neural network architecture, xiii\\ntriggers, 256\\ntrimming messages, 107-110\\ntry-catch statement, 182\\nU\\nundo/redo stacks, 255\\nUniversally Unique Identifiers (UUIDs), 106\\nupdates mode (LangGraph), 178\\nuser role, xv, 6\\nV\\nvalues mode (LangGraph), 178\\nvariance, 172\\nvector indexes, xxi\\nvector stores\\nalternative providers for, 2\\ndefinition of term, xxi, 25\\ndeployment example, 193\\nstoring embeddings in vector stores, 39-44\\nversion control, 254\\nVSCode code editor, 252\\nW\\nWeaviate, 2\\nwebhooks (LangGraph Platform), 204\\nX\\nXML format, 15\\nY\\nY ao, Shunyu, 138\\nZ\\nzero-shot prompting, xvi\\nZod, 15\\nIndex | 267'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 295, 'page_label': '268'}, page_content='About the Authors\\nMayo Oshin  is a tech entrepreneur, AI advisor, and angel investor. Mayo was an\\nearly developer contributor and advocate for the open source LangChain library and\\nan early pioneer in the popular AI ‚Äúchat‚Äù with data movement (5+ million people\\nreached through his thought-leadership ideas so far).\\nMayo has consulted with, advised, and trained hundreds of engineers and product\\nmanagers at various leading institutions, including Amazon, LinkedIn, Evercore,\\nVisa, and BCG.\\nY ou can learn more about him at his website, mayooshin.com, and get his latest ideas\\non AI and technology on X.\\nNuno Campos is a founding software engineer at LangChain, Inc. Nuno has a decade\\nof experience as a Python and JavaScript software engineer, architect and open source\\nmaintainer. He has worked for various tech startups in software engineering and data\\nscience roles. He holds an MSc in Finance.\\nColophon\\nThe animals on the cover of Learning LangChain  are European tree frogs ( Hyla\\narborea).\\nThey are primarily found in mainland Europe, typically close to vegetation. Their\\ngreen skin can adjust its color from green to grey or brown, depending on a variety\\nof factors. This ability helps to camouflage them from predators. They also use\\ntheir sticky tongues (for meals, i.e., insects) and the sticky pads on their feet (for\\nmovement) to navigate life in their habitat.\\nDuring mating season, European tree frogs congregate close to ponds so that they can\\nlay eggs near water for the soon-to-be tadpoles. The males perform a mating call that\\nis striking for its volume, particularly when many frogs are gathered in one place. The\\nfemales then lay clumps of 800 to 1,000 eggs. About 10 to 14 days later, the tadpoles\\nhatch, and the cycle of life begins anew.\\nMany of the animals on O‚ÄôReilly covers are endangered; all of them are important to\\nthe world.\\nThe cover illustration is by Karen Montgomery, based on an antique line engraving\\nfrom Meyers Kleines Lexicon. The series design is by Edie Freedman, Ellie Volckhau‚Äê\\nsen, and Karen Montgomery. The cover fonts are Gilroy Semibold and Guardian\\nSans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Con‚Äê\\ndensed; and the code font is Dalton Maag‚Äôs Ubuntu Mono.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 296, 'page_label': '269'}, page_content='Learn from experts.\\nBecome one yourself.\\n60,000+ titles | Live events with experts | Role-based courses\\nInteractive learning | Certification preparation\\nTry the O‚ÄôReilly learning platform free for 10 days.\\n¬©2025 O‚ÄôReilly Media, Inc. O‚ÄôReilly is a registered trademark of O‚ÄôReilly Media, Inc.  718900_7x9.1875')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader= PyPDFLoader('D:\\\\Langchain\\\\LangChain_Book.pdf')\n",
    "pages=loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8a4322c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f9b4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "template='''\n",
    "Write a concise and short summary of the following speech:\\n\n",
    "{text}'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9787b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.summarize import load_summarize_chain\n",
    "summarize_chain = load_summarize_chain(\n",
    "    model, chain_type=\"stuff\", prompt=prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91b9b1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='\\nWrite a concise and short summary of the following speech:\\n\\n{text}'), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000143BC944D60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000143BC944C70>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='text')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b8c2905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Write a concise and short summary of the following speech:\n",
      "\n",
      "Mayo Oshin &  \n",
      "Nuno Campos\n",
      " Learning \n",
      "LangChain\n",
      "Building AI and LLM Applications  \n",
      "with LangChain and LangGraph\n",
      "\n",
      "9 781098 167288\n",
      "57999\n",
      "ISBN:   978-1-098-16728-8\n",
      "US $79.99    CAN $99.99\n",
      "DATA\n",
      "If you‚Äôre looking to build production-ready AI applications that can reason and retrieve external data for \n",
      "context-awareness, you‚Äôll need to master LangChain‚Äîa popular development framework and platform \n",
      "for building, running, and managing agentic applications. LangChain is used by several leading companies, \n",
      "including Zapier, Replit, Databricks, and many more. This guide is an indispensable resource for developers \n",
      "who understand Python or JavaScript but are beginners eager to harness the power of AI.\n",
      "Authors Mayo Oshin and Nuno Campos demystify the use of LangChain through practical insights and in-depth \n",
      "tutorials. Starting with basic concepts, this book shows you step-by-step how to build a production-ready \n",
      "AI agent that uses your data.\n",
      "‚Ä¢ Harness the power of retrieval-augmented  \n",
      "generation (RAG) to enhance the accuracy of  \n",
      "LLMs using external up-to-date data\n",
      "‚Ä¢ Develop and deploy AI applications that interact  \n",
      "intelligently and contextually with users\n",
      "‚Ä¢ Make use of the powerful agent architecture  \n",
      "with LangGraph\n",
      "‚Ä¢ Integrate and manage third-party APIs and tools  \n",
      "to extend the functionality of your AI applications\n",
      "‚Ä¢ Monitor, test, and evaluate your AI applications  \n",
      "to improve performance\n",
      "‚Ä¢ Understand the foundations of LLM app development  \n",
      "and how they can be used with LangChain\n",
      " Learning LangChain\n",
      "‚Äú With clear explanations and actionable techniques, this is the go-to resource \n",
      "for anyone looking to harness LangChain‚Äôs power for production-ready \n",
      "generative AI and agents. A must-read for developers aiming to push the \n",
      "boundaries of this platform.‚Äù\n",
      "Tom Taulli, IT consultant and author of AI-Assisted Programming\n",
      "‚ÄúThis comprehensive guide covers everything from document retrieval and \n",
      "indexing to deploying and monitoring AI agents in production. With engaging \n",
      "examples, intuitive illustrations, and hands-on code, this book made learning \n",
      "LangChain interesting and fun!‚Äù\n",
      "Rajat K. Goel , senior software engineer, IBM\n",
      "Mayo Oshin is a tech entrepreneur,  \n",
      "AI advisor, and angel investor. He \n",
      "was an early developer contributor \n",
      "and advocate for the LangChain \n",
      "open source library and a pioneer \n",
      "in the popular AI ‚Äúchat with data‚Äù \n",
      "movement.\n",
      "Nuno Campos is a founding software \n",
      "engineer at LangChain. He has a \n",
      "decade of experience as a Python \n",
      "and JavaScript software engineer, \n",
      "architect, and open source maintainer.\n",
      "\n",
      "Praise for Learning LangChain\n",
      "With clear explanations and actionable techniques, this is the go-to resource for anyone\n",
      "looking to harness LangChain‚Äôs power for production-ready generative AI and agents.\n",
      "A must-read for developers aiming to push the boundaries of this platform.\n",
      "‚ÄîTom Taulli, IT consultant and\n",
      "author of AI-Assisted Programming\n",
      "This comprehensive guide on LangChain covers everything from document retrieval\n",
      "and indexing to deploying and monitoring AI agents in production.\n",
      "With engaging examples, intuitive illustrations, and hands-on code,\n",
      "this book made learning LangChain interesting and fun!\n",
      "‚ÄîRajat K. Goel, senior software engineer, IBM\n",
      "This book is a comprehensive LLM guide covering fundamentals to production,\n",
      "packed with technical insights, practical strategies, and robust AI patterns.\n",
      "‚ÄîGourav Singh Bais, senior data scientist and\n",
      "senior technical content writer, Allianz Services\n",
      "Prototyping generative AI apps is easy‚Äîshipping them is hard. The\n",
      "strategies and tools in Learning LangChain make it possible to turn ideas\n",
      "into modern, production-ready applications.\n",
      "‚ÄîJames Spiteri, director of product management\n",
      "for security, Elastic\n",
      "Learning LangChain provides a clear path for transforming how you build AI-powered\n",
      "applications. By breaking down flexible architectures and robust checkpointing, it offers a\n",
      "strong foundation for creating reliable, production-ready AI agents at scale.\n",
      "‚ÄîDavid O‚ÄôRegan, engineering manager for AI/ML, GitLab\n",
      "\n",
      "Learning LangChain helped us skip the boilerplate for debugging and monitoring.\n",
      "The many helpful patterns and tooling insights allowed us to move fast\n",
      "and deploy AI apps with confidence.\n",
      "‚ÄîChris Focke, chief AI scientist, AppFolio\n",
      "Teaching LangChain through clear, actionable examples, this book is a gateway to\n",
      "agentic applications that are as inspiring as Asimov‚Äôs sci-fi novels.\n",
      "‚Äî Ilya Meyzin, SVP head of data science, Dun & Bradstreet\n",
      "\n",
      "Mayo Oshin and Nuno Campos\n",
      "Learning LangChain\n",
      "Building AI and LLM Applications with\n",
      "LangChain and LangGraph\n",
      "\n",
      "978-1-098-16728-8\n",
      "[LSI]\n",
      "Learning LangChain\n",
      "by Mayo Oshin and Nuno Campos\n",
      "Copyright ¬© 2025 Olumayowa ‚ÄúMayo‚Äù Olufemi Oshin and O‚ÄôReilly Media, Inc. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O‚ÄôReilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O‚ÄôReilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com.\n",
      "Acquisitions Editor: Nicole Butterfield\n",
      "Development Editor: Corbin Collins\n",
      "Production Editor: Clare Laylock\n",
      "Copyeditor: nSight, Inc.\n",
      "Proofreader: Helena Stirling\n",
      "Indexer: Judith McConville\n",
      "Interior Designer: David Futato\n",
      "Cover Designer: Karen Montgomery\n",
      "Illustrator: Kate Dullea\n",
      "February 2025:  First Edition\n",
      "Revision History for the First Edition\n",
      "2025-02-13: First Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781098167288 for release details.\n",
      "The O‚ÄôReilly logo is a registered trademark of O‚ÄôReilly Media, Inc. Learning LangChain, the cover image,\n",
      "and related trade dress are trademarks of O‚ÄôReilly Media, Inc.\n",
      "The views expressed in this work are those of the authors and do not represent the publisher‚Äôs views.\n",
      "While the publisher and the authors have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use\n",
      "of or reliance on this work. Use of the information and instructions contained in this work is at your\n",
      "own risk. If any code samples or other technology this work contains or describes is subject to open\n",
      "source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights.\n",
      "\n",
      "Table of Contents\n",
      "Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\n",
      "1. LLM Fundamentals with LangChain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\n",
      "Getting Set Up with LangChain                                                                                       3\n",
      "Using LLMs in LangChain                                                                                                4\n",
      "Making LLM Prompts Reusable                                                                                      7\n",
      "Getting Specific Formats out of LLMs                                                                          13\n",
      "JSON Output                                                                                                                 14\n",
      "Other Machine-Readable Formats with Output Parsers                                        15\n",
      "Assembling the Many Pieces of an LLM Application                                                 16\n",
      "Using the Runnable Interface                                                                                     16\n",
      "Imperative Composition                                                                                             18\n",
      "Declarative Composition                                                                                            20\n",
      "Summary                                                                                                                           22\n",
      "2. RAG Part I: Indexing Your Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  23\n",
      "The Goal: Picking Relevant Context for LLMs                                                            24\n",
      "Embeddings: Converting Text to Numbers                                                                 25\n",
      "Embeddings Before LLMs                                                                                           25\n",
      "LLM-Based Embeddings                                                                                             27\n",
      "Semantic Embeddings Explained                                                                               27\n",
      "Converting Y our Documents into Text                                                                         30\n",
      "Splitting Y our Text into Chunks                                                                                    32\n",
      "Generating Text Embeddings                                                                                         36\n",
      "Storing Embeddings in a Vector Store                                                                          39\n",
      "Getting Set Up with PGVector                                                                                   40\n",
      "Working with Vector Stores                                                                                        41\n",
      "Tracking Changes to Y our Documents                                                                         44\n",
      "Indexing Optimization                                                                                                    48\n",
      "v\n",
      "\n",
      "MultiVectorRetriever                                                                                                   49\n",
      "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval        53\n",
      "ColBERT: Optimizing Embeddings                                                                           54\n",
      "Summary                                                                                                                           56\n",
      "3. RAG Part II: Chatting with Your Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  57\n",
      "Introducing Retrieval-Augmented Generation                                                           57\n",
      "Retrieving Relevant Documents                                                                                 59\n",
      "Generating LLM Predictions Using Relevant Documents                                     63\n",
      "Query Transformation                                                                                                    68\n",
      "Rewrite-Retrieve-Read                                                                                                68\n",
      "Multi-Query Retrieval                                                                                                 71\n",
      "RAG-Fusion                                                                                                                  74\n",
      "Hypothetical Document Embeddings                                                                       78\n",
      "Query Routing                                                                                                                  81\n",
      "Logical Routing                                                                                                             81\n",
      "Semantic Routing                                                                                                         84\n",
      "Query Construction                                                                                                        87\n",
      "Text-to-Metadata Filter                                                                                               87\n",
      "Text-to-SQL                                                                                                                   90\n",
      "Summary                                                                                                                           92\n",
      "4. Using LangGraph to Add Memory to Your Chatbot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95\n",
      "Building a Chatbot Memory System                                                                             96\n",
      "Introducing LangGraph                                                                                                  98\n",
      "Creating a StateGraph                                                                                                   101\n",
      "Adding Memory to StateGraph                                                                                   105\n",
      "Modifying Chat History                                                                                                107\n",
      "Trimming Messages                                                                                                   107\n",
      "Filtering Messages                                                                                                      110\n",
      "Merging Consecutive Messages                                                                               112\n",
      "Summary                                                                                                                         114\n",
      "5. Cognitive Architectures with LangGraph. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  115\n",
      "Architecture #1: LLM Call                                                                                            118\n",
      "Architecture #2: Chain                                                                                                  121\n",
      "\n",
      "Architecture #2: Chain                                                                                                  121\n",
      "Architecture #3: Router                                                                                                125\n",
      "Summary                                                                                                                         133\n",
      "6. Agent Architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  135\n",
      "The Plan-Do Loop                                                                                                         136\n",
      "Building a LangGraph Agent                                                                                       139\n",
      "Always Calling a Tool First                                                                                           143\n",
      "vi | Table of Contents\n",
      "\n",
      "Dealing with Many Tools                                                                                              148\n",
      "Summary                                                                                                                         153\n",
      "7. Agents II. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\n",
      "Reflection                                                                                                                        155\n",
      "Subgraphs in LangGraph                                                                                              161\n",
      "Calling a Subgraph Directly                                                                                      162\n",
      "Calling a Subgraph with a Function                                                                        164\n",
      "Multi-Agent Architectures                                                                                           165\n",
      "Supervisor Architecture                                                                                            167\n",
      "Summary                                                                                                                         170\n",
      "8. Patterns to Make the Most of LLMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  171\n",
      "Structured Output                                                                                                         173\n",
      "Intermediate Output                                                                                                  176\n",
      "Streaming LLM Output Token-by-Token                                                              178\n",
      "Human-in-the-Loop Modalities                                                                              179\n",
      "Multitasking LLMs                                                                                                     186\n",
      "Summary                                                                                                                         189\n",
      "9. Deployment: Launching Your AI Application into Production. . . . . . . . . . . . . . . . . . . . .  191\n",
      "Prerequisites                                                                                                                   191\n",
      "Install Dependencies                                                                                                  192\n",
      "Large Language Model                                                                                              192\n",
      "Vector Store                                                                                                                 193\n",
      "Backend API                                                                                                               197\n",
      "Create a LangSmith Account                                                                                    199\n",
      "Understanding the LangGraph Platform API                                                           200\n",
      "Data Models                                                                                                                201\n",
      "Features                                                                                                                        202\n",
      "Deploying Y our AI Application on LangGraph Platform                                        204\n",
      "Create a LangGraph API Config                                                                              204\n",
      "Test Y our LangGraph App Locally                                                                           205\n",
      "Deploy from the LangSmith UI                                                                               207\n",
      "Launch LangGraph Studio                                                                                        210\n",
      "Security                                                                                                                            213\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is a concise and short summary of the speech:\\n\\n\"Learning LangChain\" is a book by Mayo Oshin and Nuno Campos that provides a comprehensive guide to building production-ready AI applications using LangChain, a popular development framework for building, running, and managing agentic applications. The book covers topics such as:\\n\\n* Retrieval-Augmented Generation (RAG) to enhance the accuracy of Large Language Models (LLMs)\\n* Developing and deploying AI applications that interact intelligently and contextually with users\\n* Using the powerful agent architecture with LangGraph\\n* Integrate and manage third-party APIs and tools to extend the functionality of AI applications\\n* Monitoring, testing, and evaluating AI applications to improve performance\\n\\nThe book is designed for developers who understand Python or JavaScript but are beginners in AI and LLM development. It provides clear explanations, actionable techniques, and hands-on code to help readers build production-ready AI applications.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_output=summarize_chain.run(pages[0:10])\n",
    "summarized_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a838005",
   "metadata": {},
   "source": [
    "Map-Reduce Chain Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af88ca44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 0, 'page_label': 'Cover'}, page_content='Mayo Oshin &  \\nNuno Campos\\n Learning \\nLangChain\\nBuilding AI and LLM Applications  \\nwith LangChain and LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 1, 'page_label': 'BackCover'}, page_content='9 781098 167288\\n57999\\nISBN:   978-1-098-16728-8\\nUS $79.99    CAN $99.99\\nDATA\\nIf you‚Äôre looking to build production-ready AI applications that can reason and retrieve external data for \\ncontext-awareness, you‚Äôll need to master LangChain‚Äîa popular development framework and platform \\nfor building, running, and managing agentic applications. LangChain is used by several leading companies, \\nincluding Zapier, Replit, Databricks, and many more. This guide is an indispensable resource for developers \\nwho understand Python or JavaScript but are beginners eager to harness the power of AI.\\nAuthors Mayo Oshin and Nuno Campos demystify the use of LangChain through practical insights and in-depth \\ntutorials. Starting with basic concepts, this book shows you step-by-step how to build a production-ready \\nAI agent that uses your data.\\n‚Ä¢ Harness the power of retrieval-augmented  \\ngeneration (RAG) to enhance the accuracy of  \\nLLMs using external up-to-date data\\n‚Ä¢ Develop and deploy AI applications that interact  \\nintelligently and contextually with users\\n‚Ä¢ Make use of the powerful agent architecture  \\nwith LangGraph\\n‚Ä¢ Integrate and manage third-party APIs and tools  \\nto extend the functionality of your AI applications\\n‚Ä¢ Monitor, test, and evaluate your AI applications  \\nto improve performance\\n‚Ä¢ Understand the foundations of LLM app development  \\nand how they can be used with LangChain\\n Learning LangChain\\n‚Äú With clear explanations and actionable techniques, this is the go-to resource \\nfor anyone looking to harness LangChain‚Äôs power for production-ready \\ngenerative AI and agents. A must-read for developers aiming to push the \\nboundaries of this platform.‚Äù\\nTom Taulli, IT consultant and author of AI-Assisted Programming\\n‚ÄúThis comprehensive guide covers everything from document retrieval and \\nindexing to deploying and monitoring AI agents in production. With engaging \\nexamples, intuitive illustrations, and hands-on code, this book made learning \\nLangChain interesting and fun!‚Äù\\nRajat K. Goel , senior software engineer, IBM\\nMayo Oshin is a tech entrepreneur,  \\nAI advisor, and angel investor. He \\nwas an early developer contributor \\nand advocate for the LangChain \\nopen source library and a pioneer \\nin the popular AI ‚Äúchat with data‚Äù \\nmovement.\\nNuno Campos is a founding software \\nengineer at LangChain. He has a \\ndecade of experience as a Python \\nand JavaScript software engineer, \\narchitect, and open source maintainer.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 2, 'page_label': 'i'}, page_content='Praise for Learning LangChain\\nWith clear explanations and actionable techniques, this is the go-to resource for anyone\\nlooking to harness LangChain‚Äôs power for production-ready generative AI and agents.\\nA must-read for developers aiming to push the boundaries of this platform.\\n‚ÄîTom Taulli, IT consultant and\\nauthor of AI-Assisted Programming\\nThis comprehensive guide on LangChain covers everything from document retrieval\\nand indexing to deploying and monitoring AI agents in production.\\nWith engaging examples, intuitive illustrations, and hands-on code,\\nthis book made learning LangChain interesting and fun!\\n‚ÄîRajat K. Goel, senior software engineer, IBM\\nThis book is a comprehensive LLM guide covering fundamentals to production,\\npacked with technical insights, practical strategies, and robust AI patterns.\\n‚ÄîGourav Singh Bais, senior data scientist and\\nsenior technical content writer, Allianz Services\\nPrototyping generative AI apps is easy‚Äîshipping them is hard. The\\nstrategies and tools in Learning LangChain make it possible to turn ideas\\ninto modern, production-ready applications.\\n‚ÄîJames Spiteri, director of product management\\nfor security, Elastic\\nLearning LangChain provides a clear path for transforming how you build AI-powered\\napplications. By breaking down flexible architectures and robust checkpointing, it offers a\\nstrong foundation for creating reliable, production-ready AI agents at scale.\\n‚ÄîDavid O‚ÄôRegan, engineering manager for AI/ML, GitLab'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 3, 'page_label': 'ii'}, page_content='Learning LangChain helped us skip the boilerplate for debugging and monitoring.\\nThe many helpful patterns and tooling insights allowed us to move fast\\nand deploy AI apps with confidence.\\n‚ÄîChris Focke, chief AI scientist, AppFolio\\nTeaching LangChain through clear, actionable examples, this book is a gateway to\\nagentic applications that are as inspiring as Asimov‚Äôs sci-fi novels.\\n‚Äî Ilya Meyzin, SVP head of data science, Dun & Bradstreet'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 4, 'page_label': 'iii'}, page_content='Mayo Oshin and Nuno Campos\\nLearning LangChain\\nBuilding AI and LLM Applications with\\nLangChain and LangGraph'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 5, 'page_label': 'iv'}, page_content='978-1-098-16728-8\\n[LSI]\\nLearning LangChain\\nby Mayo Oshin and Nuno Campos\\nCopyright ¬© 2025 Olumayowa ‚ÄúMayo‚Äù Olufemi Oshin and O‚ÄôReilly Media, Inc. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O‚ÄôReilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO‚ÄôReilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\nAcquisitions Editor: Nicole Butterfield\\nDevelopment Editor: Corbin Collins\\nProduction Editor: Clare Laylock\\nCopyeditor: nSight, Inc.\\nProofreader: Helena Stirling\\nIndexer: Judith McConville\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\nFebruary 2025:  First Edition\\nRevision History for the First Edition\\n2025-02-13: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098167288 for release details.\\nThe O‚ÄôReilly logo is a registered trademark of O‚ÄôReilly Media, Inc. Learning LangChain, the cover image,\\nand related trade dress are trademarks of O‚ÄôReilly Media, Inc.\\nThe views expressed in this work are those of the authors and do not represent the publisher‚Äôs views.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\\nof or reliance on this work. Use of the information and instructions contained in this work is at your\\nown risk. If any code samples or other technology this work contains or describes is subject to open\\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 6, 'page_label': 'v'}, page_content='Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\\n1. LLM Fundamentals with LangChain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nGetting Set Up with LangChain                                                                                       3\\nUsing LLMs in LangChain                                                                                                4\\nMaking LLM Prompts Reusable                                                                                      7\\nGetting Specific Formats out of LLMs                                                                          13\\nJSON Output                                                                                                                 14\\nOther Machine-Readable Formats with Output Parsers                                        15\\nAssembling the Many Pieces of an LLM Application                                                 16\\nUsing the Runnable Interface                                                                                     16\\nImperative Composition                                                                                             18\\nDeclarative Composition                                                                                            20\\nSummary                                                                                                                           22\\n2. RAG Part I: Indexing Your Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  23\\nThe Goal: Picking Relevant Context for LLMs                                                            24\\nEmbeddings: Converting Text to Numbers                                                                 25\\nEmbeddings Before LLMs                                                                                           25\\nLLM-Based Embeddings                                                                                             27\\nSemantic Embeddings Explained                                                                               27\\nConverting Y our Documents into Text                                                                         30\\nSplitting Y our Text into Chunks                                                                                    32\\nGenerating Text Embeddings                                                                                         36\\nStoring Embeddings in a Vector Store                                                                          39\\nGetting Set Up with PGVector                                                                                   40\\nWorking with Vector Stores                                                                                        41\\nTracking Changes to Y our Documents                                                                         44\\nIndexing Optimization                                                                                                    48\\nv'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 7, 'page_label': 'vi'}, page_content='MultiVectorRetriever                                                                                                   49\\nRAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval        53\\nColBERT: Optimizing Embeddings                                                                           54\\nSummary                                                                                                                           56\\n3. RAG Part II: Chatting with Your Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  57\\nIntroducing Retrieval-Augmented Generation                                                           57\\nRetrieving Relevant Documents                                                                                 59\\nGenerating LLM Predictions Using Relevant Documents                                     63\\nQuery Transformation                                                                                                    68\\nRewrite-Retrieve-Read                                                                                                68\\nMulti-Query Retrieval                                                                                                 71\\nRAG-Fusion                                                                                                                  74\\nHypothetical Document Embeddings                                                                       78\\nQuery Routing                                                                                                                  81\\nLogical Routing                                                                                                             81\\nSemantic Routing                                                                                                         84\\nQuery Construction                                                                                                        87\\nText-to-Metadata Filter                                                                                               87\\nText-to-SQL                                                                                                                   90\\nSummary                                                                                                                           92\\n4. Using LangGraph to Add Memory to Your Chatbot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95\\nBuilding a Chatbot Memory System                                                                             96\\nIntroducing LangGraph                                                                                                  98\\nCreating a StateGraph                                                                                                   101\\nAdding Memory to StateGraph                                                                                   105\\nModifying Chat History                                                                                                107\\nTrimming Messages                                                                                                   107\\nFiltering Messages                                                                                                      110\\nMerging Consecutive Messages                                                                               112\\nSummary                                                                                                                         114\\n5. Cognitive Architectures with LangGraph. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  115\\nArchitecture #1: LLM Call                                                                                            118\\nArchitecture #2: Chain                                                                                                  121'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 7, 'page_label': 'vi'}, page_content='Architecture #2: Chain                                                                                                  121\\nArchitecture #3: Router                                                                                                125\\nSummary                                                                                                                         133\\n6. Agent Architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  135\\nThe Plan-Do Loop                                                                                                         136\\nBuilding a LangGraph Agent                                                                                       139\\nAlways Calling a Tool First                                                                                           143\\nvi | Table of Contents'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2025-04-18T11:54:13+00:00', 'author': 'Mayo Oshin and Nuno Campos', 'moddate': '2025-05-20T12:58:10-04:00', 'title': 'Learning LangChain', 'source': 'D:\\\\Langchain\\\\LangChain_Book.pdf', 'total_pages': 297, 'page': 8, 'page_label': 'vii'}, page_content='Dealing with Many Tools                                                                                              148\\nSummary                                                                                                                         153\\n7. Agents II. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\\nReflection                                                                                                                        155\\nSubgraphs in LangGraph                                                                                              161\\nCalling a Subgraph Directly                                                                                      162\\nCalling a Subgraph with a Function                                                                        164\\nMulti-Agent Architectures                                                                                           165\\nSupervisor Architecture                                                                                            167\\nSummary                                                                                                                         170\\n8. Patterns to Make the Most of LLMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  171\\nStructured Output                                                                                                         173\\nIntermediate Output                                                                                                  176\\nStreaming LLM Output Token-by-Token                                                              178\\nHuman-in-the-Loop Modalities                                                                              179\\nMultitasking LLMs                                                                                                     186\\nSummary                                                                                                                         189\\n9. Deployment: Launching Your AI Application into Production. . . . . . . . . . . . . . . . . . . . .  191\\nPrerequisites                                                                                                                   191\\nInstall Dependencies                                                                                                  192\\nLarge Language Model                                                                                              192\\nVector Store                                                                                                                 193\\nBackend API                                                                                                               197\\nCreate a LangSmith Account                                                                                    199\\nUnderstanding the LangGraph Platform API                                                           200\\nData Models                                                                                                                201\\nFeatures                                                                                                                        202\\nDeploying Y our AI Application on LangGraph Platform                                        204\\nCreate a LangGraph API Config                                                                              204\\nTest Y our LangGraph App Locally                                                                           205\\nDeploy from the LangSmith UI                                                                               207\\nLaunch LangGraph Studio                                                                                        210\\nSecurity                                                                                                                            213')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this chain type is used for large documents\n",
    "#considering only 10 documents here due to model toekn limit constraint\n",
    "\n",
    "pages[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0419c013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter= RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(pages[0:10])\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_prompt='''\n",
    "write a concise summary of the given text:\\n\n",
    "{text}'''\n",
    "\n",
    "map_prompt= PromptTemplate(\n",
    "    template=chunks_prompt,\n",
    "    input_variables=['text',]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "195ed790",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt='''\n",
    "provide the final summary of whole text after considering all main points.\n",
    "include a suitable title for the summary and mention highlights from summary in bullet points.\n",
    "\n",
    "{text}'''\n",
    "\n",
    "map_reduce_prompt=PromptTemplate(\n",
    "    template=final_prompt,\n",
    "    input_variables=['text',]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36be083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_summary_chain=load_summarize_chain(\n",
    "    llm=model,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=map_reduce_prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "397deca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Mayo Oshin &  \n",
      "Nuno Campos\n",
      " Learning \n",
      "LangChain\n",
      "Building AI and LLM Applications  \n",
      "with LangChain and LangGraph\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "9 781098 167288\n",
      "57999\n",
      "ISBN:   978-1-098-16728-8\n",
      "US $79.99    CAN $99.99\n",
      "DATA\n",
      "If you‚Äôre looking to build production-ready AI applications that can reason and retrieve external data for \n",
      "context-awareness, you‚Äôll need to master LangChain‚Äîa popular development framework and platform \n",
      "for building, running, and managing agentic applications. LangChain is used by several leading companies, \n",
      "including Zapier, Replit, Databricks, and many more. This guide is an indispensable resource for developers \n",
      "who understand Python or JavaScript but are beginners eager to harness the power of AI.\n",
      "Authors Mayo Oshin and Nuno Campos demystify the use of LangChain through practical insights and in-depth \n",
      "tutorials. Starting with basic concepts, this book shows you step-by-step how to build a production-ready \n",
      "AI agent that uses your data.\n",
      "‚Ä¢ Harness the power of retrieval-augmented  \n",
      "generation (RAG) to enhance the accuracy of  \n",
      "LLMs using external up-to-date data\n",
      "‚Ä¢ Develop and deploy AI applications that interact  \n",
      "intelligently and contextually with users\n",
      "‚Ä¢ Make use of the powerful agent architecture  \n",
      "with LangGraph\n",
      "‚Ä¢ Integrate and manage third-party APIs and tools  \n",
      "to extend the functionality of your AI applications\n",
      "‚Ä¢ Monitor, test, and evaluate your AI applications  \n",
      "to improve performance\n",
      "‚Ä¢ Understand the foundations of LLM app development  \n",
      "and how they can be used with LangChain\n",
      " Learning LangChain\n",
      "‚Äú With clear explanations and actionable techniques, this is the go-to resource \n",
      "for anyone looking to harness LangChain‚Äôs power for production-ready \n",
      "generative AI and agents. A must-read for developers aiming to push the \n",
      "boundaries of this platform.‚Äù\n",
      "Tom Taulli, IT consultant and author of AI-Assisted Programming\n",
      "‚ÄúThis comprehensive guide covers everything from document retrieval and \n",
      "indexing to deploying and monitoring AI agents in production. With engaging \n",
      "examples, intuitive illustrations, and hands-on code, this book made learning \n",
      "LangChain interesting and fun!‚Äù\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "LangChain interesting and fun!‚Äù\n",
      "Rajat K. Goel , senior software engineer, IBM\n",
      "Mayo Oshin is a tech entrepreneur,  \n",
      "AI advisor, and angel investor. He \n",
      "was an early developer contributor \n",
      "and advocate for the LangChain \n",
      "open source library and a pioneer \n",
      "in the popular AI ‚Äúchat with data‚Äù \n",
      "movement.\n",
      "Nuno Campos is a founding software \n",
      "engineer at LangChain. He has a \n",
      "decade of experience as a Python \n",
      "and JavaScript software engineer, \n",
      "architect, and open source maintainer.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Praise for Learning LangChain\n",
      "With clear explanations and actionable techniques, this is the go-to resource for anyone\n",
      "looking to harness LangChain‚Äôs power for production-ready generative AI and agents.\n",
      "A must-read for developers aiming to push the boundaries of this platform.\n",
      "‚ÄîTom Taulli, IT consultant and\n",
      "author of AI-Assisted Programming\n",
      "This comprehensive guide on LangChain covers everything from document retrieval\n",
      "and indexing to deploying and monitoring AI agents in production.\n",
      "With engaging examples, intuitive illustrations, and hands-on code,\n",
      "this book made learning LangChain interesting and fun!\n",
      "‚ÄîRajat K. Goel, senior software engineer, IBM\n",
      "This book is a comprehensive LLM guide covering fundamentals to production,\n",
      "packed with technical insights, practical strategies, and robust AI patterns.\n",
      "‚ÄîGourav Singh Bais, senior data scientist and\n",
      "senior technical content writer, Allianz Services\n",
      "Prototyping generative AI apps is easy‚Äîshipping them is hard. The\n",
      "strategies and tools in Learning LangChain make it possible to turn ideas\n",
      "into modern, production-ready applications.\n",
      "‚ÄîJames Spiteri, director of product management\n",
      "for security, Elastic\n",
      "Learning LangChain provides a clear path for transforming how you build AI-powered\n",
      "applications. By breaking down flexible architectures and robust checkpointing, it offers a\n",
      "strong foundation for creating reliable, production-ready AI agents at scale.\n",
      "‚ÄîDavid O‚ÄôRegan, engineering manager for AI/ML, GitLab\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Learning LangChain helped us skip the boilerplate for debugging and monitoring.\n",
      "The many helpful patterns and tooling insights allowed us to move fast\n",
      "and deploy AI apps with confidence.\n",
      "‚ÄîChris Focke, chief AI scientist, AppFolio\n",
      "Teaching LangChain through clear, actionable examples, this book is a gateway to\n",
      "agentic applications that are as inspiring as Asimov‚Äôs sci-fi novels.\n",
      "‚Äî Ilya Meyzin, SVP head of data science, Dun & Bradstreet\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Mayo Oshin and Nuno Campos\n",
      "Learning LangChain\n",
      "Building AI and LLM Applications with\n",
      "LangChain and LangGraph\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "978-1-098-16728-8\n",
      "[LSI]\n",
      "Learning LangChain\n",
      "by Mayo Oshin and Nuno Campos\n",
      "Copyright ¬© 2025 Olumayowa ‚ÄúMayo‚Äù Olufemi Oshin and O‚ÄôReilly Media, Inc. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O‚ÄôReilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O‚ÄôReilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com.\n",
      "Acquisitions Editor: Nicole Butterfield\n",
      "Development Editor: Corbin Collins\n",
      "Production Editor: Clare Laylock\n",
      "Copyeditor: nSight, Inc.\n",
      "Proofreader: Helena Stirling\n",
      "Indexer: Judith McConville\n",
      "Interior Designer: David Futato\n",
      "Cover Designer: Karen Montgomery\n",
      "Illustrator: Kate Dullea\n",
      "February 2025:  First Edition\n",
      "Revision History for the First Edition\n",
      "2025-02-13: First Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781098167288 for release details.\n",
      "The O‚ÄôReilly logo is a registered trademark of O‚ÄôReilly Media, Inc. Learning LangChain, the cover image,\n",
      "and related trade dress are trademarks of O‚ÄôReilly Media, Inc.\n",
      "The views expressed in this work are those of the authors and do not represent the publisher‚Äôs views.\n",
      "While the publisher and the authors have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use\n",
      "of or reliance on this work. Use of the information and instructions contained in this work is at your\n",
      "own risk. If any code samples or other technology this work contains or describes is subject to open\n",
      "source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Table of Contents\n",
      "Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\n",
      "1. LLM Fundamentals with LangChain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\n",
      "Getting Set Up with LangChain                                                                                       3\n",
      "Using LLMs in LangChain                                                                                                4\n",
      "Making LLM Prompts Reusable                                                                                      7\n",
      "Getting Specific Formats out of LLMs                                                                          13\n",
      "JSON Output                                                                                                                 14\n",
      "Other Machine-Readable Formats with Output Parsers                                        15\n",
      "Assembling the Many Pieces of an LLM Application                                                 16\n",
      "Using the Runnable Interface                                                                                     16\n",
      "Imperative Composition                                                                                             18\n",
      "Declarative Composition                                                                                            20\n",
      "Summary                                                                                                                           22\n",
      "2. RAG Part I: Indexing Your Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  23\n",
      "The Goal: Picking Relevant Context for LLMs                                                            24\n",
      "Embeddings: Converting Text to Numbers                                                                 25\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Embeddings Before LLMs                                                                                           25\n",
      "LLM-Based Embeddings                                                                                             27\n",
      "Semantic Embeddings Explained                                                                               27\n",
      "Converting Y our Documents into Text                                                                         30\n",
      "Splitting Y our Text into Chunks                                                                                    32\n",
      "Generating Text Embeddings                                                                                         36\n",
      "Storing Embeddings in a Vector Store                                                                          39\n",
      "Getting Set Up with PGVector                                                                                   40\n",
      "Working with Vector Stores                                                                                        41\n",
      "Tracking Changes to Y our Documents                                                                         44\n",
      "Indexing Optimization                                                                                                    48\n",
      "v\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "MultiVectorRetriever                                                                                                   49\n",
      "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval        53\n",
      "ColBERT: Optimizing Embeddings                                                                           54\n",
      "Summary                                                                                                                           56\n",
      "3. RAG Part II: Chatting with Your Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  57\n",
      "Introducing Retrieval-Augmented Generation                                                           57\n",
      "Retrieving Relevant Documents                                                                                 59\n",
      "Generating LLM Predictions Using Relevant Documents                                     63\n",
      "Query Transformation                                                                                                    68\n",
      "Rewrite-Retrieve-Read                                                                                                68\n",
      "Multi-Query Retrieval                                                                                                 71\n",
      "RAG-Fusion                                                                                                                  74\n",
      "Hypothetical Document Embeddings                                                                       78\n",
      "Query Routing                                                                                                                  81\n",
      "Logical Routing                                                                                                             81\n",
      "Semantic Routing                                                                                                         84\n",
      "Query Construction                                                                                                        87\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Text-to-Metadata Filter                                                                                               87\n",
      "Text-to-SQL                                                                                                                   90\n",
      "Summary                                                                                                                           92\n",
      "4. Using LangGraph to Add Memory to Your Chatbot. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95\n",
      "Building a Chatbot Memory System                                                                             96\n",
      "Introducing LangGraph                                                                                                  98\n",
      "Creating a StateGraph                                                                                                   101\n",
      "Adding Memory to StateGraph                                                                                   105\n",
      "Modifying Chat History                                                                                                107\n",
      "Trimming Messages                                                                                                   107\n",
      "Filtering Messages                                                                                                      110\n",
      "Merging Consecutive Messages                                                                               112\n",
      "Summary                                                                                                                         114\n",
      "5. Cognitive Architectures with LangGraph. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  115\n",
      "Architecture #1: LLM Call                                                                                            118\n",
      "Architecture #2: Chain                                                                                                  121\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Architecture #2: Chain                                                                                                  121\n",
      "Architecture #3: Router                                                                                                125\n",
      "Summary                                                                                                                         133\n",
      "6. Agent Architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  135\n",
      "The Plan-Do Loop                                                                                                         136\n",
      "Building a LangGraph Agent                                                                                       139\n",
      "Always Calling a Tool First                                                                                           143\n",
      "vi | Table of Contents\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Dealing with Many Tools                                                                                              148\n",
      "Summary                                                                                                                         153\n",
      "7. Agents II. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\n",
      "Reflection                                                                                                                        155\n",
      "Subgraphs in LangGraph                                                                                              161\n",
      "Calling a Subgraph Directly                                                                                      162\n",
      "Calling a Subgraph with a Function                                                                        164\n",
      "Multi-Agent Architectures                                                                                           165\n",
      "Supervisor Architecture                                                                                            167\n",
      "Summary                                                                                                                         170\n",
      "8. Patterns to Make the Most of LLMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  171\n",
      "Structured Output                                                                                                         173\n",
      "Intermediate Output                                                                                                  176\n",
      "Streaming LLM Output Token-by-Token                                                              178\n",
      "Human-in-the-Loop Modalities                                                                              179\n",
      "Multitasking LLMs                                                                                                     186\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Summary                                                                                                                         189\n",
      "9. Deployment: Launching Your AI Application into Production. . . . . . . . . . . . . . . . . . . . .  191\n",
      "Prerequisites                                                                                                                   191\n",
      "Install Dependencies                                                                                                  192\n",
      "Large Language Model                                                                                              192\n",
      "Vector Store                                                                                                                 193\n",
      "Backend API                                                                                                               197\n",
      "Create a LangSmith Account                                                                                    199\n",
      "Understanding the LangGraph Platform API                                                           200\n",
      "Data Models                                                                                                                201\n",
      "Features                                                                                                                        202\n",
      "Deploying Y our AI Application on LangGraph Platform                                        204\n",
      "Create a LangGraph API Config                                                                              204\n",
      "Test Y our LangGraph App Locally                                                                           205\n",
      "Deploy from the LangSmith UI                                                                               207\n",
      "Launch LangGraph Studio                                                                                        210\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise summary of the given text:\n",
      "\n",
      "Security                                                                                                                            213\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "provide the final summary of whole text after considering all main points.\n",
      "include a suitable title for the summary and mention highlights from summary in bullet points.\n",
      "\n",
      "Unfortunately, you didn't provide any text for me to summarize. Please provide the text, and I will do my best to create a concise summary.\n",
      "\n",
      "Here's a concise summary of the given text:\n",
      "\n",
      "This book, \"Learning LangChain\", guides developers in mastering the LangChain development framework for building production-ready AI applications. Written by Mayo Oshin and Nuno Campos, it covers topics such as retrieval-augmented generation, AI application development, and integrating third-party APIs. The book is designed for beginners with a basic understanding of Python or JavaScript and includes practical insights, in-depth tutorials, and code examples to make learning LangChain interesting and fun.\n",
      "\n",
      "Mayo Oshin and Nuno Campos are involved with LangChain, an open-source library, with Oshin being an early developer and advocate, and Campos being a founding software engineer.\n",
      "\n",
      "The provided text is a summary of praise for the book \"Learning LangChain\". The book covers LangChain, a platform for generative AI and agents, and provides comprehensive guidance on topics such as document retrieval, AI agent deployment, and production-ready applications. Reviewers praise the book for its clear explanations, actionable techniques, and engaging examples, making it a valuable resource for developers and professionals working with LangChain.\n",
      "\n",
      "LangChain, a tool for developing AI applications, has made it easier for companies like AppFolio to deploy AI projects with confidence. This is due to its helpful patterns and tooling insights that reduce the need for boilerplate setup.\n",
      "\n",
      "Unfortunately, you didn't provide the text. Please provide the text you'd like me to summarize. I'll be happy to assist you.\n",
      "\n",
      "Here's a concise summary of the given text:\n",
      "\n",
      "This is the copyright and publication information for the book \"Learning LangChain\" by Mayo Oshin and Nuno Campos, published by O'Reilly Media in February 2025. The book is available for purchase in print and online, with ISBN 978-1-098-16728-8.\n",
      "\n",
      "The provided text appears to be a table of contents for a book about using Large Language Models (LLMs) with the LangChain library. The book is divided into two chapters, with the first chapter covering LLM fundamentals and the second chapter focusing on Retrieval-Augmented Generation (RAG) models.\n",
      "\n",
      "Here's a concise summary of the table of contents:\n",
      "\n",
      "- Chapter 1: Covers the basics of using LLMs with LangChain, including setting up, making reusable prompts, and assembling LLM applications.\n",
      "- Chapter 2: Introduces Retrieval-Augmented Generation (RAG) models, specifically focusing on indexing data to pick relevant context for LLMs and converting text to numbers using embeddings.\n",
      "\n",
      "The given text appears to be a table of contents for a chapter or section on embeddings in natural language processing (NLP), specifically focusing on:\n",
      "\n",
      "1. Embeddings before Large Language Models (LLMs)\n",
      "2. Using LLMs for embeddings\n",
      "3. Semantic embeddings and their explanation\n",
      "4. Preprocessing text by converting documents and splitting text\n",
      "5. Generating text embeddings\n",
      "6. Storing embeddings in vector stores\n",
      "7. Using the PGVector library\n",
      "8. Working with vector stores\n",
      "9. Tracking document changes and indexing optimization.\n",
      "\n",
      "The provided text appears to be a table of contents for a chapter or section in a document. It lists various topics related to Retrieval-Augmented Generation (RAG), a technique that combines document retrieval and language model generation. The topics covered include:\n",
      "\n",
      "- Introduction to RAG\n",
      "- Retrieving relevant documents\n",
      "- Generating predictions using retrieved documents\n",
      "- Query transformation and routing\n",
      "- Multi-query retrieval and fusion\n",
      "- Hypothetical document embeddings\n",
      "- Query routing and construction\n",
      "\n",
      "The chapter seems to be a comprehensive guide to RAG, covering its fundamental concepts and advanced techniques.\n",
      "\n",
      "The provided text appears to be a table of contents for a chapter on using the LangGraph tool to enhance chatbots. \n",
      "\n",
      "Here is a concise summary:\n",
      "\n",
      "The chapter covers how to use LangGraph to give chatbots memory capabilities, including building a memory system, introducing LangGraph, creating a state graph, and adding memory to it. It also discusses modifying chat history, filtering, and merging messages. Finally, it explores two cognitive architectures for incorporating LangGraph into chatbots.\n",
      "\n",
      "The provided text appears to be a table of contents or an index from a book on a specific topic, likely related to artificial intelligence or programming. The summary is as follows:\n",
      "\n",
      "The table of contents outlines several chapters related to architecture, including Chain, Router, and Agent Architecture. It also mentions a Plan-Do Loop and the process of building a LangGraph Agent.\n",
      "\n",
      "The provided text appears to be a table of contents or a chapter list from a book or guide on working with Large Language Models (LLMs). The chapters cover various topics, including:\n",
      "\n",
      "- Managing multiple tools and dealing with summaries\n",
      "- Agents and multi-agent architectures\n",
      "- Calling subgraphs and supervisor architecture\n",
      "- Making the most of LLMs, including structured output, intermediate output, streaming output, human-in-the-loop modalities, and multitasking LLMs.\n",
      "\n",
      "The chapters seem to be a comprehensive guide to working with LLMs, likely focusing on their applications and best practices.\n",
      "\n",
      "Here's a concise summary of the given text:\n",
      "\n",
      "To deploy an AI application into production, follow these steps:\n",
      "\n",
      "1. Install dependencies, including a Large Language Model and Vector Store.\n",
      "2. Set up a LangSmith account and understand the LangGraph Platform API.\n",
      "3. Create a LangGraph API configuration.\n",
      "4. Test the application locally.\n",
      "5. Deploy from the LangSmith UI.\n",
      "\n",
      "These steps will help launch an AI application on the LangGraph Platform.\n",
      "\n",
      "There is no text provided. Could you please provide the text you would like me to summarize?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It seems you are trying to summarize various text snippets, but there is no original text provided. However, I can provide you with a text snippet for you to summarize.\\n\\nHere is a text snippet about the benefits of using LangChain for building AI applications:\\n\\nLangChain is an open-source library that simplifies the development of AI applications by providing a set of pre-built components and tools. With LangChain, developers can focus on building the core logic of their applications without worrying about implementing low-level details such as data retrieval and processing. This allows for faster development and deployment of AI applications, making it an attractive choice for companies looking to integrate AI into their products and services.\\n\\nLangChain's components and tools can be used to build a wide range of AI applications, including chatbots, virtual assistants, and content generation systems. The library provides a flexible and modular architecture, making it easy to integrate with other tools and frameworks.\\n\\nSome of the key benefits of using LangChain include:\\n\\n- Faster development and deployment of AI applications\\n- Simplified data retrieval and processing\\n- Increased flexibility and modularity\\n- Easy integration with other tools and frameworks\\n\\nLangChain's open-source nature also makes it a cost-effective choice for companies, as they can avoid the costs associated with proprietary software.\\n\\nPlease go ahead and create a summary based on this text snippet.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_summary_chain.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6dd8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
